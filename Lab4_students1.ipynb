{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import\n",
    "## Question 0 - Get common wikidata occupations\n",
    "\n",
    "> Write a sparql query that retrieves the top 100 occupations on wikidata (wikidata property P106).\n",
    "\n",
    "You may use the interface https://query.wikidata.org/ to try different queries. Here are some example sparql queries: https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT ?o (COUNT(?person) AS ?count) WHERE \n",
    "{\n",
    "   ?person wdt:P106 ?o\n",
    "}\n",
    "GROUP BY ?o\n",
    "ORDER BY DESC(?count)\n",
    "LIMIT 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following assertion should pass if your answer is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "occupations = ['Q82955', 'Q937857', 'Q36180', 'Q33999', 'Q1650915', 'Q1028181', 'Q1930187', 'Q177220', 'Q1622272', 'Q49757', 'Q36834', 'Q40348', 'Q47064', 'Q639669', 'Q10800557', 'Q201788', 'Q2526255', 'Q43845', 'Q28389', 'Q42973', 'Q10871364', 'Q39631', 'Q193391', 'Q482980', 'Q483501', 'Q11513337', 'Q3665646', 'Q12299841', 'Q19204627', 'Q16533', 'Q81096', 'Q11774891', 'Q188094', 'Q1281618', 'Q333634', 'Q189290', 'Q250867', 'Q33231', 'Q2259451', 'Q42603', 'Q628099', 'Q37226', 'Q2309784', 'Q901', 'Q2066131', 'Q6625963', 'Q10798782', 'Q2374149', 'Q170790', 'Q4610556', 'Q185351', 'Q486748', 'Q3055126', 'Q753110', 'Q4964182', 'Q169470', 'Q158852', 'Q1234713', 'Q14089670', 'Q10873124', 'Q3282637', 'Q593644', 'Q947873', 'Q13414980', 'Q131524', 'Q11338576', 'Q15117302', 'Q488205', 'Q14467526', 'Q183945', 'Q10843402', 'Q13382576', 'Q13141064', 'Q214917', 'Q855091', 'Q644687', 'Q19595175', 'Q121594', 'Q2865819', 'Q16010345', 'Q1231865', 'Q2405480', 'Q350979', 'Q3400985', 'Q13365117', 'Q10833314', 'Q3621491', 'Q15981151', 'Q212980', 'Q16145150', 'Q1792450', 'Q15296811', 'Q15627169', 'Q2306091', 'Q4263842', 'Q806798', 'Q5716684', 'Q2516866', 'Q3387717', 'Q131512']\n",
    "\n",
    "def evalSparql(query):\n",
    "    return requests.post('https://query.wikidata.org/sparql', data=query, headers={\n",
    "        'content-type': 'application/sparql-query',\n",
    "        'accept': 'application/json',\n",
    "        'user-agent': 'User:Tpt'\n",
    "    }).json()['results']['bindings']\n",
    "\n",
    "myOccupations = [val['o']['value'].replace('http://www.wikidata.org/entity/', '') \n",
    "                 for val in evalSparql(query)]\n",
    "assert(frozenset(occupations) == frozenset(myOccupations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupations labels\n",
    "\n",
    "We load the labels of the occupations from Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q121594': 'professor', 'Q82955': 'politician', 'Q81096': 'engineer', 'Q177220': 'singer', 'Q169470': 'physicist', 'Q201788': 'historian', 'Q170790': 'mathematician', 'Q158852': 'conductor', 'Q214917': 'playwright', 'Q188094': 'economist', 'Q212980': 'psychologist', 'Q131524': 'entrepreneur', 'Q482980': 'author', 'Q193391': 'diplomat', 'Q185351': 'jurist', 'Q250867': 'Catholic priest', 'Q483501': 'artist', 'Q488205': 'singer-songwriter', 'Q593644': 'chemist', 'Q639669': 'musician', 'Q350979': 'zoologist', 'Q486748': 'pianist', 'Q333634': 'translator', 'Q937857': 'association football player', 'Q806798': 'banker', 'Q1028181': 'painter', 'Q183945': 'record producer', 'Q855091': 'guitarist', 'Q644687': 'illustrator', 'Q1234713': 'theologian', 'Q947873': 'television presenter', 'Q1281618': 'sculptor', 'Q753110': 'songwriter', 'Q1792450': 'art historian', 'Q1930187': 'journalist', 'Q1650915': 'researcher', 'Q2066131': 'athlete', 'Q2374149': 'botanist', 'Q2526255': 'film director', 'Q2516866': 'publisher', 'Q2405480': 'voice actor', 'Q2306091': 'sociologist', 'Q2865819': 'opera singer', 'Q1231865': 'pedagogue', 'Q3282637': 'film producer', 'Q1622272': 'university teacher', 'Q3400985': 'academic', 'Q3665646': 'basketball player', 'Q3055126': 'entomologist', 'Q3387717': 'theatre director', 'Q3621491': 'archaeologist', 'Q189290': 'military officer', 'Q4263842': 'literary critic', 'Q4610556': 'model', 'Q4964182': 'philosopher', 'Q5716684': 'dancer', 'Q6625963': 'novelist', 'Q2259451': 'stage actor', 'Q628099': 'association football manager', 'Q10833314': 'tennis player', 'Q10843402': 'swimmer', 'Q10800557': 'film actor', 'Q10871364': 'baseball player', 'Q10873124': 'chess player', 'Q11338576': 'boxer', 'Q10798782': 'television actor', 'Q11774891': 'ice hockey player', 'Q11513337': 'athletics competitor', 'Q12299841': 'cricketer', 'Q13141064': 'badminton player', 'Q13365117': 'handball player', 'Q13414980': 'Australian rules footballer', 'Q14089670': 'rugby union player', 'Q14467526': 'linguist', 'Q131512': 'agriculturer', 'Q15117302': 'volleyball player', 'Q15981151': 'jazz musician', 'Q16010345': 'performer', 'Q2309784': 'sport cyclist', 'Q19204627': 'American football player', 'Q15296811': 'drawer', 'Q15627169': 'trade unionist', 'Q19595175': 'amateur wrestler', 'Q13382576': 'rower', 'Q16145150': 'music pedagogue', 'Q28389': 'screenwriter', 'Q33231': 'photographer', 'Q36180': 'writer', 'Q43845': 'businessperson', 'Q33999': 'actor', 'Q36834': 'composer', 'Q901': 'scientist', 'Q42973': 'architect', 'Q49757': 'poet', 'Q42603': 'priest', 'Q39631': 'physician', 'Q40348': 'lawyer', 'Q37226': 'teacher', 'Q16533': 'judge', 'Q47064': 'military personnel'}\n"
     ]
    }
   ],
   "source": [
    "occupations_label = {}\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT ?o ?oLabel \n",
    "WHERE { \n",
    "    VALUES ?o { %s } \n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\"\"\"% ' '.join('wd:' + o for o in occupations)\n",
    "\n",
    "for result in evalSparql(query):\n",
    "    occupations_label[result['o']['value'].replace('http://www.wikidata.org/entity/', '')] = result['oLabel']['value']\n",
    "\n",
    "print(occupations_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load *all* the labels of the occupations from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q121594': ['professor', 'Prof.'], 'Q82955': ['politician', 'political leader', 'polit.', 'political figure'], 'Q81096': ['engineer'], 'Q177220': ['singer', 'vocalist'], 'Q169470': ['physicist'], 'Q201788': ['historian', 'historians', 'historiographer'], 'Q170790': ['mathematician'], 'Q158852': ['conductor', 'Conducting'], 'Q214917': ['playwright', 'dramatist', 'scriptwriter', 'playwrite', 'Playwright, dramatist'], 'Q188094': ['economist'], 'Q212980': ['psychologist'], 'Q131524': ['entrepreneur'], 'Q482980': ['author'], 'Q193391': ['diplomat'], 'Q185351': ['jurist'], 'Q250867': ['Catholic priest', 'Roman Catholic priest', 'Catholic presbyter', 'Roman Catholic presbyter'], 'Q483501': ['artist'], 'Q488205': ['singer-songwriter', 'singer/songwriter', 'singer songwriter', 'singersongwriter'], 'Q593644': ['chemist', 'chemists'], 'Q639669': ['musician'], 'Q350979': ['zoologist', 'zooligist'], 'Q486748': ['pianist'], 'Q333634': ['translator'], 'Q937857': ['association football player', 'footballer', 'football player', 'association footballer', 'soccer player'], 'Q806798': ['banker', 'Private Banker', 'private sector banker'], 'Q1028181': ['painter'], 'Q183945': ['record producer', 'music producer'], 'Q855091': ['guitarist', 'guitar player'], 'Q644687': ['illustrator'], 'Q1234713': ['theologian', 'religious scholar'], 'Q947873': ['television presenter', 'TV presenter', 'hostess', 'TV host', 'TV personality', 'host', 'television personality', 'television host'], 'Q1281618': ['sculptor'], 'Q753110': ['songwriter', 'song writer'], 'Q1792450': ['art historian'], 'Q1930187': ['journalist', 'journo'], 'Q1650915': ['researcher'], 'Q2066131': ['athlete', 'sportsperson', 'sportsman', 'sportswoman'], 'Q2374149': ['botanist', 'botany', 'plant scientist'], 'Q2526255': ['film director', 'director', 'movie director'], 'Q2516866': ['publisher'], 'Q2405480': ['voice actor', 'voice actress', 'voice artist'], 'Q2306091': ['sociologist'], 'Q2865819': ['opera singer'], 'Q1231865': ['pedagogue', 'educationalist'], 'Q3282637': ['film producer', 'producer', 'movie producer'], 'Q1622272': ['university teacher', 'lecturer', 'university teachers', 'college lecturer', 'college professor'], 'Q3400985': ['academic', 'college graduates', 'university graduates'], 'Q3665646': ['basketball player', 'professional basketball player', 'basketballer'], 'Q3055126': ['entomologist'], 'Q3387717': ['theatre director', 'theater director', 'stage director'], 'Q3621491': ['archaeologist', 'archeologist'], 'Q189290': ['military officer', 'army officer', 'officer'], 'Q4263842': ['literary critic', 'book critic', 'literary critique'], 'Q4610556': ['model', 'fashion model', 'mannequin'], 'Q4964182': ['philosopher'], 'Q5716684': ['dancer'], 'Q6625963': ['novelist'], 'Q2259451': ['stage actor', 'stage actress', 'theater actor', 'theater actress', 'theatre actor', 'theatre actress'], 'Q628099': ['association football manager', 'football manager', 'association football coach', 'football coach', 'soccer coach', 'soccer manager'], 'Q10833314': ['tennis player'], 'Q10843402': ['swimmer'], 'Q10800557': ['film actor', 'film actress', 'movie actor', 'movie actress'], 'Q10871364': ['baseball player'], 'Q10873124': ['chess player'], 'Q11338576': ['boxer', 'pugilist'], 'Q10798782': ['television actor', 'actor', 'actress', 'television actress', 'TV actor', 'TV actress'], 'Q11774891': ['ice hockey player', 'hockey player'], 'Q11513337': ['athletics competitor', 'track and field athlete', 'athlete (restricted sense)'], 'Q12299841': ['cricketer', 'cricket player'], 'Q13141064': ['badminton player'], 'Q13365117': ['handball player', 'handballer'], 'Q13414980': ['Australian rules footballer', 'Australian footballer', 'Australian rules football player', 'Australian-rules football player'], 'Q14089670': ['rugby union player'], 'Q14467526': ['linguist', 'linguistic scholar'], 'Q131512': ['agriculturer', 'farmer', 'agriculturist', 'cultivator', 'grower', 'raiser'], 'Q15117302': ['volleyball player', 'volleyballer'], 'Q15981151': ['jazz musician'], 'Q16010345': ['performer', 'scenic artist', 'performing artist'], 'Q2309784': ['sport cyclist', 'racing cyclist', 'sport bicyclist', 'sport biker'], 'Q19204627': ['American football player', 'football player'], 'Q15296811': ['drawer', 'illustrator', 'draughtsperson', 'draughtsman', 'draftsperson', 'draftsman', 'draftswoman', 'drafter'], 'Q15627169': ['trade unionist', 'labor unionist', 'labour unionist'], 'Q19595175': ['amateur wrestler', 'wrestler'], 'Q13382576': ['rower', 'oarsman', 'oarswoman'], 'Q16145150': ['music pedagogue', 'music teacher'], 'Q28389': ['screenwriter', 'writer', 'screen writer', 'scriptwriter', 'scenarist', 'film writer', 'tv writer', 'script writer'], 'Q33231': ['photographer'], 'Q36180': ['writer', 'author', 'writers', 'authors'], 'Q43845': ['businessperson', 'businessman', 'dealer', 'business person', 'business woman', 'businesswoman', 'business man'], 'Q33999': ['actor', 'actors', 'actresses', 'actress'], 'Q36834': ['composer'], 'Q901': ['scientist', 'natural philosopher'], 'Q42973': ['architect'], 'Q49757': ['poet', 'bard', 'poetess'], 'Q42603': ['priest', 'priestess', 'reverend'], 'Q39631': ['physician', 'physicians'], 'Q40348': ['lawyer', 'attorney', 'Jurisprudente'], 'Q37226': ['teacher', 'professor', 'educator', 'schoolmaster', 'schoolmistress', 'school teacher'], 'Q16533': ['judge', 'magistrate', 'justice', 'judges', 'justices'], 'Q47064': ['military personnel']}\n"
     ]
    }
   ],
   "source": [
    "occupations_labels = {k: [v] for k, v in occupations_label.items()}\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT ?o ?altLabel \n",
    "WHERE {\n",
    "  VALUES ?o { %s }\n",
    "  ?o skos:altLabel ?altLabel . FILTER (lang(?altLabel) = \"en\")\n",
    "}\"\"\" % ' '.join('wd:' + o for o in occupations) \n",
    "\n",
    "for result in evalSparql(query):\n",
    "    occupations_labels[result['o']['value'].replace('http://www.wikidata.org/entity/', '')].append(result['altLabel']['value'])\n",
    "\n",
    "print(occupations_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia articles\n",
    "\n",
    "Here we load the training and the testing sets. To save memory space we use a generator that will read the file each time we iterate over the training or the testing examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "def loadJson(filename):\n",
    "    with gzip.open(filename, 'rt') as fp:\n",
    "        for line in fp:\n",
    "            yield json.loads(line)\n",
    "\n",
    "class MakeIter(object):\n",
    "    def __init__(self, generator_func, **kwargs):\n",
    "        self.generator_func = generator_func\n",
    "        self.kwargs = kwargs\n",
    "    def __iter__(self):\n",
    "        return self.generator_func(**self.kwargs)\n",
    "\n",
    "training_set = MakeIter(loadJson, filename='wiki-train.json.gz')\n",
    "testing_set = MakeIter(loadJson, filename='wiki-test.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract occupations from summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Dictionnary extraction\n",
    "\n",
    "> Using ```occupations_labels``` dictionnary, identify all occupations for each articles. Complete the function below to evaluate the accuracy of such approach. It will serve as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4842586814146957"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_occ = dict()\n",
    "for key, occs in occupations_labels.items():\n",
    "    for occ in occs:\n",
    "        label_to_occ[occ.lower()] = key\n",
    "\n",
    "def predict_dictionnary(example, occupations_labels):\n",
    "    occs = []\n",
    "    summary = example['summary'].lower()\n",
    "    labels = label_to_occ.keys()\n",
    "    for label in labels:\n",
    "        if label in summary:\n",
    "            occs.append(label_to_occ[label])\n",
    "    return occs\n",
    "    \n",
    "def evaluate_dictionnary(training_set, occupations_labels):\n",
    "    nexample = 0\n",
    "    accuracy = 0.\n",
    "    prediction = None\n",
    "    for example in training_set:\n",
    "        prediction = predict_dictionnary(example, occupations_labels)\n",
    "        p = frozenset(prediction)\n",
    "        g = frozenset(example['occupations'])\n",
    "        accuracy += 1.*len(p & g) / len(p | g)\n",
    "        nexample += 1\n",
    "    return accuracy / nexample\n",
    "\n",
    "evaluate_dictionnary(training_set, occupations_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Simple neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the articles \"summary\" and we take the average of the word vectors.\n",
    "This is done with spacy loaded with the fast text vectors.\n",
    "To do the installation/loading [takes 8-10 minutes, dl 1.2Go]\n",
    "```\n",
    "pip3 install spacy\n",
    "wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/cc.en.300.vec.gz\n",
    "python3 -m spacy init-model en /tmp/en_vectors_wiki_lg --vectors-loc cc.en.300.vec.gz\n",
    "rm cc.en.300.vec.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nlp = spacy.load('/tmp/en_vectors_wiki_lg')\n",
    "\n",
    "def vectorize(dataset, nlp):\n",
    "    result = {}\n",
    "    for example in dataset:\n",
    "        doc = nlp(example['summary'], disable=['parser', 'tagger'])\n",
    "        result[example['title']] = {}\n",
    "        result[example['title']]['vector'] = doc.vector\n",
    "        result[example['title']]['summary'] = example['summary']\n",
    "        if 'occupations' in example:\n",
    "            result[example['title']]['occupations'] = example['occupations']\n",
    "    return result\n",
    "    \n",
    "vectorized_training = vectorize(training_set, nlp)\n",
    "vectorized_testing = vectorize(testing_set, nlp)\n",
    "nlp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427798"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.45162819e-02 -2.45802402e-02 -4.59302496e-03 -4.09372151e-02\n",
      " -4.47662771e-02 -4.18604538e-03 -3.15232435e-03 -1.44802360e-02\n",
      " -1.68499984e-02 -3.69651243e-03 -1.16255814e-02  1.43651171e-02\n",
      "  2.02674349e-03 -5.88953542e-03 -2.17011590e-02  1.02302311e-02\n",
      " -2.49313917e-02 -5.65232616e-03 -2.25581434e-02  8.29069968e-03\n",
      " -1.44069805e-03  2.25197673e-02 -6.81395701e-04 -1.37232570e-02\n",
      " -1.26674427e-02 -3.35569866e-02  1.10627888e-02 -2.37208814e-03\n",
      " -2.30000000e-02  7.58616179e-02 -5.03487710e-04 -2.51116175e-02\n",
      "  9.26511642e-03 -2.52558179e-02 -1.51058156e-02 -9.51627828e-03\n",
      "  1.17523270e-02  1.22441910e-03  1.08139520e-03  3.39302444e-03\n",
      "  2.20116391e-03  1.46860480e-02 -1.43686021e-02  5.76395402e-03\n",
      "  1.74162779e-02 -4.76220921e-02 -1.72569733e-02 -1.49988411e-02\n",
      " -1.77732538e-02  1.58907007e-02 -7.23255938e-03  2.43825577e-02\n",
      " -2.73104683e-02 -3.67430188e-02 -1.48802334e-02 -1.34825567e-02\n",
      " -3.14348824e-02  1.95930228e-02 -6.68605033e-04 -9.24302172e-03\n",
      "  1.56976283e-04 -1.65674444e-02 -1.30372085e-02  6.16298130e-05\n",
      " -3.63139645e-03  2.74534873e-03 -1.62697677e-02 -4.70697694e-03\n",
      "  5.48139494e-03  4.39302297e-03  4.65523303e-02  2.29872130e-02\n",
      "  2.72058025e-02 -5.52790612e-03  2.19720937e-02 -4.41581383e-02\n",
      "  1.33255811e-03  1.20244222e-02  3.49267460e-02  3.76593024e-02\n",
      "  8.65232572e-03 -6.52325572e-03 -1.90407019e-02  1.03569757e-02\n",
      "  1.09301973e-03 -6.28488278e-03  3.98965068e-02 -3.81744131e-02\n",
      " -1.35965087e-02  1.74023230e-02 -1.48686031e-02  5.78604685e-03\n",
      " -8.59186146e-03  4.74418374e-03  1.54720917e-02 -6.42325589e-03\n",
      " -1.58430226e-02 -2.98779178e-02 -1.54255824e-02  3.28209326e-02\n",
      "  2.43825577e-02  1.32907031e-03  1.80883706e-02 -2.72825565e-02\n",
      "  9.28488653e-03 -7.39418622e-03 -7.98023026e-03  1.84244160e-02\n",
      " -9.45350039e-04 -1.16825579e-02  1.15813862e-03 -2.10464321e-04\n",
      " -3.00813979e-03  4.75407019e-02 -8.32790602e-03  4.11511678e-03\n",
      " -1.25604663e-02  8.92209262e-03  7.64534995e-03 -2.65965052e-02\n",
      "  6.58837147e-03 -1.12011610e-02 -9.68022924e-03  1.60023291e-02\n",
      "  1.61629519e-04  3.20906974e-02 -1.59848798e-02  1.14162825e-02\n",
      " -2.40430199e-02  5.39906919e-02 -4.80814092e-03  3.02209193e-03\n",
      "  5.89418598e-03 -3.94418649e-03 -2.68058274e-02 -8.98256153e-03\n",
      " -2.94616278e-02  3.90697829e-03  4.68255766e-03  3.96162830e-03\n",
      " -2.68069748e-02 -2.68395394e-02 -9.76740339e-05  5.67557989e-03\n",
      "  4.43197712e-02 -1.38953477e-02 -3.69888335e-01  1.04639539e-02\n",
      "  1.55372089e-02 -1.35093015e-02 -8.09988379e-02  2.67802346e-02\n",
      "  2.21941881e-02 -7.86627829e-03 -1.00313956e-02  1.52511625e-02\n",
      "  1.45744160e-01  4.61395411e-03  7.26162829e-03  3.14453505e-02\n",
      " -7.95465056e-03 -1.25395320e-02  6.95348764e-03 -2.48023286e-03\n",
      "  6.17325725e-03  1.26546472e-02  1.03558144e-02 -1.21616265e-02\n",
      " -1.27907039e-03 -1.99348871e-02 -9.01860371e-03  4.25581448e-03\n",
      "  7.45790750e-02  1.02186035e-02 -9.93953645e-03  1.72848776e-02\n",
      " -1.03779081e-02  1.46616297e-02 -3.75465187e-03 -2.26953458e-02\n",
      "  5.36046689e-04  6.64511696e-02 -2.53790785e-02  5.80627881e-02\n",
      " -1.42732579e-02  9.22453254e-02 -1.12825576e-02 -2.51837187e-02\n",
      "  3.90697736e-03  5.96395321e-03 -3.02476659e-02  2.63883732e-02\n",
      " -1.69488378e-02  7.39418576e-03  1.60662793e-02 -1.68313961e-02\n",
      " -8.25814065e-03 -1.36965141e-02  7.30697624e-03  1.63453538e-02\n",
      " -4.15407047e-02  1.05633713e-01  1.53325591e-02  6.63023209e-03\n",
      "  3.93279046e-02 -1.27697680e-02 -5.95697621e-03 -8.67441762e-03\n",
      "  1.58593040e-02  9.42093134e-03 -4.15697647e-03  1.34639572e-02\n",
      " -4.10383604e-02 -2.82325619e-03 -2.43790708e-02 -4.02325485e-03\n",
      "  1.65058132e-02  4.21395432e-03  1.25813941e-02  1.64744183e-02\n",
      " -2.81162816e-03  1.34813897e-02 -8.19302350e-03 -7.04767322e-03\n",
      "  1.67139638e-02  1.43581396e-02  1.20023256e-02  4.96162800e-03\n",
      "  1.76325571e-02 -7.07674446e-03 -4.24197726e-02 -2.34697610e-02\n",
      " -1.86058115e-02 -2.32790736e-03  2.98906974e-02  1.53604464e-03\n",
      "  1.95941851e-02 -2.67104693e-02 -1.12453466e-02 -2.54534930e-03\n",
      " -4.29302268e-03  3.56558077e-02 -4.36046888e-04 -8.16406980e-02\n",
      "  5.04779041e-01 -2.18813960e-02  1.15883695e-02  2.14848872e-02\n",
      "  7.80581404e-03  1.55116236e-02 -1.11523261e-02  4.61628864e-04\n",
      "  1.72918607e-02  1.43034859e-02  2.05546506e-02 -8.23488459e-03\n",
      " -3.16290706e-02 -4.83953534e-03 -1.82697661e-02  2.02907110e-03\n",
      " -3.51163093e-04  1.10220918e-02 -8.54755938e-02 -2.68255756e-03\n",
      "  1.83174424e-02  1.91116314e-02 -4.73488262e-03 -8.08255840e-03\n",
      "  1.37906978e-02 -7.76046468e-03 -2.82767452e-02 -2.99069774e-03\n",
      "  1.06569799e-02 -5.99999772e-03  1.11883730e-02  4.28720983e-03\n",
      " -3.12255807e-02 -8.07186142e-02  8.59302282e-03 -8.11744668e-03\n",
      " -5.36279054e-03  1.87046509e-02 -1.10972092e-01 -3.07988375e-02\n",
      "  9.47441999e-03 -1.03662787e-02  1.16337193e-02  3.22093032e-02\n",
      " -2.69790720e-02  2.25430205e-02 -1.49802361e-02 -1.05290683e-02\n",
      " -4.36534919e-02  6.34883530e-04 -2.83197612e-02 -1.37674408e-02\n",
      " -1.50220934e-02  1.30851150e-01 -1.22430259e-02  2.38767453e-02]\n"
     ]
    }
   ],
   "source": [
    "v = vectorized_training['George_Washington']['vector']\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the vectorized_training into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDict(d, percent):\n",
    "    split_idx = int(len(d) * percent)\n",
    "    d1 = dict(list(d.items())[: split_idx])\n",
    "    d2 = dict(list(d.items())[split_idx:])                \n",
    "\n",
    "    return d1, d2\n",
    "\n",
    "vectorized_training_test, vectorized_training_train = splitDict(vectorized_training, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342239"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_training_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We encode the data\n",
    "import numpy as np\n",
    "\n",
    "def encode_data(vectorized_data):\n",
    "    X = np.array([vectorized_data[article]['vector'] for article in vectorized_data])\n",
    "    y = np.array([[(1 if occupation in vectorized_data[article]['occupations'] else 0)\n",
    "                        for occupation in occupations ] for article in vectorized_data])\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = encode_data(vectorized_training_train)\n",
    "X_test, y_test = encode_data(vectorized_training_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342239, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342239, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using keras, define a sequential neural network with two layers. Use categorical_crossentropy as a loss function and softmax as the activation function of the output layer\n",
    "\n",
    "You can look into the documentation here: https://keras.io/getting-started/sequential-model-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=300))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "\n",
    "optimizer = Adam()\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 308015 samples, validate on 34224 samples\n",
      "Epoch 1/50\n",
      "308015/308015 [==============================] - 3s 10us/step - loss: 3.5608 - acc: 0.4881 - val_loss: 2.0247 - val_acc: 0.6362\n",
      "Epoch 2/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 2.3868 - acc: 0.6581 - val_loss: 1.7224 - val_acc: 0.6907\n",
      "Epoch 3/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 2.1847 - acc: 0.6880 - val_loss: 1.6056 - val_acc: 0.7125\n",
      "Epoch 4/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 2.0853 - acc: 0.7033 - val_loss: 1.5498 - val_acc: 0.7234\n",
      "Epoch 5/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 2.0255 - acc: 0.7115 - val_loss: 1.5037 - val_acc: 0.7321\n",
      "Epoch 6/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.9833 - acc: 0.7176 - val_loss: 1.4793 - val_acc: 0.7359\n",
      "Epoch 7/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.9517 - acc: 0.7213 - val_loss: 1.4538 - val_acc: 0.7409\n",
      "Epoch 8/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.9250 - acc: 0.7251 - val_loss: 1.4335 - val_acc: 0.7466\n",
      "Epoch 9/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.9036 - acc: 0.7273 - val_loss: 1.4188 - val_acc: 0.7452\n",
      "Epoch 10/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8846 - acc: 0.7293 - val_loss: 1.4096 - val_acc: 0.7491\n",
      "Epoch 11/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8676 - acc: 0.7324 - val_loss: 1.3920 - val_acc: 0.7537\n",
      "Epoch 12/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8522 - acc: 0.7333 - val_loss: 1.3919 - val_acc: 0.7550\n",
      "Epoch 13/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8387 - acc: 0.7349 - val_loss: 1.3776 - val_acc: 0.7560\n",
      "Epoch 14/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8256 - acc: 0.7372 - val_loss: 1.3729 - val_acc: 0.7587\n",
      "Epoch 15/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8138 - acc: 0.7383 - val_loss: 1.3733 - val_acc: 0.7553\n",
      "Epoch 16/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8025 - acc: 0.7391 - val_loss: 1.3637 - val_acc: 0.7564\n",
      "Epoch 17/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 1.7916 - acc: 0.7408 - val_loss: 1.3510 - val_acc: 0.7603\n",
      "Epoch 18/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 1.7818 - acc: 0.7417 - val_loss: 1.3484 - val_acc: 0.7622\n",
      "Epoch 19/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7721 - acc: 0.7434 - val_loss: 1.3472 - val_acc: 0.7599\n",
      "Epoch 20/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 1.7634 - acc: 0.7436 - val_loss: 1.3448 - val_acc: 0.7615\n",
      "Epoch 21/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7554 - acc: 0.7450 - val_loss: 1.3347 - val_acc: 0.7649\n",
      "Epoch 22/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7462 - acc: 0.7463 - val_loss: 1.3367 - val_acc: 0.7639\n",
      "Epoch 23/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7388 - acc: 0.7467 - val_loss: 1.3338 - val_acc: 0.7641\n",
      "Epoch 24/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7308 - acc: 0.7481 - val_loss: 1.3379 - val_acc: 0.7646\n",
      "Epoch 25/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7240 - acc: 0.7486 - val_loss: 1.3341 - val_acc: 0.7644\n",
      "Epoch 26/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7173 - acc: 0.7494 - val_loss: 1.3279 - val_acc: 0.7634\n",
      "Epoch 27/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7097 - acc: 0.7500 - val_loss: 1.3268 - val_acc: 0.7610\n",
      "Epoch 28/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7030 - acc: 0.7510 - val_loss: 1.3203 - val_acc: 0.7664\n",
      "Epoch 29/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6968 - acc: 0.7518 - val_loss: 1.3266 - val_acc: 0.7640\n",
      "Epoch 30/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 1.6907 - acc: 0.7525 - val_loss: 1.3202 - val_acc: 0.7654\n",
      "Epoch 31/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6837 - acc: 0.7533 - val_loss: 1.3224 - val_acc: 0.7615\n",
      "Epoch 32/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6777 - acc: 0.7537 - val_loss: 1.3208 - val_acc: 0.7650\n",
      "Epoch 33/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6719 - acc: 0.7541 - val_loss: 1.3257 - val_acc: 0.7629\n",
      "Epoch 34/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6654 - acc: 0.7554 - val_loss: 1.3239 - val_acc: 0.7624\n",
      "Epoch 35/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6594 - acc: 0.7557 - val_loss: 1.3201 - val_acc: 0.7618\n",
      "Epoch 36/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6540 - acc: 0.7566 - val_loss: 1.3231 - val_acc: 0.7631\n",
      "Epoch 37/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6487 - acc: 0.7568 - val_loss: 1.3197 - val_acc: 0.7663\n",
      "Epoch 38/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6427 - acc: 0.7581 - val_loss: 1.3195 - val_acc: 0.7684\n",
      "Epoch 39/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6375 - acc: 0.7579 - val_loss: 1.3150 - val_acc: 0.7657\n",
      "Epoch 40/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6323 - acc: 0.7581 - val_loss: 1.3244 - val_acc: 0.7619\n",
      "Epoch 41/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6270 - acc: 0.7595 - val_loss: 1.3234 - val_acc: 0.7617\n",
      "Epoch 42/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6214 - acc: 0.7601 - val_loss: 1.3270 - val_acc: 0.7601\n",
      "Epoch 43/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6162 - acc: 0.7604 - val_loss: 1.3214 - val_acc: 0.7638\n",
      "Epoch 44/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6115 - acc: 0.7606 - val_loss: 1.3241 - val_acc: 0.7622\n",
      "Epoch 45/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6061 - acc: 0.7616 - val_loss: 1.3275 - val_acc: 0.7599\n",
      "Epoch 46/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6012 - acc: 0.7622 - val_loss: 1.3222 - val_acc: 0.7651\n",
      "Epoch 47/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.5964 - acc: 0.7628 - val_loss: 1.3247 - val_acc: 0.7631\n",
      "Epoch 48/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.5910 - acc: 0.7632 - val_loss: 1.3250 - val_acc: 0.7668\n",
      "Epoch 49/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.5868 - acc: 0.7636 - val_loss: 1.3247 - val_acc: 0.7598\n",
      "Epoch 50/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.5814 - acc: 0.7645 - val_loss: 1.3295 - val_acc: 0.7646\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=1024, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Complete the function predict: output the list of occupations where the corresponding neuron on the output layer of our model has a value > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q177220', 'Q639669', 'Q33999'}\n"
     ]
    }
   ],
   "source": [
    "def predict_nn(model, article_name, vectorized_dataset):\n",
    "    input_vector = vectorized_dataset[article_name]['vector'].reshape((1, 300))\n",
    "    scores = model.predict(input_vector).reshape(100)\n",
    "    predictions = np.where(scores > 0.1)[0]\n",
    "#     print(scores[predictions])\n",
    "    return set(np.array(occupations)[predictions])\n",
    "\n",
    "print(predict_nn(model, 'Elvis_Presley', vectorized_training))\n",
    "# should be {'Q177220'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nn(vectorized_training, model):\n",
    "    nexample = 0\n",
    "    accuracy = 0.\n",
    "    prediction = None\n",
    "    for article_name in vectorized_training:\n",
    "        prediction = predict_nn(model, article_name, vectorized_training)\n",
    "        p = frozenset(prediction)\n",
    "        g = frozenset(vectorized_training[article_name]['occupations'])\n",
    "        accuracy += 1.*len(p & g) / len(p | g)\n",
    "        nexample += 1\n",
    "    return accuracy / nexample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7048576643356244\n",
      "0.6662116943899452\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_nn(vectorized_training_train, model))\n",
    "print(evaluate_nn(vectorized_training_test, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Your approach: CNN + BiRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, GRU, Dropout, Conv1D, MaxPooling1D, MaxPooling1D, Bidirectional, BatchNormalization, LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dataset into summaries, titles and occupations\n",
    "def parse(dataset):\n",
    "    titles = []\n",
    "    summaries = []\n",
    "    occs = []\n",
    "    for example in dataset:\n",
    "        titles.append(example['title'])\n",
    "        summaries.append(example['summary'])        \n",
    "        if 'occupations' in example:\n",
    "            occs.append(example['occupations'])\n",
    "        else:\n",
    "            occs.append([])\n",
    "    return titles, summaries, occs\n",
    "    \n",
    "titles_train, summaries_train, occs_train = parse(training_set)\n",
    "\n",
    "s = int(len(titles_train) * 0.8)\n",
    "titles_train_train, summaries_train_train, occs_train_train = titles_train[:s], summaries_train[:s], occs_train[:s]\n",
    "titles_train_test, summaries_train_test, occs_train_test = titles_train[s:], summaries_train[s:], occs_train[s:]\n",
    "\n",
    "titles_test, summaries_test, occs_test = parse(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 370295 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(titles_train_train)\n",
    "maxlen = 300\n",
    "training_samples = int(n_samples * 0.85)\n",
    "validation_samples = n_samples - training_samples\n",
    "max_words = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(summaries_train_train)\n",
    "\n",
    "# convert text to sequences\n",
    "sequences =  tokenizer.texts_to_sequences(summaries_train_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(summaries_train_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found', len(word_index), 'unique tokens.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_occs_to_labels(occupations, occs_train):\n",
    "    labels = []\n",
    "    for i in range(len(occs_train)):\n",
    "        label = []\n",
    "        for occ in occupations:\n",
    "            if occ in occs_train[i]:\n",
    "                label.append(1)\n",
    "            else:\n",
    "                label.append(0)\n",
    "        labels.append(label)\n",
    "    return np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (342333, 300)\n",
      "Shape of label tensor: (342333, 100)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "data_test = pad_sequences(sequences_test, maxlen=maxlen)\n",
    "labels = convert_occs_to_labels(occupations, occs_train_train)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# shuffle the data\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# split into training and testing set\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = 'glove.6B'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found', len(embeddings_index), 'word vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build embedding matrix to load into embedding layer\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 300, 300)          6000000   \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 300, 64)           57664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 300, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 100, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 100, 128)          512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 33, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 33, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 33, 256)           98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 33, 256)           1024      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 11, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 11, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 11, 400)           548400    \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 11, 300)           495900    \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 300)               405900    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               154112    \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               25700     \n",
      "=================================================================\n",
      "Total params: 8,013,948\n",
      "Trainable params: 8,011,004\n",
      "Non-trainable params: 2,944\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "\n",
    "model.add(Conv1D(64,kernel_size=3,padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Conv1D(128,kernel_size=3,padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Conv1D(256,kernel_size=3,padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "\n",
    "model.add(Bidirectional(GRU(200, return_sequences=True, recurrent_dropout = 0.15)))\n",
    "\n",
    "model.add(Bidirectional(GRU(150, return_sequences=True, recurrent_dropout = 0.15)))\n",
    "\n",
    "model.add(Bidirectional(GRU(150, recurrent_dropout = 0.1)))\n",
    "\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Glove embedding in the model\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "\n",
    "# we will not update this layer during training. \n",
    "# If I trained this layer, the test accuracy would decrease\n",
    "model.layers[0].trainable = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as tfb\n",
    "\n",
    "POS_WEIGHT = 10  # Tested with other values (5, 15, 20, 100). 10 is optimal number\n",
    "\n",
    "def weighted_binary_crossentropy(target, output):\n",
    "    \"\"\"\n",
    "    Weighted binary crossentropy between an output tensor \n",
    "    and a target tensor. POS_WEIGHT is used as a multiplier \n",
    "    for the positive targets.\n",
    "\n",
    "    Combination of the following functions:\n",
    "    * keras.losses.binary_crossentropy\n",
    "    * keras.backend.tensorflow_backend.binary_crossentropy\n",
    "    * tf.nn.weighted_cross_entropy_with_logits\n",
    "    \"\"\"\n",
    "    # transform back to logits\n",
    "    _epsilon = tfb._to_tensor(tfb.epsilon(), output.dtype.base_dtype)\n",
    "    output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "    output = tf.log(output / (1 - output))\n",
    "    # compute weighted loss\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(targets=target,\n",
    "                                                    logits=output,\n",
    "                                                    pos_weight=POS_WEIGHT)\n",
    "    return tf.reduce_mean(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 290983 samples, validate on 51350 samples\n",
      "Epoch 1/50\n",
      "290983/290983 [==============================] - 86s 294us/step - loss: 0.4379 - acc: 0.4384 - val_loss: 0.1632 - val_acc: 0.5538\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16319, saving model to best_model.h5\n",
      "Epoch 2/50\n",
      "290983/290983 [==============================] - 81s 278us/step - loss: 0.1416 - acc: 0.6342 - val_loss: 0.1241 - val_acc: 0.6702\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16319 to 0.12406, saving model to best_model.h5\n",
      "Epoch 3/50\n",
      "290983/290983 [==============================] - 79s 270us/step - loss: 0.1134 - acc: 0.6953 - val_loss: 0.1076 - val_acc: 0.7137\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.12406 to 0.10758, saving model to best_model.h5\n",
      "Epoch 4/50\n",
      "290983/290983 [==============================] - 79s 271us/step - loss: 0.1023 - acc: 0.7189 - val_loss: 0.0957 - val_acc: 0.7264\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.10758 to 0.09567, saving model to best_model.h5\n",
      "Epoch 5/50\n",
      "290983/290983 [==============================] - 79s 270us/step - loss: 0.0968 - acc: 0.7305 - val_loss: 0.0932 - val_acc: 0.7304\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09567 to 0.09316, saving model to best_model.h5\n",
      "Epoch 6/50\n",
      "290983/290983 [==============================] - 79s 271us/step - loss: 0.0933 - acc: 0.7376 - val_loss: 0.0900 - val_acc: 0.7472\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09316 to 0.09003, saving model to best_model.h5\n",
      "Epoch 7/50\n",
      "290983/290983 [==============================] - 82s 282us/step - loss: 0.0906 - acc: 0.7438 - val_loss: 0.0888 - val_acc: 0.7512\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.09003 to 0.08882, saving model to best_model.h5\n",
      "Epoch 8/50\n",
      "290983/290983 [==============================] - 81s 279us/step - loss: 0.0884 - acc: 0.7470 - val_loss: 0.0871 - val_acc: 0.7567\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.08882 to 0.08713, saving model to best_model.h5\n",
      "Epoch 9/50\n",
      "290983/290983 [==============================] - 79s 272us/step - loss: 0.0869 - acc: 0.7494 - val_loss: 0.0853 - val_acc: 0.7509\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.08713 to 0.08526, saving model to best_model.h5\n",
      "Epoch 10/50\n",
      "290983/290983 [==============================] - 79s 270us/step - loss: 0.0854 - acc: 0.7523 - val_loss: 0.0841 - val_acc: 0.7510\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.08526 to 0.08409, saving model to best_model.h5\n",
      "Epoch 11/50\n",
      "290983/290983 [==============================] - 82s 282us/step - loss: 0.0842 - acc: 0.7536 - val_loss: 0.0827 - val_acc: 0.7562\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.08409 to 0.08273, saving model to best_model.h5\n",
      "Epoch 12/50\n",
      "290983/290983 [==============================] - 79s 272us/step - loss: 0.0831 - acc: 0.7559 - val_loss: 0.0823 - val_acc: 0.7602\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.08273 to 0.08230, saving model to best_model.h5\n",
      "Epoch 13/50\n",
      "290983/290983 [==============================] - 80s 273us/step - loss: 0.0820 - acc: 0.7566 - val_loss: 0.0830 - val_acc: 0.7618\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.08230\n",
      "Epoch 14/50\n",
      "290983/290983 [==============================] - 79s 271us/step - loss: 0.0811 - acc: 0.7578 - val_loss: 0.0816 - val_acc: 0.7643\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.08230 to 0.08161, saving model to best_model.h5\n",
      "Epoch 15/50\n",
      "290983/290983 [==============================] - 80s 276us/step - loss: 0.0804 - acc: 0.7582 - val_loss: 0.0814 - val_acc: 0.7605\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.08161 to 0.08139, saving model to best_model.h5\n",
      "Epoch 16/50\n",
      "290983/290983 [==============================] - 80s 274us/step - loss: 0.0797 - acc: 0.7597 - val_loss: 0.0811 - val_acc: 0.7642\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.08139 to 0.08114, saving model to best_model.h5\n",
      "Epoch 17/50\n",
      "290983/290983 [==============================] - 79s 271us/step - loss: 0.0792 - acc: 0.7610 - val_loss: 0.0815 - val_acc: 0.7555\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.08114\n",
      "Epoch 18/50\n",
      "290983/290983 [==============================] - 79s 271us/step - loss: 0.0784 - acc: 0.7611 - val_loss: 0.0800 - val_acc: 0.7608\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.08114 to 0.07999, saving model to best_model.h5\n",
      "Epoch 19/50\n",
      "290983/290983 [==============================] - 79s 271us/step - loss: 0.0780 - acc: 0.7613 - val_loss: 0.0810 - val_acc: 0.7676\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.07999\n",
      "Epoch 20/50\n",
      "290983/290983 [==============================] - 81s 278us/step - loss: 0.0775 - acc: 0.7616 - val_loss: 0.0797 - val_acc: 0.7612\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.07999 to 0.07968, saving model to best_model.h5\n",
      "Epoch 21/50\n",
      "290983/290983 [==============================] - 79s 273us/step - loss: 0.0769 - acc: 0.7624 - val_loss: 0.0799 - val_acc: 0.7597\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.07968\n",
      "Epoch 22/50\n",
      "290983/290983 [==============================] - 80s 274us/step - loss: 0.0764 - acc: 0.7628 - val_loss: 0.0804 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.07968\n",
      "Epoch 23/50\n",
      "290983/290983 [==============================] - 80s 274us/step - loss: 0.0761 - acc: 0.7623 - val_loss: 0.0795 - val_acc: 0.7605\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.07968 to 0.07952, saving model to best_model.h5\n",
      "Epoch 24/50\n",
      "290983/290983 [==============================] - 80s 274us/step - loss: 0.0757 - acc: 0.7630 - val_loss: 0.0806 - val_acc: 0.7610\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.07952\n",
      "Epoch 25/50\n",
      "290983/290983 [==============================] - 79s 273us/step - loss: 0.0752 - acc: 0.7628 - val_loss: 0.0806 - val_acc: 0.7634\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.07952\n",
      "Epoch 26/50\n",
      "290983/290983 [==============================] - 83s 285us/step - loss: 0.0748 - acc: 0.7638 - val_loss: 0.0805 - val_acc: 0.7650\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.07952\n",
      "Epoch 27/50\n",
      "290983/290983 [==============================] - 86s 297us/step - loss: 0.0744 - acc: 0.7650 - val_loss: 0.0802 - val_acc: 0.7687\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.07952\n",
      "Epoch 28/50\n",
      "290983/290983 [==============================] - 90s 311us/step - loss: 0.0742 - acc: 0.7649 - val_loss: 0.0782 - val_acc: 0.7643\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.07952 to 0.07825, saving model to best_model.h5\n",
      "Epoch 29/50\n",
      "290983/290983 [==============================] - 87s 298us/step - loss: 0.0736 - acc: 0.7648 - val_loss: 0.0797 - val_acc: 0.7678\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.07825\n",
      "Epoch 30/50\n",
      "290983/290983 [==============================] - 87s 300us/step - loss: 0.0735 - acc: 0.7644 - val_loss: 0.0790 - val_acc: 0.7576\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.07825\n",
      "Epoch 31/50\n",
      "290983/290983 [==============================] - 90s 309us/step - loss: 0.0732 - acc: 0.7649 - val_loss: 0.0824 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.07825\n",
      "Epoch 32/50\n",
      "290983/290983 [==============================] - 88s 304us/step - loss: 0.0730 - acc: 0.7655 - val_loss: 0.0799 - val_acc: 0.7671\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.07825\n",
      "Epoch 33/50\n",
      "290983/290983 [==============================] - 85s 293us/step - loss: 0.0725 - acc: 0.7661 - val_loss: 0.0809 - val_acc: 0.7521\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.07825\n",
      "Epoch 34/50\n",
      "290983/290983 [==============================] - 87s 299us/step - loss: 0.0700 - acc: 0.7690 - val_loss: 0.0772 - val_acc: 0.7684\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.07825 to 0.07715, saving model to best_model.h5\n",
      "Epoch 35/50\n",
      "290983/290983 [==============================] - 90s 309us/step - loss: 0.0693 - acc: 0.7697 - val_loss: 0.0774 - val_acc: 0.7648\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.07715\n",
      "Epoch 36/50\n",
      "290983/290983 [==============================] - 84s 289us/step - loss: 0.0688 - acc: 0.7694 - val_loss: 0.0778 - val_acc: 0.7678\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.07715\n",
      "Epoch 37/50\n",
      "290983/290983 [==============================] - 91s 312us/step - loss: 0.0684 - acc: 0.7695 - val_loss: 0.0780 - val_acc: 0.7676\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.07715\n",
      "Epoch 38/50\n",
      "290983/290983 [==============================] - 92s 316us/step - loss: 0.0682 - acc: 0.7698 - val_loss: 0.0785 - val_acc: 0.7678\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.07715\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "290983/290983 [==============================] - 92s 318us/step - loss: 0.0680 - acc: 0.7699 - val_loss: 0.0780 - val_acc: 0.7662\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.07715\n",
      "Epoch 40/50\n",
      "290983/290983 [==============================] - 83s 285us/step - loss: 0.0675 - acc: 0.7699 - val_loss: 0.0779 - val_acc: 0.7668\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.07715\n",
      "Epoch 41/50\n",
      "290983/290983 [==============================] - 82s 282us/step - loss: 0.0673 - acc: 0.7703 - val_loss: 0.0779 - val_acc: 0.7670\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.07715\n",
      "Epoch 42/50\n",
      "290983/290983 [==============================] - 81s 280us/step - loss: 0.0673 - acc: 0.7706 - val_loss: 0.0781 - val_acc: 0.7666\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.07715\n",
      "Epoch 43/50\n",
      "290983/290983 [==============================] - 86s 295us/step - loss: 0.0672 - acc: 0.7702 - val_loss: 0.0781 - val_acc: 0.7660\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.07715\n",
      "Epoch 44/50\n",
      "290983/290983 [==============================] - 89s 307us/step - loss: 0.0671 - acc: 0.7703 - val_loss: 0.0782 - val_acc: 0.7666\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.07715\n",
      "Epoch 45/50\n",
      "290983/290983 [==============================] - 81s 279us/step - loss: 0.0670 - acc: 0.7712 - val_loss: 0.0782 - val_acc: 0.7667\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.07715\n",
      "Epoch 46/50\n",
      "290983/290983 [==============================] - 82s 283us/step - loss: 0.0670 - acc: 0.7706 - val_loss: 0.0781 - val_acc: 0.7661\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.07715\n",
      "Epoch 47/50\n",
      "290983/290983 [==============================] - 83s 284us/step - loss: 0.0670 - acc: 0.7707 - val_loss: 0.0782 - val_acc: 0.7662\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.07715\n",
      "Epoch 48/50\n",
      "290983/290983 [==============================] - 83s 284us/step - loss: 0.0669 - acc: 0.7707 - val_loss: 0.0782 - val_acc: 0.7663\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.07715\n",
      "Epoch 49/50\n",
      "290983/290983 [==============================] - 80s 276us/step - loss: 0.0669 - acc: 0.7711 - val_loss: 0.0782 - val_acc: 0.7659\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.07715\n",
      "Epoch 50/50\n",
      "290983/290983 [==============================] - 91s 313us/step - loss: 0.0668 - acc: 0.7700 - val_loss: 0.0782 - val_acc: 0.7659\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.07715\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', \n",
    "                      factor=0.2, \n",
    "                      patience=5, \n",
    "                      verbose=1, \n",
    "                      mode='auto', \n",
    "                      min_delta=0.0001, \n",
    "                      cooldown=0, \n",
    "                      min_lr=0),\n",
    "    ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True, verbose=1, mode='min')\n",
    "]\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "    loss=weighted_binary_crossentropy,\n",
    "    metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    "    batch_size=2000,\n",
    "    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn_2(model, input_vector, print_score = False):\n",
    "    \n",
    "    scores = model.predict(input_vector).reshape(100)\n",
    "    predictions1 = np.where(scores > 0.3)[0]\n",
    "    predictions2 = np.where(scores > 0.5)[0]\n",
    "    predictions3 = np.where(scores > 0.6)[0]\n",
    "    predictions4 = np.where(scores > 0.7)[0]\n",
    "    predictions5 = np.where(scores > 0.8)[0]\n",
    "    predictions6 = np.where(scores > 0.9)[0]\n",
    "    if print_score:\n",
    "        print(scores[predictions1])\n",
    "        print(scores[predictions2])\n",
    "        print(scores[predictions3])\n",
    "        print(scores[predictions4])\n",
    "        print(scores[predictions5])\n",
    "        print(scores[predictions6])\n",
    "    res1 = set(np.array(occupations)[predictions1])\n",
    "    res2 = set(np.array(occupations)[predictions2])\n",
    "    res3 = set(np.array(occupations)[predictions3])\n",
    "    res4 = set(np.array(occupations)[predictions4])\n",
    "    res5 = set(np.array(occupations)[predictions5])\n",
    "    res6 = set(np.array(occupations)[predictions6])\n",
    "    return res1, res2, res3, res4, res5, res6\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "=========================\n",
      "0.0117  :  0.0069\n",
      "0.0117  :  0.0079\n",
      "0.0117  :  0.0082\n",
      "0.0117  :  0.0085\n",
      "0.0117  :  0.0088\n",
      "0.0117  :  0.0093\n",
      "=========================\n",
      "0.0234  :  0.0139\n",
      "0.0234  :  0.0158\n",
      "0.0234  :  0.0165\n",
      "0.0234  :  0.0171\n",
      "0.0234  :  0.0176\n",
      "0.0234  :  0.0187\n",
      "=========================\n",
      "0.0351  :  0.0205\n",
      "0.0351  :  0.0234\n",
      "0.0351  :  0.0245\n",
      "0.0351  :  0.0256\n",
      "0.0351  :  0.0263\n",
      "0.0351  :  0.028\n",
      "=========================\n",
      "0.0467  :  0.0274\n",
      "0.0467  :  0.0312\n",
      "0.0467  :  0.0327\n",
      "0.0467  :  0.0342\n",
      "0.0467  :  0.0352\n",
      "0.0467  :  0.0375\n",
      "=========================\n",
      "0.0584  :  0.034\n",
      "0.0584  :  0.0387\n",
      "0.0584  :  0.0406\n",
      "0.0584  :  0.0425\n",
      "0.0584  :  0.0438\n",
      "0.0584  :  0.0467\n",
      "=========================\n",
      "0.0701  :  0.0408\n",
      "0.0701  :  0.0464\n",
      "0.0701  :  0.0487\n",
      "0.0701  :  0.0509\n",
      "0.0701  :  0.0526\n",
      "0.0701  :  0.0561\n",
      "=========================\n",
      "0.0818  :  0.048\n",
      "0.0818  :  0.0546\n",
      "0.0818  :  0.0572\n",
      "0.0818  :  0.0598\n",
      "0.0818  :  0.0617\n",
      "0.0818  :  0.0657\n",
      "=========================\n",
      "0.0935  :  0.0547\n",
      "0.0935  :  0.0623\n",
      "0.0935  :  0.0653\n",
      "0.0935  :  0.0683\n",
      "0.0935  :  0.0704\n",
      "0.0935  :  0.075\n",
      "=========================\n",
      "0.1052  :  0.0619\n",
      "0.1052  :  0.0702\n",
      "0.1052  :  0.0736\n",
      "0.1052  :  0.0769\n",
      "0.1052  :  0.0793\n",
      "0.1052  :  0.0846\n",
      "=========================\n",
      "0.1168  :  0.0687\n",
      "0.1168  :  0.0781\n",
      "0.1168  :  0.0818\n",
      "0.1168  :  0.0854\n",
      "0.1168  :  0.088\n",
      "0.1168  :  0.0938\n",
      "=========================\n",
      "0.1285  :  0.0758\n",
      "0.1285  :  0.086\n",
      "0.1285  :  0.0901\n",
      "0.1285  :  0.094\n",
      "0.1285  :  0.0968\n",
      "0.1285  :  0.1032\n",
      "=========================\n",
      "0.1402  :  0.0827\n",
      "0.1402  :  0.0938\n",
      "0.1402  :  0.0983\n",
      "0.1402  :  0.1026\n",
      "0.1402  :  0.1059\n",
      "0.1402  :  0.1127\n",
      "=========================\n",
      "0.1519  :  0.0895\n",
      "0.1519  :  0.1016\n",
      "0.1519  :  0.1065\n",
      "0.1519  :  0.1111\n",
      "0.1519  :  0.1147\n",
      "0.1519  :  0.122\n",
      "=========================\n",
      "0.1636  :  0.0966\n",
      "0.1636  :  0.1095\n",
      "0.1636  :  0.1149\n",
      "0.1636  :  0.1197\n",
      "0.1636  :  0.1236\n",
      "0.1636  :  0.1315\n",
      "=========================\n",
      "0.1753  :  0.1035\n",
      "0.1753  :  0.1173\n",
      "0.1753  :  0.1231\n",
      "0.1753  :  0.1283\n",
      "0.1753  :  0.1325\n",
      "0.1753  :  0.1409\n",
      "=========================\n",
      "0.187  :  0.1107\n",
      "0.187  :  0.1253\n",
      "0.187  :  0.1315\n",
      "0.187  :  0.137\n",
      "0.187  :  0.1416\n",
      "0.187  :  0.1505\n",
      "=========================\n",
      "0.1986  :  0.1175\n",
      "0.1986  :  0.133\n",
      "0.1986  :  0.1396\n",
      "0.1986  :  0.1455\n",
      "0.1986  :  0.1503\n",
      "0.1986  :  0.1599\n",
      "=========================\n",
      "0.2103  :  0.1249\n",
      "0.2103  :  0.1413\n",
      "0.2103  :  0.1482\n",
      "0.2103  :  0.1543\n",
      "0.2103  :  0.1594\n",
      "0.2103  :  0.1694\n",
      "=========================\n",
      "0.222  :  0.1313\n",
      "0.222  :  0.1486\n",
      "0.222  :  0.156\n",
      "0.222  :  0.1624\n",
      "0.222  :  0.1679\n",
      "0.222  :  0.1786\n",
      "=========================\n",
      "0.2337  :  0.1386\n",
      "0.2337  :  0.1568\n",
      "0.2337  :  0.1644\n",
      "0.2337  :  0.1713\n",
      "0.2337  :  0.1771\n",
      "0.2337  :  0.1882\n",
      "=========================\n",
      "0.2454  :  0.1458\n",
      "0.2454  :  0.1649\n",
      "0.2454  :  0.173\n",
      "0.2454  :  0.1802\n",
      "0.2454  :  0.1862\n",
      "0.2454  :  0.1979\n",
      "=========================\n",
      "0.2571  :  0.1538\n",
      "0.2571  :  0.1738\n",
      "0.2571  :  0.1821\n",
      "0.2571  :  0.1896\n",
      "0.2571  :  0.1959\n",
      "0.2571  :  0.208\n",
      "=========================\n",
      "0.2687  :  0.1608\n",
      "0.2687  :  0.1816\n",
      "0.2687  :  0.1903\n",
      "0.2687  :  0.1981\n",
      "0.2687  :  0.2048\n",
      "0.2687  :  0.2176\n",
      "=========================\n",
      "0.2804  :  0.1681\n",
      "0.2804  :  0.1897\n",
      "0.2804  :  0.1987\n",
      "0.2804  :  0.2069\n",
      "0.2804  :  0.2139\n",
      "0.2804  :  0.2272\n",
      "=========================\n",
      "0.2921  :  0.1755\n",
      "0.2921  :  0.1978\n",
      "0.2921  :  0.2072\n",
      "0.2921  :  0.2155\n",
      "0.2921  :  0.2228\n",
      "0.2921  :  0.2367\n",
      "=========================\n",
      "0.3038  :  0.183\n",
      "0.3038  :  0.206\n",
      "0.3038  :  0.2157\n",
      "0.3038  :  0.2244\n",
      "0.3038  :  0.232\n",
      "0.3038  :  0.2463\n",
      "=========================\n",
      "0.3155  :  0.1911\n",
      "0.3155  :  0.2147\n",
      "0.3155  :  0.2247\n",
      "0.3155  :  0.2336\n",
      "0.3155  :  0.2415\n",
      "0.3155  :  0.2562\n",
      "=========================\n",
      "0.3272  :  0.1987\n",
      "0.3272  :  0.223\n",
      "0.3272  :  0.2333\n",
      "0.3272  :  0.2425\n",
      "0.3272  :  0.2507\n",
      "0.3272  :  0.2659\n",
      "=========================\n",
      "0.3388  :  0.2065\n",
      "0.3388  :  0.2314\n",
      "0.3388  :  0.2419\n",
      "0.3388  :  0.2514\n",
      "0.3388  :  0.2598\n",
      "0.3388  :  0.2756\n",
      "=========================\n",
      "0.3505  :  0.2139\n",
      "0.3505  :  0.2395\n",
      "0.3505  :  0.2504\n",
      "0.3505  :  0.2602\n",
      "0.3505  :  0.269\n",
      "0.3505  :  0.2851\n",
      "=========================\n",
      "0.3622  :  0.2209\n",
      "0.3622  :  0.2472\n",
      "0.3622  :  0.2583\n",
      "0.3622  :  0.2685\n",
      "0.3622  :  0.2775\n",
      "0.3622  :  0.2942\n",
      "=========================\n",
      "0.3739  :  0.2286\n",
      "0.3739  :  0.2557\n",
      "0.3739  :  0.267\n",
      "0.3739  :  0.2775\n",
      "0.3739  :  0.2868\n",
      "0.3739  :  0.3039\n",
      "=========================\n",
      "0.3856  :  0.2355\n",
      "0.3856  :  0.2632\n",
      "0.3856  :  0.2749\n",
      "0.3856  :  0.2857\n",
      "0.3856  :  0.295\n",
      "0.3856  :  0.3125\n",
      "=========================\n",
      "0.3973  :  0.2406\n",
      "0.3973  :  0.2695\n",
      "0.3973  :  0.2816\n",
      "0.3973  :  0.2928\n",
      "0.3973  :  0.3022\n",
      "0.3973  :  0.3204\n",
      "=========================\n",
      "0.409  :  0.2467\n",
      "0.409  :  0.2766\n",
      "0.409  :  0.2891\n",
      "0.409  :  0.3005\n",
      "0.409  :  0.3101\n",
      "0.409  :  0.3287\n",
      "=========================\n",
      "0.4206  :  0.2531\n",
      "0.4206  :  0.2839\n",
      "0.4206  :  0.2968\n",
      "0.4206  :  0.3084\n",
      "0.4206  :  0.3179\n",
      "0.4206  :  0.337\n",
      "=========================\n",
      "0.4323  :  0.2597\n",
      "0.4323  :  0.2914\n",
      "0.4323  :  0.3047\n",
      "0.4323  :  0.3165\n",
      "0.4323  :  0.326\n",
      "0.4323  :  0.3456\n",
      "=========================\n",
      "0.444  :  0.2665\n",
      "0.444  :  0.299\n",
      "0.444  :  0.3126\n",
      "0.444  :  0.3247\n",
      "0.444  :  0.3343\n",
      "0.444  :  0.3542\n",
      "=========================\n",
      "0.4557  :  0.2732\n",
      "0.4557  :  0.3067\n",
      "0.4557  :  0.3206\n",
      "0.4557  :  0.3329\n",
      "0.4557  :  0.3427\n",
      "0.4557  :  0.3632\n",
      "=========================\n",
      "0.4674  :  0.2799\n",
      "0.4674  :  0.3142\n",
      "0.4674  :  0.3285\n",
      "0.4674  :  0.341\n",
      "0.4674  :  0.351\n",
      "0.4674  :  0.3719\n",
      "=========================\n",
      "0.4791  :  0.2866\n",
      "0.4791  :  0.3219\n",
      "0.4791  :  0.3366\n",
      "0.4791  :  0.3493\n",
      "0.4791  :  0.3594\n",
      "0.4791  :  0.3808\n",
      "=========================\n",
      "0.4907  :  0.2932\n",
      "0.4907  :  0.3293\n",
      "0.4907  :  0.3444\n",
      "0.4907  :  0.3574\n",
      "0.4907  :  0.3676\n",
      "0.4907  :  0.3895\n",
      "=========================\n",
      "0.5024  :  0.2998\n",
      "0.5024  :  0.3368\n",
      "0.5024  :  0.3522\n",
      "0.5024  :  0.3655\n",
      "0.5024  :  0.3759\n",
      "0.5024  :  0.3984\n",
      "=========================\n",
      "0.5141  :  0.3067\n",
      "0.5141  :  0.3445\n",
      "0.5141  :  0.3602\n",
      "0.5141  :  0.3739\n",
      "0.5141  :  0.3844\n",
      "0.5141  :  0.4074\n",
      "=========================\n",
      "0.5258  :  0.3134\n",
      "0.5258  :  0.352\n",
      "0.5258  :  0.3681\n",
      "0.5258  :  0.3821\n",
      "0.5258  :  0.393\n",
      "0.5258  :  0.4165\n",
      "=========================\n",
      "0.5375  :  0.3199\n",
      "0.5375  :  0.3595\n",
      "0.5375  :  0.376\n",
      "0.5375  :  0.3903\n",
      "0.5375  :  0.4015\n",
      "0.5375  :  0.4255\n",
      "=========================\n",
      "0.5492  :  0.3265\n",
      "0.5492  :  0.367\n",
      "0.5492  :  0.3841\n",
      "0.5492  :  0.3986\n",
      "0.5492  :  0.4101\n",
      "0.5492  :  0.4348\n",
      "=========================\n",
      "0.5609  :  0.333\n",
      "0.5609  :  0.3744\n",
      "0.5609  :  0.3918\n",
      "0.5609  :  0.4067\n",
      "0.5609  :  0.4184\n",
      "0.5609  :  0.4438\n",
      "=========================\n",
      "0.5725  :  0.34\n",
      "0.5725  :  0.3824\n",
      "0.5725  :  0.4002\n",
      "0.5725  :  0.4153\n",
      "0.5725  :  0.4273\n",
      "0.5725  :  0.4533\n",
      "=========================\n",
      "0.5842  :  0.3465\n",
      "0.5842  :  0.3898\n",
      "0.5842  :  0.408\n",
      "0.5842  :  0.4235\n",
      "0.5842  :  0.4358\n",
      "0.5842  :  0.4626\n",
      "=========================\n",
      "0.5959  :  0.3532\n",
      "0.5959  :  0.3973\n",
      "0.5959  :  0.416\n",
      "0.5959  :  0.4318\n",
      "0.5959  :  0.4444\n",
      "0.5959  :  0.4718\n",
      "=========================\n",
      "0.6076  :  0.36\n",
      "0.6076  :  0.405\n",
      "0.6076  :  0.4241\n",
      "0.6076  :  0.4402\n",
      "0.6076  :  0.4531\n",
      "0.6076  :  0.481\n",
      "=========================\n",
      "0.6193  :  0.3672\n",
      "0.6193  :  0.413\n",
      "0.6193  :  0.4325\n",
      "0.6193  :  0.4489\n",
      "0.6193  :  0.4621\n",
      "0.6193  :  0.4906\n",
      "=========================\n",
      "0.631  :  0.374\n",
      "0.631  :  0.4207\n",
      "0.631  :  0.4406\n",
      "0.631  :  0.4574\n",
      "0.631  :  0.4709\n",
      "0.631  :  0.5\n",
      "=========================\n",
      "0.6426  :  0.3811\n",
      "0.6426  :  0.4287\n",
      "0.6426  :  0.4489\n",
      "0.6426  :  0.4661\n",
      "0.6426  :  0.4798\n",
      "0.6426  :  0.5094\n",
      "=========================\n",
      "0.6543  :  0.388\n",
      "0.6543  :  0.4365\n",
      "0.6543  :  0.4571\n",
      "0.6543  :  0.4747\n",
      "0.6543  :  0.4887\n",
      "0.6543  :  0.5189\n",
      "=========================\n",
      "0.666  :  0.3949\n",
      "0.666  :  0.4443\n",
      "0.666  :  0.4652\n",
      "0.666  :  0.4832\n",
      "0.666  :  0.4975\n",
      "0.666  :  0.5283\n",
      "=========================\n",
      "0.6777  :  0.4018\n",
      "0.6777  :  0.4523\n",
      "0.6777  :  0.4737\n",
      "0.6777  :  0.4918\n",
      "0.6777  :  0.5064\n",
      "0.6777  :  0.5377\n",
      "=========================\n",
      "0.6894  :  0.4082\n",
      "0.6894  :  0.4596\n",
      "0.6894  :  0.4814\n",
      "0.6894  :  0.5\n",
      "0.6894  :  0.5149\n",
      "0.6894  :  0.5469\n",
      "=========================\n",
      "0.7011  :  0.4151\n",
      "0.7011  :  0.4675\n",
      "0.7011  :  0.4896\n",
      "0.7011  :  0.5086\n",
      "0.7011  :  0.5238\n",
      "0.7011  :  0.5564\n",
      "=========================\n",
      "0.7128  :  0.4219\n",
      "0.7128  :  0.4753\n",
      "0.7128  :  0.4978\n",
      "0.7128  :  0.5172\n",
      "0.7128  :  0.5328\n",
      "0.7128  :  0.5659\n",
      "=========================\n",
      "0.7244  :  0.4291\n",
      "0.7244  :  0.4834\n",
      "0.7244  :  0.5063\n",
      "0.7244  :  0.526\n",
      "0.7244  :  0.5419\n",
      "0.7244  :  0.5756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "0.7361  :  0.4358\n",
      "0.7361  :  0.491\n",
      "0.7361  :  0.5143\n",
      "0.7361  :  0.5344\n",
      "0.7361  :  0.5507\n",
      "0.7361  :  0.585\n",
      "=========================\n",
      "0.7478  :  0.4427\n",
      "0.7478  :  0.4989\n",
      "0.7478  :  0.5226\n",
      "0.7478  :  0.543\n",
      "0.7478  :  0.5597\n",
      "0.7478  :  0.5946\n",
      "=========================\n",
      "0.7595  :  0.4497\n",
      "0.7595  :  0.5068\n",
      "0.7595  :  0.5308\n",
      "0.7595  :  0.5516\n",
      "0.7595  :  0.5687\n",
      "0.7595  :  0.6041\n",
      "=========================\n",
      "0.7712  :  0.4566\n",
      "0.7712  :  0.5145\n",
      "0.7712  :  0.5389\n",
      "0.7712  :  0.56\n",
      "0.7712  :  0.5774\n",
      "0.7712  :  0.6134\n",
      "=========================\n",
      "0.7829  :  0.4634\n",
      "0.7829  :  0.5222\n",
      "0.7829  :  0.5471\n",
      "0.7829  :  0.5686\n",
      "0.7829  :  0.5862\n",
      "0.7829  :  0.6228\n",
      "=========================\n",
      "0.7945  :  0.4704\n",
      "0.7945  :  0.5301\n",
      "0.7945  :  0.5554\n",
      "0.7945  :  0.5772\n",
      "0.7945  :  0.5953\n",
      "0.7945  :  0.6323\n",
      "=========================\n",
      "0.8062  :  0.4777\n",
      "0.8062  :  0.5382\n",
      "0.8062  :  0.5639\n",
      "0.8062  :  0.586\n",
      "0.8062  :  0.6044\n",
      "0.8062  :  0.6419\n",
      "=========================\n",
      "0.8179  :  0.4846\n",
      "0.8179  :  0.546\n",
      "0.8179  :  0.5721\n",
      "0.8179  :  0.5946\n",
      "0.8179  :  0.6132\n",
      "0.8179  :  0.6512\n",
      "=========================\n",
      "0.8296  :  0.4919\n",
      "0.8296  :  0.5542\n",
      "0.8296  :  0.5806\n",
      "0.8296  :  0.6034\n",
      "0.8296  :  0.6224\n",
      "0.8296  :  0.6609\n",
      "=========================\n",
      "0.8413  :  0.4983\n",
      "0.8413  :  0.5615\n",
      "0.8413  :  0.5882\n",
      "0.8413  :  0.6115\n",
      "0.8413  :  0.6307\n",
      "0.8413  :  0.6699\n",
      "=========================\n",
      "0.853  :  0.505\n",
      "0.853  :  0.5691\n",
      "0.853  :  0.5962\n",
      "0.853  :  0.6198\n",
      "0.853  :  0.6393\n",
      "0.853  :  0.679\n",
      "=========================\n",
      "0.8646  :  0.5124\n",
      "0.8646  :  0.5773\n",
      "0.8646  :  0.6048\n",
      "0.8646  :  0.6288\n",
      "0.8646  :  0.6486\n",
      "0.8646  :  0.6887\n",
      "=========================\n",
      "0.8763  :  0.5205\n",
      "0.8763  :  0.5861\n",
      "0.8763  :  0.6138\n",
      "0.8763  :  0.638\n",
      "0.8763  :  0.6582\n",
      "0.8763  :  0.6988\n",
      "=========================\n",
      "0.888  :  0.5276\n",
      "0.888  :  0.594\n",
      "0.888  :  0.6222\n",
      "0.888  :  0.6467\n",
      "0.888  :  0.667\n",
      "0.888  :  0.7082\n",
      "=========================\n",
      "0.8997  :  0.5353\n",
      "0.8997  :  0.6025\n",
      "0.8997  :  0.631\n",
      "0.8997  :  0.6558\n",
      "0.8997  :  0.6764\n",
      "0.8997  :  0.718\n",
      "=========================\n",
      "0.9114  :  0.5427\n",
      "0.9114  :  0.6108\n",
      "0.9114  :  0.6396\n",
      "0.9114  :  0.6647\n",
      "0.9114  :  0.6855\n",
      "0.9114  :  0.7276\n",
      "=========================\n",
      "0.9231  :  0.5503\n",
      "0.9231  :  0.6191\n",
      "0.9231  :  0.6483\n",
      "0.9231  :  0.6736\n",
      "0.9231  :  0.6947\n",
      "0.9231  :  0.7374\n",
      "=========================\n",
      "0.9348  :  0.5582\n",
      "0.9348  :  0.6278\n",
      "0.9348  :  0.6572\n",
      "0.9348  :  0.6827\n",
      "0.9348  :  0.7041\n",
      "0.9348  :  0.7473\n",
      "=========================\n",
      "0.9464  :  0.5657\n",
      "0.9464  :  0.6358\n",
      "0.9464  :  0.6655\n",
      "0.9464  :  0.6913\n",
      "0.9464  :  0.713\n",
      "0.9464  :  0.7567\n",
      "=========================\n",
      "0.9581  :  0.5734\n",
      "0.9581  :  0.6442\n",
      "0.9581  :  0.6742\n",
      "0.9581  :  0.7003\n",
      "0.9581  :  0.7222\n",
      "0.9581  :  0.7663\n",
      "=========================\n",
      "0.9698  :  0.5809\n",
      "0.9698  :  0.6524\n",
      "0.9698  :  0.6827\n",
      "0.9698  :  0.7091\n",
      "0.9698  :  0.7313\n",
      "0.9698  :  0.7759\n",
      "=========================\n",
      "0.9815  :  0.5881\n",
      "0.9815  :  0.6604\n",
      "0.9815  :  0.6909\n",
      "0.9815  :  0.7176\n",
      "0.9815  :  0.7401\n",
      "0.9815  :  0.7852\n",
      "=========================\n",
      "0.9932  :  0.5958\n",
      "0.9932  :  0.6687\n",
      "0.9932  :  0.6996\n",
      "0.9932  :  0.7265\n",
      "0.9932  :  0.7492\n",
      "0.9932  :  0.7947\n",
      "(0.6002059655984983, 0.6734663012888759, 0.7044471950024948, 0.7315881716225514, 0.7544596149266123, 0.8000891664837851)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_nn_2(titles, input_vectors, occs, model):\n",
    "    nexample = len(titles)\n",
    "    accuracy1 = 0.\n",
    "    accuracy2 = 0.\n",
    "    accuracy3 = 0.\n",
    "    accuracy4 = 0.\n",
    "    accuracy5 = 0.\n",
    "    accuracy6 = 0.\n",
    "    prediction = None\n",
    "    for i in range(len(titles)):        \n",
    "        input_vector = input_vectors[i].reshape(1, -1)\n",
    "        prediction1, prediction2, prediction3, prediction4, prediction5, prediction6 = predict_nn_2(model, input_vector)\n",
    "        p1 = frozenset(prediction1)\n",
    "        p2 = frozenset(prediction2)\n",
    "        p3 = frozenset(prediction3)\n",
    "        p4 = frozenset(prediction4)\n",
    "        p5 = frozenset(prediction5)\n",
    "        p6 = frozenset(prediction6)\n",
    "        g = frozenset(occs[i])\n",
    "        accuracy1 += 1. / nexample * len(p1 & g) / len(p1 | g)\n",
    "        accuracy2 += 1. / nexample * len(p2 & g) / len(p2 | g)\n",
    "        accuracy3 += 1. / nexample * len(p3 & g) / len(p3 | g)\n",
    "        accuracy4 += 1. / nexample * len(p4 & g) / len(p4 | g)\n",
    "        accuracy5 += 1. / nexample * len(p5 & g) / len(p5 | g)\n",
    "        accuracy6 += 1. / nexample * len(p5 & g) / len(p6 | g)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"=========================\")\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy1, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy2, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy3, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy4, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy5, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy6, 4))\n",
    "    return accuracy1, accuracy2, accuracy3, accuracy4, accuracy5, accuracy6\n",
    "\n",
    "# print(evaluate_nn_2(titles_train, summaries_train, occs_train, model))\n",
    "print(evaluate_nn_2(titles_train_test, data_test, occs_train_test, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> With threshold of **0.9**, I had the test accuracy of **0.8000891664837851**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn_3(model, input_vector, print_score = False):\n",
    "    \n",
    "    scores = model.predict(input_vector).reshape(100)\n",
    "    predictions = np.where(scores > 0.9)[0]\n",
    "    res = set(np.array(occupations)[predictions])\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "643107 / 643108  -  99.999844505122 %\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "\n",
    "sequences_res = tokenizer.texts_to_sequences(summaries_test)\n",
    "data_res = pad_sequences(sequences_res, maxlen=maxlen)\n",
    "\n",
    "def export(start=0):\n",
    "    with gzip.open('results.json.gz', 'wt') as output:\n",
    "        for i in range(start, len(titles_test)):    \n",
    "            input_vector = data_res[i].reshape(1, -1)\n",
    "            prediction = predict_nn_3(model, input_vector)\n",
    "            sol = list(prediction)            \n",
    "            output.write(json.dumps({'title':titles_test[i], 'prediction': sol}) + \"\\n\")\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            print(i,\"/\", len(titles_test), \" - \", i * 100 / len(titles_test), \"%\")\n",
    "\n",
    "export()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Model Architecture: ***CNN + BiRNN***\n",
    "- loss: ***weighted_binary_crossentropy*** - Give more weight to the 1-label\n",
    "- Test Accuracy: ***0.8000891664837851***\n",
    "- Threshold: ***0.9\n",
    "- Using the most **20,000** common words in the dataset.\n",
    "- Word Embedding: **Glove with 400,000 vocabs** (https://nlp.stanford.edu/projects/glove/). \n",
    "\n",
    "## Some Experiments to try to increase the accuracy but did not work\n",
    "- I tested with other word embeddings like: word2vec, fasttext but the Glove still had the highest accuracy.\n",
    "- I tried to increase and decrease the number of convolutional layers to 1 and 4 but the test accuracy dropped. 3 seems to be the optimal number.\n",
    "- I also varied the kernel size of the convolutional and max_pooling layers but the test accuracy dropped.\n",
    "- More dense layers and more RNN layers also did not help while the running time increase exponentially.\n",
    "- However, the accuracy increase a lot when I increase the size of each RNN layer. Due to computational limitation I cannot test with bigger GRU layers.\n",
    "- I increased the number of common words to 50,000 100,000 and 400,000 with bigger word vector vocab but the accuracy also dropped. This seems not intuitively because I think more words will result in better accuracy.\n",
    "- I tried the jaccard_distance loss function but the model cannot learn.\n",
    "- I tried to train the embedding layer but the result was not good.\n",
    "- I also changed the number of words per summary to 100 and 500 but the accuracy is not better.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
