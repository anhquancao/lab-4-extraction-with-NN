{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import\n",
    "## Question 0 - Get common wikidata occupations\n",
    "\n",
    "> Write a sparql query that retrieves the top 100 occupations on wikidata (wikidata property P106).\n",
    "\n",
    "You may use the interface https://query.wikidata.org/ to try different queries. Here are some example sparql queries: https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT ?o (COUNT(?person) AS ?count) WHERE \n",
    "{\n",
    "   ?person wdt:P106 ?o\n",
    "}\n",
    "GROUP BY ?o\n",
    "ORDER BY DESC(?count)\n",
    "LIMIT 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following assertion should pass if your answer is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "occupations = ['Q82955', 'Q937857', 'Q36180', 'Q33999', 'Q1650915', 'Q1028181', 'Q1930187', 'Q177220', 'Q1622272', 'Q49757', 'Q36834', 'Q40348', 'Q47064', 'Q639669', 'Q10800557', 'Q201788', 'Q2526255', 'Q43845', 'Q28389', 'Q42973', 'Q10871364', 'Q39631', 'Q193391', 'Q482980', 'Q483501', 'Q11513337', 'Q3665646', 'Q12299841', 'Q19204627', 'Q16533', 'Q81096', 'Q11774891', 'Q188094', 'Q1281618', 'Q333634', 'Q189290', 'Q250867', 'Q33231', 'Q2259451', 'Q42603', 'Q628099', 'Q37226', 'Q2309784', 'Q901', 'Q2066131', 'Q6625963', 'Q10798782', 'Q2374149', 'Q170790', 'Q4610556', 'Q185351', 'Q486748', 'Q3055126', 'Q753110', 'Q4964182', 'Q169470', 'Q158852', 'Q1234713', 'Q14089670', 'Q10873124', 'Q3282637', 'Q593644', 'Q947873', 'Q13414980', 'Q131524', 'Q11338576', 'Q15117302', 'Q488205', 'Q14467526', 'Q183945', 'Q10843402', 'Q13382576', 'Q13141064', 'Q214917', 'Q855091', 'Q644687', 'Q19595175', 'Q121594', 'Q2865819', 'Q16010345', 'Q1231865', 'Q2405480', 'Q350979', 'Q3400985', 'Q13365117', 'Q10833314', 'Q3621491', 'Q15981151', 'Q212980', 'Q16145150', 'Q1792450', 'Q15296811', 'Q15627169', 'Q2306091', 'Q4263842', 'Q806798', 'Q5716684', 'Q2516866', 'Q3387717', 'Q131512']\n",
    "\n",
    "def evalSparql(query):\n",
    "    return requests.post('https://query.wikidata.org/sparql', data=query, headers={\n",
    "        'content-type': 'application/sparql-query',\n",
    "        'accept': 'application/json',\n",
    "        'user-agent': 'User:Tpt'\n",
    "    }).json()['results']['bindings']\n",
    "\n",
    "myOccupations = [val['o']['value'].replace('http://www.wikidata.org/entity/', '') \n",
    "                 for val in evalSparql(query)]\n",
    "assert(frozenset(occupations) == frozenset(myOccupations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupations labels\n",
    "\n",
    "We load the labels of the occupations from Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q82955': 'politician', 'Q121594': 'professor', 'Q177220': 'singer', 'Q169470': 'physicist', 'Q170790': 'mathematician', 'Q81096': 'engineer', 'Q201788': 'historian', 'Q188094': 'economist', 'Q212980': 'psychologist', 'Q214917': 'playwright', 'Q131524': 'entrepreneur', 'Q183945': 'record producer', 'Q193391': 'diplomat', 'Q189290': 'military officer', 'Q185351': 'jurist', 'Q350979': 'zoologist', 'Q483501': 'artist', 'Q482980': 'author', 'Q333634': 'translator', 'Q158852': 'conductor', 'Q486748': 'pianist', 'Q488205': 'singer-songwriter', 'Q250867': 'Catholic priest', 'Q593644': 'chemist', 'Q639669': 'musician', 'Q644687': 'illustrator', 'Q628099': 'association football manager', 'Q855091': 'guitarist', 'Q937857': 'association football player', 'Q947873': 'television presenter', 'Q806798': 'banker', 'Q1028181': 'painter', 'Q753110': 'songwriter', 'Q1234713': 'theologian', 'Q1281618': 'sculptor', 'Q1622272': 'university teacher', 'Q1792450': 'art historian', 'Q1650915': 'researcher', 'Q1930187': 'journalist', 'Q2306091': 'sociologist', 'Q2374149': 'botanist', 'Q2526255': 'film director', 'Q2516866': 'publisher', 'Q2066131': 'athlete', 'Q2405480': 'voice actor', 'Q1231865': 'pedagogue', 'Q2865819': 'opera singer', 'Q2259451': 'stage actor', 'Q3282637': 'film producer', 'Q3387717': 'theatre director', 'Q3055126': 'entomologist', 'Q3400985': 'academic', 'Q3665646': 'basketball player', 'Q3621491': 'archaeologist', 'Q4610556': 'model', 'Q4263842': 'literary critic', 'Q4964182': 'philosopher', 'Q5716684': 'dancer', 'Q6625963': 'novelist', 'Q10843402': 'swimmer', 'Q10833314': 'tennis player', 'Q10871364': 'baseball player', 'Q10798782': 'television actor', 'Q10873124': 'chess player', 'Q10800557': 'film actor', 'Q11338576': 'boxer', 'Q11513337': 'athletics competitor', 'Q2309784': 'sport cyclist', 'Q11774891': 'ice hockey player', 'Q12299841': 'cricketer', 'Q13141064': 'badminton player', 'Q13365117': 'handball player', 'Q13414980': 'Australian rules footballer', 'Q14089670': 'rugby union player', 'Q14467526': 'linguist', 'Q15117302': 'volleyball player', 'Q15627169': 'trade unionist', 'Q15981151': 'jazz musician', 'Q16010345': 'performer', 'Q131512': 'agriculturer', 'Q13382576': 'rower', 'Q19204627': 'American football player', 'Q15296811': 'drawer', 'Q19595175': 'amateur wrestler', 'Q16145150': 'music pedagogue', 'Q901': 'scientist', 'Q33999': 'actor', 'Q33231': 'photographer', 'Q36834': 'composer', 'Q16533': 'judge', 'Q40348': 'lawyer', 'Q36180': 'writer', 'Q42973': 'architect', 'Q43845': 'businessperson', 'Q39631': 'physician', 'Q28389': 'screenwriter', 'Q42603': 'priest', 'Q49757': 'poet', 'Q37226': 'teacher', 'Q47064': 'military personnel'}\n"
     ]
    }
   ],
   "source": [
    "occupations_label = {}\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT ?o ?oLabel \n",
    "WHERE { \n",
    "    VALUES ?o { %s } \n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\"\"\"% ' '.join('wd:' + o for o in occupations)\n",
    "\n",
    "for result in evalSparql(query):\n",
    "    occupations_label[result['o']['value'].replace('http://www.wikidata.org/entity/', '')] = result['oLabel']['value']\n",
    "\n",
    "print(occupations_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load *all* the labels of the occupations from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q82955': ['politician', 'political leader', 'polit.', 'political figure'], 'Q121594': ['professor', 'Prof.'], 'Q177220': ['singer', 'vocalist'], 'Q169470': ['physicist'], 'Q170790': ['mathematician'], 'Q81096': ['engineer'], 'Q201788': ['historian', 'historiographer', 'historians'], 'Q188094': ['economist'], 'Q212980': ['psychologist'], 'Q214917': ['playwright', 'scriptwriter', 'dramatist', 'Playwright, dramatist', 'playwrite'], 'Q131524': ['entrepreneur'], 'Q183945': ['record producer', 'music producer'], 'Q193391': ['diplomat'], 'Q189290': ['military officer', 'officer', 'army officer'], 'Q185351': ['jurist'], 'Q350979': ['zoologist', 'zooligist'], 'Q483501': ['artist'], 'Q482980': ['author'], 'Q333634': ['translator'], 'Q158852': ['conductor', 'Conducting'], 'Q486748': ['pianist'], 'Q488205': ['singer-songwriter', 'singer songwriter', 'singer/songwriter', 'singersongwriter'], 'Q250867': ['Catholic priest', 'Roman Catholic priest', 'Catholic presbyter', 'Roman Catholic presbyter'], 'Q593644': ['chemist', 'chemists'], 'Q639669': ['musician'], 'Q644687': ['illustrator'], 'Q628099': ['association football manager', 'football manager', 'soccer coach', 'association football coach', 'football coach', 'soccer manager'], 'Q855091': ['guitarist', 'guitar player'], 'Q937857': ['association football player', 'football player', 'footballer', 'association footballer', 'soccer player'], 'Q947873': ['television presenter', 'TV host', 'host', 'hostess', 'TV presenter', 'television personality', 'TV personality', 'television host'], 'Q806798': ['banker', 'Private Banker', 'private sector banker'], 'Q1028181': ['painter'], 'Q753110': ['songwriter', 'song writer'], 'Q1234713': ['theologian', 'religious scholar'], 'Q1281618': ['sculptor'], 'Q1622272': ['university teacher', 'lecturer', 'college lecturer', 'college professor', 'university teachers'], 'Q1792450': ['art historian'], 'Q1650915': ['researcher'], 'Q1930187': ['journalist', 'journo'], 'Q2306091': ['sociologist'], 'Q2374149': ['botanist', 'botany', 'plant scientist'], 'Q2526255': ['film director', 'director', 'movie director'], 'Q2516866': ['publisher'], 'Q2066131': ['athlete', 'sportsperson', 'sportsman', 'sportswoman'], 'Q2405480': ['voice actor', 'voice actress', 'voice artist'], 'Q1231865': ['pedagogue', 'educationalist'], 'Q2865819': ['opera singer'], 'Q2259451': ['stage actor', 'stage actress', 'theater actor', 'theater actress', 'theatre actor', 'theatre actress'], 'Q3282637': ['film producer', 'producer', 'movie producer'], 'Q3387717': ['theatre director', 'stage director', 'theater director'], 'Q3055126': ['entomologist'], 'Q3400985': ['academic', 'college graduates', 'university graduates'], 'Q3665646': ['basketball player', 'professional basketball player', 'basketballer'], 'Q3621491': ['archaeologist', 'archeologist'], 'Q4610556': ['model', 'fashion model', 'mannequin'], 'Q4263842': ['literary critic', 'book critic', 'literary critique'], 'Q4964182': ['philosopher'], 'Q5716684': ['dancer'], 'Q6625963': ['novelist'], 'Q10843402': ['swimmer'], 'Q10833314': ['tennis player'], 'Q10871364': ['baseball player'], 'Q10798782': ['television actor', 'actress', 'actor', 'TV actor', 'television actress', 'TV actress'], 'Q10873124': ['chess player'], 'Q10800557': ['film actor', 'film actress', 'movie actor', 'movie actress'], 'Q11338576': ['boxer', 'pugilist'], 'Q11513337': ['athletics competitor', 'track and field athlete', 'athlete (restricted sense)'], 'Q2309784': ['sport cyclist', 'racing cyclist', 'sport bicyclist', 'sport biker'], 'Q11774891': ['ice hockey player', 'hockey player'], 'Q12299841': ['cricketer', 'cricket player'], 'Q13141064': ['badminton player'], 'Q13365117': ['handball player', 'handballer'], 'Q13414980': ['Australian rules footballer', 'Australian footballer', 'Australian rules football player', 'Australian-rules football player'], 'Q14089670': ['rugby union player'], 'Q14467526': ['linguist', 'linguistic scholar'], 'Q15117302': ['volleyball player', 'volleyballer'], 'Q15627169': ['trade unionist', 'labor unionist', 'labour unionist'], 'Q15981151': ['jazz musician'], 'Q16010345': ['performer', 'performing artist', 'scenic artist'], 'Q131512': ['agriculturer', 'farmer', 'agriculturist', 'cultivator', 'grower', 'raiser'], 'Q13382576': ['rower', 'oarsman', 'oarswoman'], 'Q19204627': ['American football player', 'football player'], 'Q15296811': ['drawer', 'illustrator', 'draftsperson', 'draughtsperson', 'draftsman', 'drafter', 'draftswoman', 'draughtsman'], 'Q19595175': ['amateur wrestler', 'wrestler'], 'Q16145150': ['music pedagogue', 'music teacher'], 'Q901': ['scientist', 'natural philosopher'], 'Q33999': ['actor', 'actresses', 'actress', 'actors'], 'Q33231': ['photographer'], 'Q36834': ['composer'], 'Q16533': ['judge', 'magistrate', 'justice', 'judges', 'justices'], 'Q40348': ['lawyer', 'attorney', 'Jurisprudente'], 'Q36180': ['writer', 'author', 'writers', 'authors'], 'Q42973': ['architect'], 'Q43845': ['businessperson', 'business man', 'business person', 'business woman', 'businessman', 'businesswoman', 'dealer'], 'Q39631': ['physician', 'physicians'], 'Q28389': ['screenwriter', 'scenarist', 'scriptwriter', 'writer', 'film writer', 'screen writer', 'tv writer', 'script writer'], 'Q42603': ['priest', 'priestess', 'reverend'], 'Q49757': ['poet', 'bard', 'poetess'], 'Q37226': ['teacher', 'professor', 'educator', 'schoolmaster', 'schoolmistress', 'school teacher'], 'Q47064': ['military personnel']}\n"
     ]
    }
   ],
   "source": [
    "occupations_labels = {k: [v] for k, v in occupations_label.items()}\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT ?o ?altLabel \n",
    "WHERE {\n",
    "  VALUES ?o { %s }\n",
    "  ?o skos:altLabel ?altLabel . FILTER (lang(?altLabel) = \"en\")\n",
    "}\"\"\" % ' '.join('wd:' + o for o in occupations) \n",
    "\n",
    "for result in evalSparql(query):\n",
    "    occupations_labels[result['o']['value'].replace('http://www.wikidata.org/entity/', '')].append(result['altLabel']['value'])\n",
    "\n",
    "print(occupations_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia articles\n",
    "\n",
    "Here we load the training and the testing sets. To save memory space we use a generator that will read the file each time we iterate over the training or the testing examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "def loadJson(filename):\n",
    "    with gzip.open(filename, 'rt') as fp:\n",
    "        for line in fp:\n",
    "            yield json.loads(line)\n",
    "\n",
    "class MakeIter(object):\n",
    "    def __init__(self, generator_func, **kwargs):\n",
    "        self.generator_func = generator_func\n",
    "        self.kwargs = kwargs\n",
    "    def __iter__(self):\n",
    "        return self.generator_func(**self.kwargs)\n",
    "\n",
    "training_set = MakeIter(loadJson, filename='wiki-train.json.gz')\n",
    "testing_set = MakeIter(loadJson, filename='wiki-test.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract occupations from summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Dictionnary extraction\n",
    "\n",
    "> Using ```occupations_labels``` dictionnary, identify all occupations for each articles. Complete the function below to evaluate the accuracy of such approach. It will serve as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4842586814146957"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_occ = dict()\n",
    "for key, occs in occupations_labels.items():\n",
    "    for occ in occs:\n",
    "        label_to_occ[occ.lower()] = key\n",
    "\n",
    "def predict_dictionnary(example, occupations_labels):\n",
    "    occs = []\n",
    "    summary = example['summary'].lower()\n",
    "    labels = label_to_occ.keys()\n",
    "    for label in labels:\n",
    "        if label in summary:\n",
    "            occs.append(label_to_occ[label])\n",
    "    return occs\n",
    "    \n",
    "def evaluate_dictionnary(training_set, occupations_labels):\n",
    "    nexample = 0\n",
    "    accuracy = 0.\n",
    "    prediction = None\n",
    "    for example in training_set:\n",
    "        prediction = predict_dictionnary(example, occupations_labels)\n",
    "        p = frozenset(prediction)\n",
    "        g = frozenset(example['occupations'])\n",
    "        accuracy += 1.*len(p & g) / len(p | g)\n",
    "        nexample += 1\n",
    "    return accuracy / nexample\n",
    "\n",
    "evaluate_dictionnary(training_set, occupations_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Simple neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the articles \"summary\" and we take the average of the word vectors.\n",
    "This is done with spacy loaded with the fast text vectors.\n",
    "To do the installation/loading [takes 8-10 minutes, dl 1.2Go]\n",
    "```\n",
    "pip3 install spacy\n",
    "wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/cc.en.300.vec.gz\n",
    "python3 -m spacy init-model en /tmp/en_vectors_wiki_lg --vectors-loc cc.en.300.vec.gz\n",
    "rm cc.en.300.vec.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nlp = spacy.load('/tmp/en_vectors_wiki_lg')\n",
    "\n",
    "def vectorize(dataset, nlp):\n",
    "    result = {}\n",
    "    for example in dataset:\n",
    "        doc = nlp(example['summary'], disable=['parser', 'tagger'])\n",
    "        result[example['title']] = {}\n",
    "        result[example['title']]['vector'] = doc.vector\n",
    "        result[example['title']]['summary'] = example['summary']\n",
    "        if 'occupations' in example:\n",
    "            result[example['title']]['occupations'] = example['occupations']\n",
    "    return result\n",
    "    \n",
    "vectorized_training = vectorize(training_set, nlp)\n",
    "vectorized_testing = vectorize(testing_set, nlp)\n",
    "nlp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427798"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.45162819e-02 -2.45802402e-02 -4.59302496e-03 -4.09372151e-02\n",
      " -4.47662771e-02 -4.18604538e-03 -3.15232435e-03 -1.44802360e-02\n",
      " -1.68499984e-02 -3.69651243e-03 -1.16255814e-02  1.43651171e-02\n",
      "  2.02674349e-03 -5.88953542e-03 -2.17011590e-02  1.02302311e-02\n",
      " -2.49313917e-02 -5.65232616e-03 -2.25581434e-02  8.29069968e-03\n",
      " -1.44069805e-03  2.25197673e-02 -6.81395701e-04 -1.37232570e-02\n",
      " -1.26674427e-02 -3.35569866e-02  1.10627888e-02 -2.37208814e-03\n",
      " -2.30000000e-02  7.58616179e-02 -5.03487710e-04 -2.51116175e-02\n",
      "  9.26511642e-03 -2.52558179e-02 -1.51058156e-02 -9.51627828e-03\n",
      "  1.17523270e-02  1.22441910e-03  1.08139520e-03  3.39302444e-03\n",
      "  2.20116391e-03  1.46860480e-02 -1.43686021e-02  5.76395402e-03\n",
      "  1.74162779e-02 -4.76220921e-02 -1.72569733e-02 -1.49988411e-02\n",
      " -1.77732538e-02  1.58907007e-02 -7.23255938e-03  2.43825577e-02\n",
      " -2.73104683e-02 -3.67430188e-02 -1.48802334e-02 -1.34825567e-02\n",
      " -3.14348824e-02  1.95930228e-02 -6.68605033e-04 -9.24302172e-03\n",
      "  1.56976283e-04 -1.65674444e-02 -1.30372085e-02  6.16298130e-05\n",
      " -3.63139645e-03  2.74534873e-03 -1.62697677e-02 -4.70697694e-03\n",
      "  5.48139494e-03  4.39302297e-03  4.65523303e-02  2.29872130e-02\n",
      "  2.72058025e-02 -5.52790612e-03  2.19720937e-02 -4.41581383e-02\n",
      "  1.33255811e-03  1.20244222e-02  3.49267460e-02  3.76593024e-02\n",
      "  8.65232572e-03 -6.52325572e-03 -1.90407019e-02  1.03569757e-02\n",
      "  1.09301973e-03 -6.28488278e-03  3.98965068e-02 -3.81744131e-02\n",
      " -1.35965087e-02  1.74023230e-02 -1.48686031e-02  5.78604685e-03\n",
      " -8.59186146e-03  4.74418374e-03  1.54720917e-02 -6.42325589e-03\n",
      " -1.58430226e-02 -2.98779178e-02 -1.54255824e-02  3.28209326e-02\n",
      "  2.43825577e-02  1.32907031e-03  1.80883706e-02 -2.72825565e-02\n",
      "  9.28488653e-03 -7.39418622e-03 -7.98023026e-03  1.84244160e-02\n",
      " -9.45350039e-04 -1.16825579e-02  1.15813862e-03 -2.10464321e-04\n",
      " -3.00813979e-03  4.75407019e-02 -8.32790602e-03  4.11511678e-03\n",
      " -1.25604663e-02  8.92209262e-03  7.64534995e-03 -2.65965052e-02\n",
      "  6.58837147e-03 -1.12011610e-02 -9.68022924e-03  1.60023291e-02\n",
      "  1.61629519e-04  3.20906974e-02 -1.59848798e-02  1.14162825e-02\n",
      " -2.40430199e-02  5.39906919e-02 -4.80814092e-03  3.02209193e-03\n",
      "  5.89418598e-03 -3.94418649e-03 -2.68058274e-02 -8.98256153e-03\n",
      " -2.94616278e-02  3.90697829e-03  4.68255766e-03  3.96162830e-03\n",
      " -2.68069748e-02 -2.68395394e-02 -9.76740339e-05  5.67557989e-03\n",
      "  4.43197712e-02 -1.38953477e-02 -3.69888335e-01  1.04639539e-02\n",
      "  1.55372089e-02 -1.35093015e-02 -8.09988379e-02  2.67802346e-02\n",
      "  2.21941881e-02 -7.86627829e-03 -1.00313956e-02  1.52511625e-02\n",
      "  1.45744160e-01  4.61395411e-03  7.26162829e-03  3.14453505e-02\n",
      " -7.95465056e-03 -1.25395320e-02  6.95348764e-03 -2.48023286e-03\n",
      "  6.17325725e-03  1.26546472e-02  1.03558144e-02 -1.21616265e-02\n",
      " -1.27907039e-03 -1.99348871e-02 -9.01860371e-03  4.25581448e-03\n",
      "  7.45790750e-02  1.02186035e-02 -9.93953645e-03  1.72848776e-02\n",
      " -1.03779081e-02  1.46616297e-02 -3.75465187e-03 -2.26953458e-02\n",
      "  5.36046689e-04  6.64511696e-02 -2.53790785e-02  5.80627881e-02\n",
      " -1.42732579e-02  9.22453254e-02 -1.12825576e-02 -2.51837187e-02\n",
      "  3.90697736e-03  5.96395321e-03 -3.02476659e-02  2.63883732e-02\n",
      " -1.69488378e-02  7.39418576e-03  1.60662793e-02 -1.68313961e-02\n",
      " -8.25814065e-03 -1.36965141e-02  7.30697624e-03  1.63453538e-02\n",
      " -4.15407047e-02  1.05633713e-01  1.53325591e-02  6.63023209e-03\n",
      "  3.93279046e-02 -1.27697680e-02 -5.95697621e-03 -8.67441762e-03\n",
      "  1.58593040e-02  9.42093134e-03 -4.15697647e-03  1.34639572e-02\n",
      " -4.10383604e-02 -2.82325619e-03 -2.43790708e-02 -4.02325485e-03\n",
      "  1.65058132e-02  4.21395432e-03  1.25813941e-02  1.64744183e-02\n",
      " -2.81162816e-03  1.34813897e-02 -8.19302350e-03 -7.04767322e-03\n",
      "  1.67139638e-02  1.43581396e-02  1.20023256e-02  4.96162800e-03\n",
      "  1.76325571e-02 -7.07674446e-03 -4.24197726e-02 -2.34697610e-02\n",
      " -1.86058115e-02 -2.32790736e-03  2.98906974e-02  1.53604464e-03\n",
      "  1.95941851e-02 -2.67104693e-02 -1.12453466e-02 -2.54534930e-03\n",
      " -4.29302268e-03  3.56558077e-02 -4.36046888e-04 -8.16406980e-02\n",
      "  5.04779041e-01 -2.18813960e-02  1.15883695e-02  2.14848872e-02\n",
      "  7.80581404e-03  1.55116236e-02 -1.11523261e-02  4.61628864e-04\n",
      "  1.72918607e-02  1.43034859e-02  2.05546506e-02 -8.23488459e-03\n",
      " -3.16290706e-02 -4.83953534e-03 -1.82697661e-02  2.02907110e-03\n",
      " -3.51163093e-04  1.10220918e-02 -8.54755938e-02 -2.68255756e-03\n",
      "  1.83174424e-02  1.91116314e-02 -4.73488262e-03 -8.08255840e-03\n",
      "  1.37906978e-02 -7.76046468e-03 -2.82767452e-02 -2.99069774e-03\n",
      "  1.06569799e-02 -5.99999772e-03  1.11883730e-02  4.28720983e-03\n",
      " -3.12255807e-02 -8.07186142e-02  8.59302282e-03 -8.11744668e-03\n",
      " -5.36279054e-03  1.87046509e-02 -1.10972092e-01 -3.07988375e-02\n",
      "  9.47441999e-03 -1.03662787e-02  1.16337193e-02  3.22093032e-02\n",
      " -2.69790720e-02  2.25430205e-02 -1.49802361e-02 -1.05290683e-02\n",
      " -4.36534919e-02  6.34883530e-04 -2.83197612e-02 -1.37674408e-02\n",
      " -1.50220934e-02  1.30851150e-01 -1.22430259e-02  2.38767453e-02]\n"
     ]
    }
   ],
   "source": [
    "v = vectorized_training['George_Washington']['vector']\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the vectorized_training into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDict(d, percent):\n",
    "    split_idx = int(len(d) * percent)\n",
    "    d1 = dict(list(d.items())[: split_idx])\n",
    "    d2 = dict(list(d.items())[split_idx:])                \n",
    "\n",
    "    return d1, d2\n",
    "\n",
    "vectorized_training_test, vectorized_training_train = splitDict(vectorized_training, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342239"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_training_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We encode the data\n",
    "import numpy as np\n",
    "\n",
    "def encode_data(vectorized_data):\n",
    "    X = np.array([vectorized_data[article]['vector'] for article in vectorized_data])\n",
    "    y = np.array([[(1 if occupation in vectorized_data[article]['occupations'] else 0)\n",
    "                        for occupation in occupations ] for article in vectorized_data])\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = encode_data(vectorized_training_train)\n",
    "X_test, y_test = encode_data(vectorized_training_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342239, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342239, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using keras, define a sequential neural network with two layers. Use categorical_crossentropy as a loss function and softmax as the activation function of the output layer\n",
    "\n",
    "You can look into the documentation here: https://keras.io/getting-started/sequential-model-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=300))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "\n",
    "optimizer = Adam()\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 308015 samples, validate on 34224 samples\n",
      "Epoch 1/50\n",
      "308015/308015 [==============================] - 3s 10us/step - loss: 3.5608 - acc: 0.4881 - val_loss: 2.0247 - val_acc: 0.6362\n",
      "Epoch 2/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 2.3868 - acc: 0.6581 - val_loss: 1.7224 - val_acc: 0.6907\n",
      "Epoch 3/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 2.1847 - acc: 0.6880 - val_loss: 1.6056 - val_acc: 0.7125\n",
      "Epoch 4/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 2.0853 - acc: 0.7033 - val_loss: 1.5498 - val_acc: 0.7234\n",
      "Epoch 5/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 2.0255 - acc: 0.7115 - val_loss: 1.5037 - val_acc: 0.7321\n",
      "Epoch 6/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.9833 - acc: 0.7176 - val_loss: 1.4793 - val_acc: 0.7359\n",
      "Epoch 7/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.9517 - acc: 0.7213 - val_loss: 1.4538 - val_acc: 0.7409\n",
      "Epoch 8/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.9250 - acc: 0.7251 - val_loss: 1.4335 - val_acc: 0.7466\n",
      "Epoch 9/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.9036 - acc: 0.7273 - val_loss: 1.4188 - val_acc: 0.7452\n",
      "Epoch 10/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8846 - acc: 0.7293 - val_loss: 1.4096 - val_acc: 0.7491\n",
      "Epoch 11/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8676 - acc: 0.7324 - val_loss: 1.3920 - val_acc: 0.7537\n",
      "Epoch 12/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8522 - acc: 0.7333 - val_loss: 1.3919 - val_acc: 0.7550\n",
      "Epoch 13/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8387 - acc: 0.7349 - val_loss: 1.3776 - val_acc: 0.7560\n",
      "Epoch 14/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8256 - acc: 0.7372 - val_loss: 1.3729 - val_acc: 0.7587\n",
      "Epoch 15/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8138 - acc: 0.7383 - val_loss: 1.3733 - val_acc: 0.7553\n",
      "Epoch 16/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8025 - acc: 0.7391 - val_loss: 1.3637 - val_acc: 0.7564\n",
      "Epoch 17/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 1.7916 - acc: 0.7408 - val_loss: 1.3510 - val_acc: 0.7603\n",
      "Epoch 18/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 1.7818 - acc: 0.7417 - val_loss: 1.3484 - val_acc: 0.7622\n",
      "Epoch 19/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7721 - acc: 0.7434 - val_loss: 1.3472 - val_acc: 0.7599\n",
      "Epoch 20/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 1.7634 - acc: 0.7436 - val_loss: 1.3448 - val_acc: 0.7615\n",
      "Epoch 21/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7554 - acc: 0.7450 - val_loss: 1.3347 - val_acc: 0.7649\n",
      "Epoch 22/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7462 - acc: 0.7463 - val_loss: 1.3367 - val_acc: 0.7639\n",
      "Epoch 23/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7388 - acc: 0.7467 - val_loss: 1.3338 - val_acc: 0.7641\n",
      "Epoch 24/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7308 - acc: 0.7481 - val_loss: 1.3379 - val_acc: 0.7646\n",
      "Epoch 25/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7240 - acc: 0.7486 - val_loss: 1.3341 - val_acc: 0.7644\n",
      "Epoch 26/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7173 - acc: 0.7494 - val_loss: 1.3279 - val_acc: 0.7634\n",
      "Epoch 27/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7097 - acc: 0.7500 - val_loss: 1.3268 - val_acc: 0.7610\n",
      "Epoch 28/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7030 - acc: 0.7510 - val_loss: 1.3203 - val_acc: 0.7664\n",
      "Epoch 29/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6968 - acc: 0.7518 - val_loss: 1.3266 - val_acc: 0.7640\n",
      "Epoch 30/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 1.6907 - acc: 0.7525 - val_loss: 1.3202 - val_acc: 0.7654\n",
      "Epoch 31/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6837 - acc: 0.7533 - val_loss: 1.3224 - val_acc: 0.7615\n",
      "Epoch 32/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6777 - acc: 0.7537 - val_loss: 1.3208 - val_acc: 0.7650\n",
      "Epoch 33/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6719 - acc: 0.7541 - val_loss: 1.3257 - val_acc: 0.7629\n",
      "Epoch 34/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6654 - acc: 0.7554 - val_loss: 1.3239 - val_acc: 0.7624\n",
      "Epoch 35/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6594 - acc: 0.7557 - val_loss: 1.3201 - val_acc: 0.7618\n",
      "Epoch 36/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6540 - acc: 0.7566 - val_loss: 1.3231 - val_acc: 0.7631\n",
      "Epoch 37/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6487 - acc: 0.7568 - val_loss: 1.3197 - val_acc: 0.7663\n",
      "Epoch 38/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6427 - acc: 0.7581 - val_loss: 1.3195 - val_acc: 0.7684\n",
      "Epoch 39/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6375 - acc: 0.7579 - val_loss: 1.3150 - val_acc: 0.7657\n",
      "Epoch 40/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6323 - acc: 0.7581 - val_loss: 1.3244 - val_acc: 0.7619\n",
      "Epoch 41/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6270 - acc: 0.7595 - val_loss: 1.3234 - val_acc: 0.7617\n",
      "Epoch 42/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6214 - acc: 0.7601 - val_loss: 1.3270 - val_acc: 0.7601\n",
      "Epoch 43/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6162 - acc: 0.7604 - val_loss: 1.3214 - val_acc: 0.7638\n",
      "Epoch 44/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6115 - acc: 0.7606 - val_loss: 1.3241 - val_acc: 0.7622\n",
      "Epoch 45/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6061 - acc: 0.7616 - val_loss: 1.3275 - val_acc: 0.7599\n",
      "Epoch 46/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6012 - acc: 0.7622 - val_loss: 1.3222 - val_acc: 0.7651\n",
      "Epoch 47/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.5964 - acc: 0.7628 - val_loss: 1.3247 - val_acc: 0.7631\n",
      "Epoch 48/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.5910 - acc: 0.7632 - val_loss: 1.3250 - val_acc: 0.7668\n",
      "Epoch 49/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.5868 - acc: 0.7636 - val_loss: 1.3247 - val_acc: 0.7598\n",
      "Epoch 50/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.5814 - acc: 0.7645 - val_loss: 1.3295 - val_acc: 0.7646\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=1024, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Complete the function predict: output the list of occupations where the corresponding neuron on the output layer of our model has a value > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q177220', 'Q639669', 'Q33999'}\n"
     ]
    }
   ],
   "source": [
    "def predict_nn(model, article_name, vectorized_dataset):\n",
    "    input_vector = vectorized_dataset[article_name]['vector'].reshape((1, 300))\n",
    "    scores = model.predict(input_vector).reshape(100)\n",
    "    predictions = np.where(scores > 0.1)[0]\n",
    "#     print(scores[predictions])\n",
    "    return set(np.array(occupations)[predictions])\n",
    "\n",
    "print(predict_nn(model, 'Elvis_Presley', vectorized_training))\n",
    "# should be {'Q177220'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nn(vectorized_training, model):\n",
    "    nexample = 0\n",
    "    accuracy = 0.\n",
    "    prediction = None\n",
    "    for article_name in vectorized_training:\n",
    "        prediction = predict_nn(model, article_name, vectorized_training)\n",
    "        p = frozenset(prediction)\n",
    "        g = frozenset(vectorized_training[article_name]['occupations'])\n",
    "        accuracy += 1.*len(p & g) / len(p | g)\n",
    "        nexample += 1\n",
    "    return accuracy / nexample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7048576643356244\n",
      "0.6662116943899452\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_nn(vectorized_training_train, model))\n",
    "print(evaluate_nn(vectorized_training_test, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Your approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, GRU, Dropout, Conv1D, MaxPooling1D, MaxPooling1D, Bidirectional, BatchNormalization, LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dataset into summaries, titles and occupations\n",
    "def parse(dataset):\n",
    "    titles = []\n",
    "    summaries = []\n",
    "    occs = []\n",
    "    for example in dataset:\n",
    "        titles.append(example['title'])\n",
    "        summaries.append(example['summary'])        \n",
    "        if 'occupations' in example:\n",
    "            occs.append(example['occupations'])\n",
    "        else:\n",
    "            occs.append([])\n",
    "    return titles, summaries, occs\n",
    "    \n",
    "titles_train, summaries_train, occs_train = parse(training_set)\n",
    "\n",
    "s = int(len(titles_train) * 0.8)\n",
    "titles_train_train, summaries_train_train, occs_train_train = titles_train[:s], summaries_train[:s], occs_train[:s]\n",
    "titles_train_test, summaries_train_test, occs_train_test = titles_train[s:], summaries_train[s:], occs_train[s:]\n",
    "\n",
    "titles_test, summaries_test, occs_test = parse(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 370295 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(titles_train_train)\n",
    "maxlen = 300\n",
    "training_samples = int(n_samples * 0.85)\n",
    "validation_samples = n_samples - training_samples\n",
    "max_words = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(summaries_train_train)\n",
    "\n",
    "# convert text to sequences\n",
    "sequences =  tokenizer.texts_to_sequences(summaries_train_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(summaries_train_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found', len(word_index), 'unique tokens.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_occs_to_labels(occupations, occs_train):\n",
    "    labels = []\n",
    "    for i in range(len(occs_train)):\n",
    "        label = []\n",
    "        for occ in occupations:\n",
    "            if occ in occs_train[i]:\n",
    "                label.append(1)\n",
    "            else:\n",
    "                label.append(0)\n",
    "        labels.append(label)\n",
    "    return np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (342333, 300)\n",
      "Shape of label tensor: (342333, 100)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "data_test = pad_sequences(sequences_test, maxlen=maxlen)\n",
    "labels = convert_occs_to_labels(occupations, occs_train_train)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# shuffle the data\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# split into training and testing set\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = 'glove.6B'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found', len(embeddings_index), 'word vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build embedding matrix to load into embedding layer\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 300, 300)          6000000   \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 300, 64)           57664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 300, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 100, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 100, 128)          512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 33, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 33, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 33, 256)           98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 33, 256)           1024      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 11, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 11, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_13 (Bidirectio (None, 11, 256)           295680    \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 11, 200)           214200    \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (None, 200)               180600    \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 512)               102912    \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 100)               25700     \n",
      "=================================================================\n",
      "Total params: 7,203,028\n",
      "Trainable params: 7,200,084\n",
      "Non-trainable params: 2,944\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "\n",
    "model.add(Conv1D(64,kernel_size=3,padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Conv1D(128,kernel_size=3,padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Conv1D(256,kernel_size=3,padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "\n",
    "model.add(Bidirectional(GRU(128, return_sequences=True, recurrent_dropout = 0.1)))\n",
    "\n",
    "model.add(Bidirectional(GRU(100, return_sequences=True, recurrent_dropout = 0.1)))\n",
    "\n",
    "model.add(Bidirectional(GRU(100, recurrent_dropout = 0.1)))\n",
    "\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.15))\n",
    "\n",
    "# model.add(GRU(128,\n",
    "#               dropout=0.1,\n",
    "#               recurrent_dropout=0.5,\n",
    "#               return_sequences=True))\n",
    "# model.add(GRU(128,\n",
    "#               dropout=0.1,\n",
    "#               recurrent_dropout=0.3,\n",
    "#               return_sequences=True\n",
    "#              ))\n",
    "\n",
    "# model.add(Flatten())\n",
    "\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(512, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(100, activation='softmax'))\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Glove embedding in the model\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False # we will not update this layer during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as tfb\n",
    "\n",
    "POS_WEIGHT = 10  # multiplier for positive targets\n",
    "\n",
    "def weighted_binary_crossentropy(target, output):\n",
    "    \"\"\"\n",
    "    Weighted binary crossentropy between an output tensor \n",
    "    and a target tensor. POS_WEIGHT is used as a multiplier \n",
    "    for the positive targets.\n",
    "\n",
    "    Combination of the following functions:\n",
    "    * keras.losses.binary_crossentropy\n",
    "    * keras.backend.tensorflow_backend.binary_crossentropy\n",
    "    * tf.nn.weighted_cross_entropy_with_logits\n",
    "    \"\"\"\n",
    "    # transform back to logits\n",
    "    _epsilon = tfb._to_tensor(tfb.epsilon(), output.dtype.base_dtype)\n",
    "    output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "    output = tf.log(output / (1 - output))\n",
    "    # compute weighted loss\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(targets=target,\n",
    "                                                    logits=output,\n",
    "                                                    pos_weight=POS_WEIGHT)\n",
    "    return tf.reduce_mean(loss, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 290983 samples, validate on 51350 samples\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-24ed8d129cb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     validation_data=(x_val, y_val))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2697\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_make_callable_from_options'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2698\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2699\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;31m# not already marked as initialized.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 is_initialized = session.run(\n\u001b[0;32m--> 199\u001b[0;31m                     [tf.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0muninitialized_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    ReduceLROnPlateau(monitor='val_loss', \n",
    "                      factor=0.2, \n",
    "                      patience=5, \n",
    "                      verbose=1, \n",
    "                      mode='auto', \n",
    "                      min_delta=0.0001, \n",
    "                      cooldown=0, \n",
    "                      min_lr=0),\n",
    "    ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True, verbose=1, mode='min')\n",
    "]\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "#     loss='categorical_crossentropy',\n",
    "#     loss='binary_crossentropy',\n",
    "    loss=weighted_binary_crossentropy,\n",
    "    metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    "    batch_size=2000,\n",
    "    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q177220', 'Q10800557', 'Q28389', 'Q486748', 'Q855091']\n"
     ]
    }
   ],
   "source": [
    "title = 'Elvis_Presley'\n",
    "idx = titles_train_train.index(title)\n",
    "input_vector = data[idx].reshape(1, maxlen)\n",
    "print(occs_train_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9998683]\n",
      "[0.9998683]\n",
      "[0.9998683]\n",
      "[0.9998683]\n",
      "[0.9998683]\n",
      "[0.9998683]\n",
      "({'Q10871364'}, {'Q10871364'}, {'Q10871364'}, {'Q10871364'}, {'Q10871364'}, {'Q10871364'})\n"
     ]
    }
   ],
   "source": [
    "def predict_nn_2(model, input_vector, print_score = False):\n",
    "    \n",
    "    scores = model.predict(input_vector).reshape(100)\n",
    "    predictions1 = np.where(scores > 0.3)[0]\n",
    "    predictions2 = np.where(scores > 0.5)[0]\n",
    "    predictions3 = np.where(scores > 0.6)[0]\n",
    "    predictions4 = np.where(scores > 0.7)[0]\n",
    "    predictions5 = np.where(scores > 0.8)[0]\n",
    "    predictions6 = np.where(scores > 0.9)[0]\n",
    "    if print_score:\n",
    "        print(scores[predictions1])\n",
    "        print(scores[predictions2])\n",
    "        print(scores[predictions3])\n",
    "        print(scores[predictions4])\n",
    "        print(scores[predictions5])\n",
    "        print(scores[predictions6])\n",
    "    res1 = set(np.array(occupations)[predictions1])\n",
    "    res2 = set(np.array(occupations)[predictions2])\n",
    "    res3 = set(np.array(occupations)[predictions3])\n",
    "    res4 = set(np.array(occupations)[predictions4])\n",
    "    res5 = set(np.array(occupations)[predictions5])\n",
    "    res6 = set(np.array(occupations)[predictions6])\n",
    "    return res1, res2, res3, res4, res5, res6\n",
    "\n",
    "title = 'Elvis_Presley'\n",
    "idx = titles_train.index(title)\n",
    "\n",
    "print(predict_nn_2(model, input_vector, print_score=True))\n",
    "# should be {'Q177220'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(titles_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "=========================\n",
      "0.0117  :  0.0059\n",
      "0.0117  :  0.0068\n",
      "0.0117  :  0.0072\n",
      "0.0117  :  0.0078\n",
      "0.0117  :  0.0083\n",
      "0.0117  :  0.0091\n",
      "=========================\n",
      "0.0234  :  0.012\n",
      "0.0234  :  0.0138\n",
      "0.0234  :  0.0146\n",
      "0.0234  :  0.0156\n",
      "0.0234  :  0.0166\n",
      "0.0234  :  0.0184\n",
      "=========================\n",
      "0.0351  :  0.0177\n",
      "0.0351  :  0.0204\n",
      "0.0351  :  0.0217\n",
      "0.0351  :  0.0231\n",
      "0.0351  :  0.0246\n",
      "0.0351  :  0.0273\n",
      "=========================\n",
      "0.0467  :  0.0237\n",
      "0.0467  :  0.0273\n",
      "0.0467  :  0.029\n",
      "0.0467  :  0.0309\n",
      "0.0467  :  0.0329\n",
      "0.0467  :  0.0365\n",
      "=========================\n",
      "0.0584  :  0.0292\n",
      "0.0584  :  0.0338\n",
      "0.0584  :  0.036\n",
      "0.0584  :  0.0384\n",
      "0.0584  :  0.0409\n",
      "0.0584  :  0.0455\n",
      "=========================\n",
      "0.0701  :  0.0351\n",
      "0.0701  :  0.0407\n",
      "0.0701  :  0.0432\n",
      "0.0701  :  0.046\n",
      "0.0701  :  0.049\n",
      "0.0701  :  0.0545\n",
      "=========================\n",
      "0.0818  :  0.0415\n",
      "0.0818  :  0.0479\n",
      "0.0818  :  0.0509\n",
      "0.0818  :  0.0541\n",
      "0.0818  :  0.0577\n",
      "0.0818  :  0.064\n",
      "=========================\n",
      "0.0935  :  0.0473\n",
      "0.0935  :  0.0546\n",
      "0.0935  :  0.0581\n",
      "0.0935  :  0.0617\n",
      "0.0935  :  0.0658\n",
      "0.0935  :  0.0731\n",
      "=========================\n",
      "0.1052  :  0.0535\n",
      "0.1052  :  0.0617\n",
      "0.1052  :  0.0656\n",
      "0.1052  :  0.0697\n",
      "0.1052  :  0.0742\n",
      "0.1052  :  0.0823\n",
      "=========================\n",
      "0.1168  :  0.0595\n",
      "0.1168  :  0.0687\n",
      "0.1168  :  0.073\n",
      "0.1168  :  0.0775\n",
      "0.1168  :  0.0825\n",
      "0.1168  :  0.0913\n",
      "=========================\n",
      "0.1285  :  0.0657\n",
      "0.1285  :  0.0758\n",
      "0.1285  :  0.0806\n",
      "0.1285  :  0.0854\n",
      "0.1285  :  0.0909\n",
      "0.1285  :  0.1006\n",
      "=========================\n",
      "0.1402  :  0.0716\n",
      "0.1402  :  0.0827\n",
      "0.1402  :  0.0879\n",
      "0.1402  :  0.0932\n",
      "0.1402  :  0.0993\n",
      "0.1402  :  0.1099\n",
      "=========================\n",
      "0.1519  :  0.0775\n",
      "0.1519  :  0.0896\n",
      "0.1519  :  0.0952\n",
      "0.1519  :  0.1009\n",
      "0.1519  :  0.1075\n",
      "0.1519  :  0.1189\n",
      "=========================\n",
      "0.1636  :  0.0837\n",
      "0.1636  :  0.0966\n",
      "0.1636  :  0.1027\n",
      "0.1636  :  0.1089\n",
      "0.1636  :  0.1158\n",
      "0.1636  :  0.1281\n",
      "=========================\n",
      "0.1753  :  0.0896\n",
      "0.1753  :  0.1034\n",
      "0.1753  :  0.11\n",
      "0.1753  :  0.1165\n",
      "0.1753  :  0.1239\n",
      "0.1753  :  0.1371\n",
      "=========================\n",
      "0.187  :  0.0958\n",
      "0.187  :  0.1106\n",
      "0.187  :  0.1175\n",
      "0.187  :  0.1245\n",
      "0.187  :  0.1323\n",
      "0.187  :  0.1463\n",
      "=========================\n",
      "0.1986  :  0.1017\n",
      "0.1986  :  0.1174\n",
      "0.1986  :  0.1248\n",
      "0.1986  :  0.1322\n",
      "0.1986  :  0.1405\n",
      "0.1986  :  0.1554\n",
      "=========================\n",
      "0.2103  :  0.1083\n",
      "0.2103  :  0.1249\n",
      "0.2103  :  0.1326\n",
      "0.2103  :  0.1404\n",
      "0.2103  :  0.1492\n",
      "0.2103  :  0.1648\n",
      "=========================\n",
      "0.222  :  0.1139\n",
      "0.222  :  0.1314\n",
      "0.222  :  0.1396\n",
      "0.222  :  0.1478\n",
      "0.222  :  0.157\n",
      "0.222  :  0.1736\n",
      "=========================\n",
      "0.2337  :  0.1201\n",
      "0.2337  :  0.1387\n",
      "0.2337  :  0.1473\n",
      "0.2337  :  0.1559\n",
      "0.2337  :  0.1655\n",
      "0.2337  :  0.183\n",
      "=========================\n",
      "0.2454  :  0.1264\n",
      "0.2454  :  0.146\n",
      "0.2454  :  0.155\n",
      "0.2454  :  0.1641\n",
      "0.2454  :  0.1742\n",
      "0.2454  :  0.1925\n",
      "=========================\n",
      "0.2571  :  0.1335\n",
      "0.2571  :  0.154\n",
      "0.2571  :  0.1634\n",
      "0.2571  :  0.1729\n",
      "0.2571  :  0.1834\n",
      "0.2571  :  0.2024\n",
      "=========================\n",
      "0.2687  :  0.1396\n",
      "0.2687  :  0.161\n",
      "0.2687  :  0.1708\n",
      "0.2687  :  0.1808\n",
      "0.2687  :  0.1918\n",
      "0.2687  :  0.2116\n",
      "=========================\n",
      "0.2804  :  0.1461\n",
      "0.2804  :  0.1684\n",
      "0.2804  :  0.1785\n",
      "0.2804  :  0.1889\n",
      "0.2804  :  0.2003\n",
      "0.2804  :  0.2209\n",
      "=========================\n",
      "0.2921  :  0.1527\n",
      "0.2921  :  0.1758\n",
      "0.2921  :  0.1863\n",
      "0.2921  :  0.197\n",
      "0.2921  :  0.2089\n",
      "0.2921  :  0.2302\n",
      "=========================\n",
      "0.3038  :  0.1594\n",
      "0.3038  :  0.1834\n",
      "0.3038  :  0.1942\n",
      "0.3038  :  0.2052\n",
      "0.3038  :  0.2174\n",
      "0.3038  :  0.2395\n",
      "=========================\n",
      "0.3155  :  0.1669\n",
      "0.3155  :  0.1916\n",
      "0.3155  :  0.2026\n",
      "0.3155  :  0.2139\n",
      "0.3155  :  0.2265\n",
      "0.3155  :  0.2491\n",
      "=========================\n",
      "0.3272  :  0.1739\n",
      "0.3272  :  0.1992\n",
      "0.3272  :  0.2105\n",
      "0.3272  :  0.2221\n",
      "0.3272  :  0.2351\n",
      "0.3272  :  0.2584\n",
      "=========================\n",
      "0.3388  :  0.181\n",
      "0.3388  :  0.2071\n",
      "0.3388  :  0.2186\n",
      "0.3388  :  0.2305\n",
      "0.3388  :  0.2437\n",
      "0.3388  :  0.2678\n",
      "=========================\n",
      "0.3505  :  0.1878\n",
      "0.3505  :  0.2145\n",
      "0.3505  :  0.2263\n",
      "0.3505  :  0.2385\n",
      "0.3505  :  0.2522\n",
      "0.3505  :  0.2771\n",
      "=========================\n",
      "0.3622  :  0.194\n",
      "0.3622  :  0.2216\n",
      "0.3622  :  0.2337\n",
      "0.3622  :  0.2462\n",
      "0.3622  :  0.2603\n",
      "0.3622  :  0.2859\n",
      "=========================\n",
      "0.3739  :  0.201\n",
      "0.3739  :  0.2293\n",
      "0.3739  :  0.2418\n",
      "0.3739  :  0.2547\n",
      "0.3739  :  0.2692\n",
      "0.3739  :  0.2954\n",
      "=========================\n",
      "0.3856  :  0.2071\n",
      "0.3856  :  0.2362\n",
      "0.3856  :  0.249\n",
      "0.3856  :  0.2622\n",
      "0.3856  :  0.2771\n",
      "0.3856  :  0.3041\n",
      "=========================\n",
      "0.3973  :  0.211\n",
      "0.3973  :  0.2415\n",
      "0.3973  :  0.2548\n",
      "0.3973  :  0.2686\n",
      "0.3973  :  0.284\n",
      "0.3973  :  0.312\n",
      "=========================\n",
      "0.409  :  0.2164\n",
      "0.409  :  0.2478\n",
      "0.409  :  0.2616\n",
      "0.409  :  0.2758\n",
      "0.409  :  0.2916\n",
      "0.409  :  0.3205\n",
      "=========================\n",
      "0.4206  :  0.2219\n",
      "0.4206  :  0.2542\n",
      "0.4206  :  0.2684\n",
      "0.4206  :  0.2832\n",
      "0.4206  :  0.2993\n",
      "0.4206  :  0.329\n",
      "=========================\n",
      "0.4323  :  0.2276\n",
      "0.4323  :  0.2608\n",
      "0.4323  :  0.2755\n",
      "0.4323  :  0.2906\n",
      "0.4323  :  0.3072\n",
      "0.4323  :  0.3377\n",
      "=========================\n",
      "0.444  :  0.2334\n",
      "0.444  :  0.2676\n",
      "0.444  :  0.2827\n",
      "0.444  :  0.2983\n",
      "0.444  :  0.3153\n",
      "0.444  :  0.3465\n",
      "=========================\n",
      "0.4557  :  0.2392\n",
      "0.4557  :  0.2744\n",
      "0.4557  :  0.29\n",
      "0.4557  :  0.3059\n",
      "0.4557  :  0.3234\n",
      "0.4557  :  0.3555\n",
      "=========================\n",
      "0.4674  :  0.2451\n",
      "0.4674  :  0.2812\n",
      "0.4674  :  0.2971\n",
      "0.4674  :  0.3134\n",
      "0.4674  :  0.3313\n",
      "0.4674  :  0.3643\n",
      "=========================\n",
      "0.4791  :  0.2509\n",
      "0.4791  :  0.2879\n",
      "0.4791  :  0.3043\n",
      "0.4791  :  0.3211\n",
      "0.4791  :  0.3394\n",
      "0.4791  :  0.3732\n",
      "=========================\n",
      "0.4907  :  0.2567\n",
      "0.4907  :  0.2946\n",
      "0.4907  :  0.3113\n",
      "0.4907  :  0.3285\n",
      "0.4907  :  0.3472\n",
      "0.4907  :  0.382\n",
      "=========================\n",
      "0.5024  :  0.2625\n",
      "0.5024  :  0.3013\n",
      "0.5024  :  0.3184\n",
      "0.5024  :  0.336\n",
      "0.5024  :  0.3551\n",
      "0.5024  :  0.3908\n",
      "=========================\n",
      "0.5141  :  0.2686\n",
      "0.5141  :  0.3082\n",
      "0.5141  :  0.3257\n",
      "0.5141  :  0.3436\n",
      "0.5141  :  0.3632\n",
      "0.5141  :  0.3997\n",
      "=========================\n",
      "0.5258  :  0.2744\n",
      "0.5258  :  0.3149\n",
      "0.5258  :  0.3327\n",
      "0.5258  :  0.3512\n",
      "0.5258  :  0.3711\n",
      "0.5258  :  0.4086\n",
      "=========================\n",
      "0.5375  :  0.2801\n",
      "0.5375  :  0.3214\n",
      "0.5375  :  0.3397\n",
      "0.5375  :  0.3586\n",
      "0.5375  :  0.379\n",
      "0.5375  :  0.4174\n",
      "=========================\n",
      "0.5492  :  0.2857\n",
      "0.5492  :  0.328\n",
      "0.5492  :  0.3467\n",
      "0.5492  :  0.3661\n",
      "0.5492  :  0.387\n",
      "0.5492  :  0.4264\n",
      "=========================\n",
      "0.5609  :  0.2913\n",
      "0.5609  :  0.3345\n",
      "0.5609  :  0.3536\n",
      "0.5609  :  0.3735\n",
      "0.5609  :  0.3949\n",
      "0.5609  :  0.4351\n",
      "=========================\n",
      "0.5725  :  0.2976\n",
      "0.5725  :  0.3417\n",
      "0.5725  :  0.3612\n",
      "0.5725  :  0.3815\n",
      "0.5725  :  0.4033\n",
      "0.5725  :  0.4443\n",
      "=========================\n",
      "0.5842  :  0.3033\n",
      "0.5842  :  0.3482\n",
      "0.5842  :  0.3681\n",
      "0.5842  :  0.389\n",
      "0.5842  :  0.4112\n",
      "0.5842  :  0.4532\n",
      "=========================\n",
      "0.5959  :  0.309\n",
      "0.5959  :  0.3548\n",
      "0.5959  :  0.3751\n",
      "0.5959  :  0.3964\n",
      "0.5959  :  0.4192\n",
      "0.5959  :  0.4622\n",
      "=========================\n",
      "0.6076  :  0.3149\n",
      "0.6076  :  0.3616\n",
      "0.6076  :  0.3824\n",
      "0.6076  :  0.4041\n",
      "0.6076  :  0.4274\n",
      "0.6076  :  0.4713\n",
      "=========================\n",
      "0.6193  :  0.3213\n",
      "0.6193  :  0.3689\n",
      "0.6193  :  0.39\n",
      "0.6193  :  0.4122\n",
      "0.6193  :  0.4359\n",
      "0.6193  :  0.4806\n",
      "=========================\n",
      "0.631  :  0.327\n",
      "0.631  :  0.3757\n",
      "0.631  :  0.3972\n",
      "0.631  :  0.4198\n",
      "0.631  :  0.4441\n",
      "0.631  :  0.4897\n",
      "=========================\n",
      "0.6426  :  0.3332\n",
      "0.6426  :  0.3827\n",
      "0.6426  :  0.4046\n",
      "0.6426  :  0.4276\n",
      "0.6426  :  0.4524\n",
      "0.6426  :  0.4988\n",
      "=========================\n",
      "0.6543  :  0.3392\n",
      "0.6543  :  0.3896\n",
      "0.6543  :  0.4119\n",
      "0.6543  :  0.4354\n",
      "0.6543  :  0.4607\n",
      "0.6543  :  0.508\n",
      "=========================\n",
      "0.666  :  0.3453\n",
      "0.666  :  0.3966\n",
      "0.666  :  0.4192\n",
      "0.666  :  0.443\n",
      "0.666  :  0.4688\n",
      "0.666  :  0.517\n",
      "=========================\n",
      "0.6777  :  0.3515\n",
      "0.6777  :  0.4037\n",
      "0.6777  :  0.4268\n",
      "0.6777  :  0.451\n",
      "0.6777  :  0.4773\n",
      "0.6777  :  0.5262\n",
      "=========================\n",
      "0.6894  :  0.3568\n",
      "0.6894  :  0.4101\n",
      "0.6894  :  0.4336\n",
      "0.6894  :  0.4582\n",
      "0.6894  :  0.4851\n",
      "0.6894  :  0.535\n",
      "=========================\n",
      "0.7011  :  0.3628\n",
      "0.7011  :  0.4171\n",
      "0.7011  :  0.441\n",
      "0.7011  :  0.4661\n",
      "0.7011  :  0.4935\n",
      "0.7011  :  0.5442\n",
      "=========================\n",
      "0.7128  :  0.3687\n",
      "0.7128  :  0.4239\n",
      "0.7128  :  0.4483\n",
      "0.7128  :  0.4738\n",
      "0.7128  :  0.5017\n",
      "0.7128  :  0.5534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "0.7244  :  0.3749\n",
      "0.7244  :  0.4311\n",
      "0.7244  :  0.4559\n",
      "0.7244  :  0.4819\n",
      "0.7244  :  0.5103\n",
      "0.7244  :  0.5628\n",
      "=========================\n",
      "0.7361  :  0.3807\n",
      "0.7361  :  0.4379\n",
      "0.7361  :  0.4631\n",
      "0.7361  :  0.4896\n",
      "0.7361  :  0.5185\n",
      "0.7361  :  0.572\n",
      "=========================\n",
      "0.7478  :  0.3868\n",
      "0.7478  :  0.4449\n",
      "0.7478  :  0.4705\n",
      "0.7478  :  0.4974\n",
      "0.7478  :  0.5268\n",
      "0.7478  :  0.5814\n",
      "=========================\n",
      "0.7595  :  0.3929\n",
      "0.7595  :  0.4519\n",
      "0.7595  :  0.4779\n",
      "0.7595  :  0.5052\n",
      "0.7595  :  0.535\n",
      "0.7595  :  0.5905\n",
      "=========================\n",
      "0.7712  :  0.399\n",
      "0.7712  :  0.4588\n",
      "0.7712  :  0.4852\n",
      "0.7712  :  0.5129\n",
      "0.7712  :  0.5433\n",
      "0.7712  :  0.5995\n",
      "=========================\n",
      "0.7829  :  0.4049\n",
      "0.7829  :  0.4656\n",
      "0.7829  :  0.4924\n",
      "0.7829  :  0.5205\n",
      "0.7829  :  0.5514\n",
      "0.7829  :  0.6087\n",
      "=========================\n",
      "0.7945  :  0.4111\n",
      "0.7945  :  0.4727\n",
      "0.7945  :  0.4999\n",
      "0.7945  :  0.5284\n",
      "0.7945  :  0.5597\n",
      "0.7945  :  0.618\n",
      "=========================\n",
      "0.8062  :  0.4175\n",
      "0.8062  :  0.4799\n",
      "0.8062  :  0.5075\n",
      "0.8062  :  0.5365\n",
      "0.8062  :  0.5683\n",
      "0.8062  :  0.6274\n",
      "=========================\n",
      "0.8179  :  0.4233\n",
      "0.8179  :  0.4868\n",
      "0.8179  :  0.5148\n",
      "0.8179  :  0.5441\n",
      "0.8179  :  0.5765\n",
      "0.8179  :  0.6365\n",
      "=========================\n",
      "0.8296  :  0.4298\n",
      "0.8296  :  0.4942\n",
      "0.8296  :  0.5225\n",
      "0.8296  :  0.5523\n",
      "0.8296  :  0.5851\n",
      "0.8296  :  0.6459\n",
      "=========================\n",
      "0.8413  :  0.4353\n",
      "0.8413  :  0.5006\n",
      "0.8413  :  0.5293\n",
      "0.8413  :  0.5595\n",
      "0.8413  :  0.5929\n",
      "0.8413  :  0.6547\n",
      "=========================\n",
      "0.853  :  0.4412\n",
      "0.853  :  0.5074\n",
      "0.853  :  0.5364\n",
      "0.853  :  0.567\n",
      "0.853  :  0.6008\n",
      "0.853  :  0.6637\n",
      "=========================\n",
      "0.8646  :  0.4477\n",
      "0.8646  :  0.5148\n",
      "0.8646  :  0.5442\n",
      "0.8646  :  0.5752\n",
      "0.8646  :  0.6095\n",
      "0.8646  :  0.6732\n",
      "=========================\n",
      "0.8763  :  0.4551\n",
      "0.8763  :  0.523\n",
      "0.8763  :  0.5527\n",
      "0.8763  :  0.5839\n",
      "0.8763  :  0.6186\n",
      "0.8763  :  0.683\n",
      "=========================\n",
      "0.888  :  0.4613\n",
      "0.888  :  0.5301\n",
      "0.888  :  0.5602\n",
      "0.888  :  0.5919\n",
      "0.888  :  0.627\n",
      "0.888  :  0.6923\n",
      "=========================\n",
      "0.8997  :  0.4683\n",
      "0.8997  :  0.5379\n",
      "0.8997  :  0.5684\n",
      "0.8997  :  0.6004\n",
      "0.8997  :  0.6359\n",
      "0.8997  :  0.7018\n",
      "=========================\n",
      "0.9114  :  0.4749\n",
      "0.9114  :  0.5455\n",
      "0.9114  :  0.5764\n",
      "0.9114  :  0.6087\n",
      "0.9114  :  0.6445\n",
      "0.9114  :  0.7111\n",
      "=========================\n",
      "0.9231  :  0.4817\n",
      "0.9231  :  0.5531\n",
      "0.9231  :  0.5843\n",
      "0.9231  :  0.617\n",
      "0.9231  :  0.6532\n",
      "0.9231  :  0.7206\n",
      "=========================\n",
      "0.9348  :  0.489\n",
      "0.9348  :  0.561\n",
      "0.9348  :  0.5926\n",
      "0.9348  :  0.6255\n",
      "0.9348  :  0.6621\n",
      "0.9348  :  0.7301\n",
      "=========================\n",
      "0.9464  :  0.4959\n",
      "0.9464  :  0.5686\n",
      "0.9464  :  0.6004\n",
      "0.9464  :  0.6337\n",
      "0.9464  :  0.6706\n",
      "0.9464  :  0.7393\n",
      "=========================\n",
      "0.9581  :  0.5029\n",
      "0.9581  :  0.5764\n",
      "0.9581  :  0.6084\n",
      "0.9581  :  0.642\n",
      "0.9581  :  0.6793\n",
      "0.9581  :  0.7487\n",
      "=========================\n",
      "0.9698  :  0.5098\n",
      "0.9698  :  0.584\n",
      "0.9698  :  0.6163\n",
      "0.9698  :  0.6501\n",
      "0.9698  :  0.6878\n",
      "0.9698  :  0.758\n",
      "=========================\n",
      "0.9815  :  0.5162\n",
      "0.9815  :  0.5912\n",
      "0.9815  :  0.6238\n",
      "0.9815  :  0.658\n",
      "0.9815  :  0.696\n",
      "0.9815  :  0.767\n",
      "=========================\n",
      "0.9932  :  0.5232\n",
      "0.9932  :  0.599\n",
      "0.9932  :  0.6319\n",
      "0.9932  :  0.6664\n",
      "0.9932  :  0.7048\n",
      "0.9932  :  0.7763\n",
      "(0.5271895909157703, 0.603347935915702, 0.6364334907510414, 0.6711151250501953, 0.7098120382297205, 0.7816983296092981)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_nn_2(titles, input_vectors, occs, model):\n",
    "    nexample = len(titles)\n",
    "    accuracy1 = 0.\n",
    "    accuracy2 = 0.\n",
    "    accuracy3 = 0.\n",
    "    accuracy4 = 0.\n",
    "    accuracy5 = 0.\n",
    "    accuracy6 = 0.\n",
    "    prediction = None\n",
    "    for i in range(len(titles)):        \n",
    "        input_vector = input_vectors[i].reshape(1, -1)\n",
    "        prediction1, prediction2, prediction3, prediction4, prediction5, prediction6 = predict_nn_2(model, input_vector)\n",
    "        p1 = frozenset(prediction1)\n",
    "        p2 = frozenset(prediction2)\n",
    "        p3 = frozenset(prediction3)\n",
    "        p4 = frozenset(prediction4)\n",
    "        p5 = frozenset(prediction5)\n",
    "        p6 = frozenset(prediction6)\n",
    "        g = frozenset(occs[i])\n",
    "        accuracy1 += 1. / nexample * len(p1 & g) / len(p1 | g)\n",
    "        accuracy2 += 1. / nexample * len(p2 & g) / len(p2 | g)\n",
    "        accuracy3 += 1. / nexample * len(p3 & g) / len(p3 | g)\n",
    "        accuracy4 += 1. / nexample * len(p4 & g) / len(p4 | g)\n",
    "        accuracy5 += 1. / nexample * len(p5 & g) / len(p5 | g)\n",
    "        accuracy6 += 1. / nexample * len(p5 & g) / len(p6 | g)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"=========================\")\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy1, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy2, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy3, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy4, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy5, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy6, 4))\n",
    "    return accuracy1, accuracy2, accuracy3, accuracy4, accuracy5, accuracy6\n",
    "\n",
    "# print(evaluate_nn_2(titles_train, summaries_train, occs_train, model))\n",
    "print(evaluate_nn_2(titles_train_test, data_test, occs_train_test, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate_nn_2(titles_train_train, data, occs_train_train, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***IMPORTANT*** Output format of requested file 'results.json.gz': each line must be a json string representing a dictionnary:\n",
    "> ```{ 'title': THE_ARTICLE_NAME, 'prediction': [THE_LIST_OF_OCCUPATIONS]}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example if testset_solutions is a dictionnary: article_name (key) -> prediction_list (value) use this function:\n",
    "def export(testset_solutions):\n",
    "    with gzip.open('results.json.gz', 'wt') as output:\n",
    "        for article in testset_solutions:\n",
    "            output.write(json.dumps({'title':article, 'prediction':testset_solutions[article]}) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
