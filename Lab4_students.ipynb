{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import\n",
    "## Question 0 - Get common wikidata occupations\n",
    "\n",
    "> Write a sparql query that retrieves the top 100 occupations on wikidata (wikidata property P106).\n",
    "\n",
    "You may use the interface https://query.wikidata.org/ to try different queries. Here are some example sparql queries: https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT ?o (COUNT(?person) AS ?count) WHERE \n",
    "{\n",
    "   ?person wdt:P106 ?o\n",
    "}\n",
    "GROUP BY ?o\n",
    "ORDER BY DESC(?count)\n",
    "LIMIT 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following assertion should pass if your answer is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "occupations = ['Q82955', 'Q937857', 'Q36180', 'Q33999', 'Q1650915', 'Q1028181', 'Q1930187', 'Q177220', 'Q1622272', 'Q49757', 'Q36834', 'Q40348', 'Q47064', 'Q639669', 'Q10800557', 'Q201788', 'Q2526255', 'Q43845', 'Q28389', 'Q42973', 'Q10871364', 'Q39631', 'Q193391', 'Q482980', 'Q483501', 'Q11513337', 'Q3665646', 'Q12299841', 'Q19204627', 'Q16533', 'Q81096', 'Q11774891', 'Q188094', 'Q1281618', 'Q333634', 'Q189290', 'Q250867', 'Q33231', 'Q2259451', 'Q42603', 'Q628099', 'Q37226', 'Q2309784', 'Q901', 'Q2066131', 'Q6625963', 'Q10798782', 'Q2374149', 'Q170790', 'Q4610556', 'Q185351', 'Q486748', 'Q3055126', 'Q753110', 'Q4964182', 'Q169470', 'Q158852', 'Q1234713', 'Q14089670', 'Q10873124', 'Q3282637', 'Q593644', 'Q947873', 'Q13414980', 'Q131524', 'Q11338576', 'Q15117302', 'Q488205', 'Q14467526', 'Q183945', 'Q10843402', 'Q13382576', 'Q13141064', 'Q214917', 'Q855091', 'Q644687', 'Q19595175', 'Q121594', 'Q2865819', 'Q16010345', 'Q1231865', 'Q2405480', 'Q350979', 'Q3400985', 'Q13365117', 'Q10833314', 'Q3621491', 'Q15981151', 'Q212980', 'Q16145150', 'Q1792450', 'Q15296811', 'Q15627169', 'Q2306091', 'Q4263842', 'Q806798', 'Q5716684', 'Q2516866', 'Q3387717', 'Q131512']\n",
    "\n",
    "def evalSparql(query):\n",
    "    return requests.post('https://query.wikidata.org/sparql', data=query, headers={\n",
    "        'content-type': 'application/sparql-query',\n",
    "        'accept': 'application/json',\n",
    "        'user-agent': 'User:Tpt'\n",
    "    }).json()['results']['bindings']\n",
    "\n",
    "myOccupations = [val['o']['value'].replace('http://www.wikidata.org/entity/', '') \n",
    "                 for val in evalSparql(query)]\n",
    "assert(frozenset(occupations) == frozenset(myOccupations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupations labels\n",
    "\n",
    "We load the labels of the occupations from Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q82955': 'politician', 'Q121594': 'professor', 'Q177220': 'singer', 'Q169470': 'physicist', 'Q170790': 'mathematician', 'Q81096': 'engineer', 'Q201788': 'historian', 'Q188094': 'economist', 'Q212980': 'psychologist', 'Q214917': 'playwright', 'Q131524': 'entrepreneur', 'Q183945': 'record producer', 'Q193391': 'diplomat', 'Q189290': 'military officer', 'Q185351': 'jurist', 'Q350979': 'zoologist', 'Q483501': 'artist', 'Q482980': 'author', 'Q333634': 'translator', 'Q158852': 'conductor', 'Q486748': 'pianist', 'Q488205': 'singer-songwriter', 'Q250867': 'Catholic priest', 'Q593644': 'chemist', 'Q639669': 'musician', 'Q644687': 'illustrator', 'Q628099': 'association football manager', 'Q855091': 'guitarist', 'Q937857': 'association football player', 'Q947873': 'television presenter', 'Q806798': 'banker', 'Q1028181': 'painter', 'Q753110': 'songwriter', 'Q1234713': 'theologian', 'Q1281618': 'sculptor', 'Q1622272': 'university teacher', 'Q1792450': 'art historian', 'Q1650915': 'researcher', 'Q1930187': 'journalist', 'Q2306091': 'sociologist', 'Q2374149': 'botanist', 'Q2526255': 'film director', 'Q2516866': 'publisher', 'Q2066131': 'athlete', 'Q2405480': 'voice actor', 'Q1231865': 'pedagogue', 'Q2865819': 'opera singer', 'Q2259451': 'stage actor', 'Q3282637': 'film producer', 'Q3387717': 'theatre director', 'Q3055126': 'entomologist', 'Q3400985': 'academic', 'Q3665646': 'basketball player', 'Q3621491': 'archaeologist', 'Q4610556': 'model', 'Q4263842': 'literary critic', 'Q4964182': 'philosopher', 'Q5716684': 'dancer', 'Q6625963': 'novelist', 'Q10843402': 'swimmer', 'Q10833314': 'tennis player', 'Q10871364': 'baseball player', 'Q10798782': 'television actor', 'Q10873124': 'chess player', 'Q10800557': 'film actor', 'Q11338576': 'boxer', 'Q11513337': 'athletics competitor', 'Q2309784': 'sport cyclist', 'Q11774891': 'ice hockey player', 'Q12299841': 'cricketer', 'Q13141064': 'badminton player', 'Q13365117': 'handball player', 'Q13414980': 'Australian rules footballer', 'Q14089670': 'rugby union player', 'Q14467526': 'linguist', 'Q15117302': 'volleyball player', 'Q15627169': 'trade unionist', 'Q15981151': 'jazz musician', 'Q16010345': 'performer', 'Q131512': 'agriculturer', 'Q13382576': 'rower', 'Q19204627': 'American football player', 'Q15296811': 'drawer', 'Q19595175': 'amateur wrestler', 'Q16145150': 'music pedagogue', 'Q901': 'scientist', 'Q33999': 'actor', 'Q33231': 'photographer', 'Q36834': 'composer', 'Q16533': 'judge', 'Q40348': 'lawyer', 'Q36180': 'writer', 'Q42973': 'architect', 'Q43845': 'businessperson', 'Q39631': 'physician', 'Q28389': 'screenwriter', 'Q42603': 'priest', 'Q49757': 'poet', 'Q37226': 'teacher', 'Q47064': 'military personnel'}\n"
     ]
    }
   ],
   "source": [
    "occupations_label = {}\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT ?o ?oLabel \n",
    "WHERE { \n",
    "    VALUES ?o { %s } \n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\"\"\"% ' '.join('wd:' + o for o in occupations)\n",
    "\n",
    "for result in evalSparql(query):\n",
    "    occupations_label[result['o']['value'].replace('http://www.wikidata.org/entity/', '')] = result['oLabel']['value']\n",
    "\n",
    "print(occupations_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load *all* the labels of the occupations from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q82955': ['politician', 'political leader', 'polit.', 'political figure'], 'Q121594': ['professor', 'Prof.'], 'Q177220': ['singer', 'vocalist'], 'Q169470': ['physicist'], 'Q170790': ['mathematician'], 'Q81096': ['engineer'], 'Q201788': ['historian', 'historians', 'historiographer'], 'Q188094': ['economist'], 'Q212980': ['psychologist'], 'Q214917': ['playwright', 'dramatist', 'scriptwriter', 'playwrite', 'Playwright, dramatist'], 'Q131524': ['entrepreneur'], 'Q183945': ['record producer', 'music producer'], 'Q193391': ['diplomat'], 'Q189290': ['military officer', 'army officer', 'officer'], 'Q185351': ['jurist'], 'Q350979': ['zoologist', 'zooligist'], 'Q483501': ['artist'], 'Q482980': ['author'], 'Q333634': ['translator'], 'Q158852': ['conductor', 'Conducting'], 'Q486748': ['pianist'], 'Q488205': ['singer-songwriter', 'singer/songwriter', 'singer songwriter', 'singersongwriter'], 'Q250867': ['Catholic priest', 'Roman Catholic priest', 'Catholic presbyter', 'Roman Catholic presbyter'], 'Q593644': ['chemist', 'chemists'], 'Q639669': ['musician'], 'Q644687': ['illustrator'], 'Q628099': ['association football manager', 'football manager', 'association football coach', 'football coach', 'soccer coach', 'soccer manager'], 'Q855091': ['guitarist', 'guitar player'], 'Q937857': ['association football player', 'footballer', 'football player', 'association footballer', 'soccer player'], 'Q947873': ['television presenter', 'TV presenter', 'hostess', 'TV host', 'TV personality', 'host', 'television personality', 'television host'], 'Q806798': ['banker', 'Private Banker', 'private sector banker'], 'Q1028181': ['painter'], 'Q753110': ['songwriter', 'song writer'], 'Q1234713': ['theologian', 'religious scholar'], 'Q1281618': ['sculptor'], 'Q1622272': ['university teacher', 'lecturer', 'university teachers', 'college lecturer', 'college professor'], 'Q1792450': ['art historian'], 'Q1650915': ['researcher'], 'Q1930187': ['journalist', 'journo'], 'Q2306091': ['sociologist'], 'Q2374149': ['botanist', 'botany', 'plant scientist'], 'Q2526255': ['film director', 'director', 'movie director'], 'Q2516866': ['publisher'], 'Q2066131': ['athlete', 'sportsperson', 'sportsman', 'sportswoman'], 'Q2405480': ['voice actor', 'voice actress', 'voice artist'], 'Q1231865': ['pedagogue', 'educationalist'], 'Q2865819': ['opera singer'], 'Q2259451': ['stage actor', 'stage actress', 'theater actor', 'theater actress', 'theatre actor', 'theatre actress'], 'Q3282637': ['film producer', 'producer', 'movie producer'], 'Q3387717': ['theatre director', 'theater director', 'stage director'], 'Q3055126': ['entomologist'], 'Q3400985': ['academic', 'college graduates', 'university graduates'], 'Q3665646': ['basketball player', 'professional basketball player', 'basketballer'], 'Q3621491': ['archaeologist', 'archeologist'], 'Q4610556': ['model', 'fashion model', 'mannequin'], 'Q4263842': ['literary critic', 'book critic', 'literary critique'], 'Q4964182': ['philosopher'], 'Q5716684': ['dancer'], 'Q6625963': ['novelist'], 'Q10843402': ['swimmer'], 'Q10833314': ['tennis player'], 'Q10871364': ['baseball player'], 'Q10798782': ['television actor', 'actor', 'actress', 'television actress', 'TV actor', 'TV actress'], 'Q10873124': ['chess player'], 'Q10800557': ['film actor', 'film actress', 'movie actor', 'movie actress'], 'Q11338576': ['boxer', 'pugilist'], 'Q11513337': ['athletics competitor', 'track and field athlete', 'athlete (restricted sense)'], 'Q2309784': ['sport cyclist', 'racing cyclist', 'sport bicyclist', 'sport biker'], 'Q11774891': ['ice hockey player', 'hockey player'], 'Q12299841': ['cricketer', 'cricket player'], 'Q13141064': ['badminton player'], 'Q13365117': ['handball player', 'handballer'], 'Q13414980': ['Australian rules footballer', 'Australian footballer', 'Australian rules football player', 'Australian-rules football player'], 'Q14089670': ['rugby union player'], 'Q14467526': ['linguist', 'linguistic scholar'], 'Q15117302': ['volleyball player', 'volleyballer'], 'Q15627169': ['trade unionist', 'labor unionist', 'labour unionist'], 'Q15981151': ['jazz musician'], 'Q16010345': ['performer', 'scenic artist', 'performing artist'], 'Q131512': ['agriculturer', 'farmer', 'agriculturist', 'cultivator', 'grower', 'raiser'], 'Q13382576': ['rower', 'oarsman', 'oarswoman'], 'Q19204627': ['American football player', 'football player'], 'Q15296811': ['drawer', 'illustrator', 'draughtsperson', 'draughtsman', 'draftsperson', 'draftsman', 'draftswoman', 'drafter'], 'Q19595175': ['amateur wrestler', 'wrestler'], 'Q16145150': ['music pedagogue', 'music teacher'], 'Q901': ['scientist', 'natural philosopher'], 'Q33999': ['actor', 'actors', 'actresses', 'actress'], 'Q33231': ['photographer'], 'Q36834': ['composer'], 'Q16533': ['judge', 'magistrate', 'justice', 'judges', 'justices'], 'Q40348': ['lawyer', 'attorney', 'Jurisprudente'], 'Q36180': ['writer', 'author', 'writers', 'authors'], 'Q42973': ['architect'], 'Q43845': ['businessperson', 'businessman', 'dealer', 'business person', 'business woman', 'businesswoman', 'business man'], 'Q39631': ['physician', 'physicians'], 'Q28389': ['screenwriter', 'writer', 'screen writer', 'scriptwriter', 'scenarist', 'film writer', 'tv writer', 'script writer'], 'Q42603': ['priest', 'priestess', 'reverend'], 'Q49757': ['poet', 'bard', 'poetess'], 'Q37226': ['teacher', 'professor', 'educator', 'schoolmaster', 'schoolmistress', 'school teacher'], 'Q47064': ['military personnel']}\n"
     ]
    }
   ],
   "source": [
    "occupations_labels = {k: [v] for k, v in occupations_label.items()}\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT ?o ?altLabel \n",
    "WHERE {\n",
    "  VALUES ?o { %s }\n",
    "  ?o skos:altLabel ?altLabel . FILTER (lang(?altLabel) = \"en\")\n",
    "}\"\"\" % ' '.join('wd:' + o for o in occupations) \n",
    "\n",
    "for result in evalSparql(query):\n",
    "    occupations_labels[result['o']['value'].replace('http://www.wikidata.org/entity/', '')].append(result['altLabel']['value'])\n",
    "\n",
    "print(occupations_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia articles\n",
    "\n",
    "Here we load the training and the testing sets. To save memory space we use a generator that will read the file each time we iterate over the training or the testing examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "def loadJson(filename):\n",
    "    with gzip.open(filename, 'rt') as fp:\n",
    "        for line in fp:\n",
    "            yield json.loads(line)\n",
    "\n",
    "class MakeIter(object):\n",
    "    def __init__(self, generator_func, **kwargs):\n",
    "        self.generator_func = generator_func\n",
    "        self.kwargs = kwargs\n",
    "    def __iter__(self):\n",
    "        return self.generator_func(**self.kwargs)\n",
    "\n",
    "training_set = MakeIter(loadJson, filename='wiki-train.json.gz')\n",
    "testing_set = MakeIter(loadJson, filename='wiki-test.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract occupations from summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Dictionnary extraction\n",
    "\n",
    "> Using ```occupations_labels``` dictionnary, identify all occupations for each articles. Complete the function below to evaluate the accuracy of such approach. It will serve as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4842586814146957"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_occ = dict()\n",
    "for key, occs in occupations_labels.items():\n",
    "    for occ in occs:\n",
    "        label_to_occ[occ.lower()] = key\n",
    "\n",
    "def predict_dictionnary(example, occupations_labels):\n",
    "    occs = []\n",
    "    summary = example['summary'].lower()\n",
    "    labels = label_to_occ.keys()\n",
    "    for label in labels:\n",
    "        if label in summary:\n",
    "            occs.append(label_to_occ[label])\n",
    "    return occs\n",
    "    \n",
    "def evaluate_dictionnary(training_set, occupations_labels):\n",
    "    nexample = 0\n",
    "    accuracy = 0.\n",
    "    prediction = None\n",
    "    for example in training_set:\n",
    "        prediction = predict_dictionnary(example, occupations_labels)\n",
    "        p = frozenset(prediction)\n",
    "        g = frozenset(example['occupations'])\n",
    "        accuracy += 1.*len(p & g) / len(p | g)\n",
    "        nexample += 1\n",
    "    return accuracy / nexample\n",
    "\n",
    "evaluate_dictionnary(training_set, occupations_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Simple neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the articles \"summary\" and we take the average of the word vectors.\n",
    "This is done with spacy loaded with the fast text vectors.\n",
    "To do the installation/loading [takes 8-10 minutes, dl 1.2Go]\n",
    "```\n",
    "pip3 install spacy\n",
    "wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/cc.en.300.vec.gz\n",
    "python3 -m spacy init-model en /tmp/en_vectors_wiki_lg --vectors-loc cc.en.300.vec.gz\n",
    "rm cc.en.300.vec.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nlp = spacy.load('/tmp/en_vectors_wiki_lg')\n",
    "\n",
    "def vectorize(dataset, nlp):\n",
    "    result = {}\n",
    "    for example in dataset:\n",
    "        doc = nlp(example['summary'], disable=['parser', 'tagger'])\n",
    "        result[example['title']] = {}\n",
    "        result[example['title']]['vector'] = doc.vector\n",
    "        result[example['title']]['summary'] = example['summary']\n",
    "        if 'occupations' in example:\n",
    "            result[example['title']]['occupations'] = example['occupations']\n",
    "    return result\n",
    "    \n",
    "vectorized_training = vectorize(training_set, nlp)\n",
    "vectorized_testing = vectorize(testing_set, nlp)\n",
    "nlp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427798"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.45162819e-02 -2.45802402e-02 -4.59302496e-03 -4.09372151e-02\n",
      " -4.47662771e-02 -4.18604538e-03 -3.15232435e-03 -1.44802360e-02\n",
      " -1.68499984e-02 -3.69651243e-03 -1.16255814e-02  1.43651171e-02\n",
      "  2.02674349e-03 -5.88953542e-03 -2.17011590e-02  1.02302311e-02\n",
      " -2.49313917e-02 -5.65232616e-03 -2.25581434e-02  8.29069968e-03\n",
      " -1.44069805e-03  2.25197673e-02 -6.81395701e-04 -1.37232570e-02\n",
      " -1.26674427e-02 -3.35569866e-02  1.10627888e-02 -2.37208814e-03\n",
      " -2.30000000e-02  7.58616179e-02 -5.03487710e-04 -2.51116175e-02\n",
      "  9.26511642e-03 -2.52558179e-02 -1.51058156e-02 -9.51627828e-03\n",
      "  1.17523270e-02  1.22441910e-03  1.08139520e-03  3.39302444e-03\n",
      "  2.20116391e-03  1.46860480e-02 -1.43686021e-02  5.76395402e-03\n",
      "  1.74162779e-02 -4.76220921e-02 -1.72569733e-02 -1.49988411e-02\n",
      " -1.77732538e-02  1.58907007e-02 -7.23255938e-03  2.43825577e-02\n",
      " -2.73104683e-02 -3.67430188e-02 -1.48802334e-02 -1.34825567e-02\n",
      " -3.14348824e-02  1.95930228e-02 -6.68605033e-04 -9.24302172e-03\n",
      "  1.56976283e-04 -1.65674444e-02 -1.30372085e-02  6.16298130e-05\n",
      " -3.63139645e-03  2.74534873e-03 -1.62697677e-02 -4.70697694e-03\n",
      "  5.48139494e-03  4.39302297e-03  4.65523303e-02  2.29872130e-02\n",
      "  2.72058025e-02 -5.52790612e-03  2.19720937e-02 -4.41581383e-02\n",
      "  1.33255811e-03  1.20244222e-02  3.49267460e-02  3.76593024e-02\n",
      "  8.65232572e-03 -6.52325572e-03 -1.90407019e-02  1.03569757e-02\n",
      "  1.09301973e-03 -6.28488278e-03  3.98965068e-02 -3.81744131e-02\n",
      " -1.35965087e-02  1.74023230e-02 -1.48686031e-02  5.78604685e-03\n",
      " -8.59186146e-03  4.74418374e-03  1.54720917e-02 -6.42325589e-03\n",
      " -1.58430226e-02 -2.98779178e-02 -1.54255824e-02  3.28209326e-02\n",
      "  2.43825577e-02  1.32907031e-03  1.80883706e-02 -2.72825565e-02\n",
      "  9.28488653e-03 -7.39418622e-03 -7.98023026e-03  1.84244160e-02\n",
      " -9.45350039e-04 -1.16825579e-02  1.15813862e-03 -2.10464321e-04\n",
      " -3.00813979e-03  4.75407019e-02 -8.32790602e-03  4.11511678e-03\n",
      " -1.25604663e-02  8.92209262e-03  7.64534995e-03 -2.65965052e-02\n",
      "  6.58837147e-03 -1.12011610e-02 -9.68022924e-03  1.60023291e-02\n",
      "  1.61629519e-04  3.20906974e-02 -1.59848798e-02  1.14162825e-02\n",
      " -2.40430199e-02  5.39906919e-02 -4.80814092e-03  3.02209193e-03\n",
      "  5.89418598e-03 -3.94418649e-03 -2.68058274e-02 -8.98256153e-03\n",
      " -2.94616278e-02  3.90697829e-03  4.68255766e-03  3.96162830e-03\n",
      " -2.68069748e-02 -2.68395394e-02 -9.76740339e-05  5.67557989e-03\n",
      "  4.43197712e-02 -1.38953477e-02 -3.69888335e-01  1.04639539e-02\n",
      "  1.55372089e-02 -1.35093015e-02 -8.09988379e-02  2.67802346e-02\n",
      "  2.21941881e-02 -7.86627829e-03 -1.00313956e-02  1.52511625e-02\n",
      "  1.45744160e-01  4.61395411e-03  7.26162829e-03  3.14453505e-02\n",
      " -7.95465056e-03 -1.25395320e-02  6.95348764e-03 -2.48023286e-03\n",
      "  6.17325725e-03  1.26546472e-02  1.03558144e-02 -1.21616265e-02\n",
      " -1.27907039e-03 -1.99348871e-02 -9.01860371e-03  4.25581448e-03\n",
      "  7.45790750e-02  1.02186035e-02 -9.93953645e-03  1.72848776e-02\n",
      " -1.03779081e-02  1.46616297e-02 -3.75465187e-03 -2.26953458e-02\n",
      "  5.36046689e-04  6.64511696e-02 -2.53790785e-02  5.80627881e-02\n",
      " -1.42732579e-02  9.22453254e-02 -1.12825576e-02 -2.51837187e-02\n",
      "  3.90697736e-03  5.96395321e-03 -3.02476659e-02  2.63883732e-02\n",
      " -1.69488378e-02  7.39418576e-03  1.60662793e-02 -1.68313961e-02\n",
      " -8.25814065e-03 -1.36965141e-02  7.30697624e-03  1.63453538e-02\n",
      " -4.15407047e-02  1.05633713e-01  1.53325591e-02  6.63023209e-03\n",
      "  3.93279046e-02 -1.27697680e-02 -5.95697621e-03 -8.67441762e-03\n",
      "  1.58593040e-02  9.42093134e-03 -4.15697647e-03  1.34639572e-02\n",
      " -4.10383604e-02 -2.82325619e-03 -2.43790708e-02 -4.02325485e-03\n",
      "  1.65058132e-02  4.21395432e-03  1.25813941e-02  1.64744183e-02\n",
      " -2.81162816e-03  1.34813897e-02 -8.19302350e-03 -7.04767322e-03\n",
      "  1.67139638e-02  1.43581396e-02  1.20023256e-02  4.96162800e-03\n",
      "  1.76325571e-02 -7.07674446e-03 -4.24197726e-02 -2.34697610e-02\n",
      " -1.86058115e-02 -2.32790736e-03  2.98906974e-02  1.53604464e-03\n",
      "  1.95941851e-02 -2.67104693e-02 -1.12453466e-02 -2.54534930e-03\n",
      " -4.29302268e-03  3.56558077e-02 -4.36046888e-04 -8.16406980e-02\n",
      "  5.04779041e-01 -2.18813960e-02  1.15883695e-02  2.14848872e-02\n",
      "  7.80581404e-03  1.55116236e-02 -1.11523261e-02  4.61628864e-04\n",
      "  1.72918607e-02  1.43034859e-02  2.05546506e-02 -8.23488459e-03\n",
      " -3.16290706e-02 -4.83953534e-03 -1.82697661e-02  2.02907110e-03\n",
      " -3.51163093e-04  1.10220918e-02 -8.54755938e-02 -2.68255756e-03\n",
      "  1.83174424e-02  1.91116314e-02 -4.73488262e-03 -8.08255840e-03\n",
      "  1.37906978e-02 -7.76046468e-03 -2.82767452e-02 -2.99069774e-03\n",
      "  1.06569799e-02 -5.99999772e-03  1.11883730e-02  4.28720983e-03\n",
      " -3.12255807e-02 -8.07186142e-02  8.59302282e-03 -8.11744668e-03\n",
      " -5.36279054e-03  1.87046509e-02 -1.10972092e-01 -3.07988375e-02\n",
      "  9.47441999e-03 -1.03662787e-02  1.16337193e-02  3.22093032e-02\n",
      " -2.69790720e-02  2.25430205e-02 -1.49802361e-02 -1.05290683e-02\n",
      " -4.36534919e-02  6.34883530e-04 -2.83197612e-02 -1.37674408e-02\n",
      " -1.50220934e-02  1.30851150e-01 -1.22430259e-02  2.38767453e-02]\n"
     ]
    }
   ],
   "source": [
    "v = vectorized_training['George_Washington']['vector']\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the vectorized_training into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDict(d, percent):\n",
    "    split_idx = int(len(d) * percent)\n",
    "    d1 = dict(list(d.items())[: split_idx])\n",
    "    d2 = dict(list(d.items())[split_idx:])                \n",
    "\n",
    "    return d1, d2\n",
    "\n",
    "vectorized_training_test, vectorized_training_train = splitDict(vectorized_training, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342239"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_training_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We encode the data\n",
    "import numpy as np\n",
    "\n",
    "def encode_data(vectorized_data):\n",
    "    X = np.array([vectorized_data[article]['vector'] for article in vectorized_data])\n",
    "    y = np.array([[(1 if occupation in vectorized_data[article]['occupations'] else 0)\n",
    "                        for occupation in occupations ] for article in vectorized_data])\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = encode_data(vectorized_training_train)\n",
    "X_test, y_test = encode_data(vectorized_training_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342239, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342239, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using keras, define a sequential neural network with two layers. Use categorical_crossentropy as a loss function and softmax as the activation function of the output layer\n",
    "\n",
    "You can look into the documentation here: https://keras.io/getting-started/sequential-model-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=300))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "\n",
    "optimizer = Adam()\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 308015 samples, validate on 34224 samples\n",
      "Epoch 1/50\n",
      "308015/308015 [==============================] - 3s 10us/step - loss: 3.5608 - acc: 0.4881 - val_loss: 2.0247 - val_acc: 0.6362\n",
      "Epoch 2/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 2.3868 - acc: 0.6581 - val_loss: 1.7224 - val_acc: 0.6907\n",
      "Epoch 3/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 2.1847 - acc: 0.6880 - val_loss: 1.6056 - val_acc: 0.7125\n",
      "Epoch 4/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 2.0853 - acc: 0.7033 - val_loss: 1.5498 - val_acc: 0.7234\n",
      "Epoch 5/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 2.0255 - acc: 0.7115 - val_loss: 1.5037 - val_acc: 0.7321\n",
      "Epoch 6/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.9833 - acc: 0.7176 - val_loss: 1.4793 - val_acc: 0.7359\n",
      "Epoch 7/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.9517 - acc: 0.7213 - val_loss: 1.4538 - val_acc: 0.7409\n",
      "Epoch 8/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.9250 - acc: 0.7251 - val_loss: 1.4335 - val_acc: 0.7466\n",
      "Epoch 9/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.9036 - acc: 0.7273 - val_loss: 1.4188 - val_acc: 0.7452\n",
      "Epoch 10/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8846 - acc: 0.7293 - val_loss: 1.4096 - val_acc: 0.7491\n",
      "Epoch 11/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8676 - acc: 0.7324 - val_loss: 1.3920 - val_acc: 0.7537\n",
      "Epoch 12/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8522 - acc: 0.7333 - val_loss: 1.3919 - val_acc: 0.7550\n",
      "Epoch 13/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8387 - acc: 0.7349 - val_loss: 1.3776 - val_acc: 0.7560\n",
      "Epoch 14/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8256 - acc: 0.7372 - val_loss: 1.3729 - val_acc: 0.7587\n",
      "Epoch 15/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8138 - acc: 0.7383 - val_loss: 1.3733 - val_acc: 0.7553\n",
      "Epoch 16/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8025 - acc: 0.7391 - val_loss: 1.3637 - val_acc: 0.7564\n",
      "Epoch 17/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 1.7916 - acc: 0.7408 - val_loss: 1.3510 - val_acc: 0.7603\n",
      "Epoch 18/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 1.7818 - acc: 0.7417 - val_loss: 1.3484 - val_acc: 0.7622\n",
      "Epoch 19/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7721 - acc: 0.7434 - val_loss: 1.3472 - val_acc: 0.7599\n",
      "Epoch 20/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 1.7634 - acc: 0.7436 - val_loss: 1.3448 - val_acc: 0.7615\n",
      "Epoch 21/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7554 - acc: 0.7450 - val_loss: 1.3347 - val_acc: 0.7649\n",
      "Epoch 22/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7462 - acc: 0.7463 - val_loss: 1.3367 - val_acc: 0.7639\n",
      "Epoch 23/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7388 - acc: 0.7467 - val_loss: 1.3338 - val_acc: 0.7641\n",
      "Epoch 24/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7308 - acc: 0.7481 - val_loss: 1.3379 - val_acc: 0.7646\n",
      "Epoch 25/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7240 - acc: 0.7486 - val_loss: 1.3341 - val_acc: 0.7644\n",
      "Epoch 26/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7173 - acc: 0.7494 - val_loss: 1.3279 - val_acc: 0.7634\n",
      "Epoch 27/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7097 - acc: 0.7500 - val_loss: 1.3268 - val_acc: 0.7610\n",
      "Epoch 28/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7030 - acc: 0.7510 - val_loss: 1.3203 - val_acc: 0.7664\n",
      "Epoch 29/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6968 - acc: 0.7518 - val_loss: 1.3266 - val_acc: 0.7640\n",
      "Epoch 30/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 1.6907 - acc: 0.7525 - val_loss: 1.3202 - val_acc: 0.7654\n",
      "Epoch 31/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6837 - acc: 0.7533 - val_loss: 1.3224 - val_acc: 0.7615\n",
      "Epoch 32/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6777 - acc: 0.7537 - val_loss: 1.3208 - val_acc: 0.7650\n",
      "Epoch 33/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6719 - acc: 0.7541 - val_loss: 1.3257 - val_acc: 0.7629\n",
      "Epoch 34/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6654 - acc: 0.7554 - val_loss: 1.3239 - val_acc: 0.7624\n",
      "Epoch 35/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6594 - acc: 0.7557 - val_loss: 1.3201 - val_acc: 0.7618\n",
      "Epoch 36/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6540 - acc: 0.7566 - val_loss: 1.3231 - val_acc: 0.7631\n",
      "Epoch 37/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6487 - acc: 0.7568 - val_loss: 1.3197 - val_acc: 0.7663\n",
      "Epoch 38/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6427 - acc: 0.7581 - val_loss: 1.3195 - val_acc: 0.7684\n",
      "Epoch 39/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6375 - acc: 0.7579 - val_loss: 1.3150 - val_acc: 0.7657\n",
      "Epoch 40/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6323 - acc: 0.7581 - val_loss: 1.3244 - val_acc: 0.7619\n",
      "Epoch 41/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6270 - acc: 0.7595 - val_loss: 1.3234 - val_acc: 0.7617\n",
      "Epoch 42/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6214 - acc: 0.7601 - val_loss: 1.3270 - val_acc: 0.7601\n",
      "Epoch 43/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6162 - acc: 0.7604 - val_loss: 1.3214 - val_acc: 0.7638\n",
      "Epoch 44/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6115 - acc: 0.7606 - val_loss: 1.3241 - val_acc: 0.7622\n",
      "Epoch 45/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6061 - acc: 0.7616 - val_loss: 1.3275 - val_acc: 0.7599\n",
      "Epoch 46/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6012 - acc: 0.7622 - val_loss: 1.3222 - val_acc: 0.7651\n",
      "Epoch 47/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.5964 - acc: 0.7628 - val_loss: 1.3247 - val_acc: 0.7631\n",
      "Epoch 48/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.5910 - acc: 0.7632 - val_loss: 1.3250 - val_acc: 0.7668\n",
      "Epoch 49/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.5868 - acc: 0.7636 - val_loss: 1.3247 - val_acc: 0.7598\n",
      "Epoch 50/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.5814 - acc: 0.7645 - val_loss: 1.3295 - val_acc: 0.7646\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=1024, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Complete the function predict: output the list of occupations where the corresponding neuron on the output layer of our model has a value > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q177220', 'Q639669', 'Q33999'}\n"
     ]
    }
   ],
   "source": [
    "def predict_nn(model, article_name, vectorized_dataset):\n",
    "    input_vector = vectorized_dataset[article_name]['vector'].reshape((1, 300))\n",
    "    scores = model.predict(input_vector).reshape(100)\n",
    "    predictions = np.where(scores > 0.1)[0]\n",
    "#     print(scores[predictions])\n",
    "    return set(np.array(occupations)[predictions])\n",
    "\n",
    "print(predict_nn(model, 'Elvis_Presley', vectorized_training))\n",
    "# should be {'Q177220'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nn(vectorized_training, model):\n",
    "    nexample = 0\n",
    "    accuracy = 0.\n",
    "    prediction = None\n",
    "    for article_name in vectorized_training:\n",
    "        prediction = predict_nn(model, article_name, vectorized_training)\n",
    "        p = frozenset(prediction)\n",
    "        g = frozenset(vectorized_training[article_name]['occupations'])\n",
    "        accuracy += 1.*len(p & g) / len(p | g)\n",
    "        nexample += 1\n",
    "    return accuracy / nexample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7048576643356244\n",
      "0.6662116943899452\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_nn(vectorized_training_train, model))\n",
    "print(evaluate_nn(vectorized_training_test, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Your approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, GRU, Dropout, Conv1D, MaxPooling1D, MaxPooling1D, Bidirectional, BatchNormalization, LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dataset into summaries, titles and occupations\n",
    "def parse(dataset):\n",
    "    titles = []\n",
    "    summaries = []\n",
    "    occs = []\n",
    "    ml = 0 \n",
    "    for example in dataset:\n",
    "        titles.append(example['title'])\n",
    "        summaries.append(example['summary'].lower())        \n",
    "        ml = max(len(example['summary']), ml)\n",
    "        if 'occupations' in example:\n",
    "            occs.append(example['occupations'])\n",
    "        else:\n",
    "            occs.append([])\n",
    "    return titles, summaries, occs\n",
    "    \n",
    "titles_train, summaries_train, occs_train = parse(training_set)\n",
    "titles_test, summaries_test, occs_test = parse(testing_set)\n",
    "\n",
    "# split the training_set into train and test set \n",
    "s = int(len(titles_train) * 0.9)\n",
    "titles_train_train, summaries_train_train, occs_train_train = titles_train[:s], summaries_train[:s], occs_train[:s]\n",
    "titles_train_test, summaries_train_test, occs_train_test = titles_train[s:], summaries_train[s:], occs_train[s:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 397985 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(titles_train_train)\n",
    "maxlen = 300\n",
    "training_samples = int(n_samples * 0.9)\n",
    "validation_samples = n_samples - training_samples\n",
    "max_words = 400000\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(summaries_train_train)\n",
    "\n",
    "# convert text to sequences\n",
    "sequences =  tokenizer.texts_to_sequences(summaries_train_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(summaries_train_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found', len(word_index), 'unique tokens.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_occs_to_labels(occupations, occs_train):\n",
    "    labels = []\n",
    "    for i in range(len(occs_train)):\n",
    "        label = []\n",
    "        \n",
    "        for occ in occupations:\n",
    "            if occ in occs_train[i]:\n",
    "                label.append(1)\n",
    "            else:\n",
    "                label.append(0)\n",
    "        labels.append(label)\n",
    "    return np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad the sequences so all the sequences have the same length\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "data_test = pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "# encode the occupations\n",
    "labels = convert_occs_to_labels(occupations, occs_train_train)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# shuffle the data\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# split into training and validation set\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load the Glove word embedding\n",
    "glove_dir = 'glove.6B'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found', len(embeddings_index), 'word vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build embedding matrix to load into embedding layer\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 300, 300)          120000000 \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 300, 32)           28832     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 300, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 150, 32)           0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 150, 32)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 150, 32)           3104      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 150, 32)           128       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 75, 32)            0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 75, 32)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_9 (Bidirection (None, 75, 64)            12480     \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 64)                18624     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 100)               12900     \n",
      "=================================================================\n",
      "Total params: 120,085,028\n",
      "Trainable params: 120,084,644\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "\n",
    "model.add(Conv1D(64,kernel_size=3,padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv1D(128,kernel_size=3,padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv1D(256,kernel_size=3,padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "model.add(Bidirectional(GRU(100, return_sequences=True, recurrent_dropout = 0.1, dropout = 0.1)))\n",
    "\n",
    "model.add(Bidirectional(GRU(100, return_sequences=True, recurrent_dropout = 0.1, dropout = 0.1)))\n",
    "\n",
    "model.add(Bidirectional(GRU(100, return_sequences=True, recurrent_dropout = 0.1, dropout = 0.1)))\n",
    "\n",
    "model.add(Bidirectional(GRU(100, recurrent_dropout = 0.1, dropout = 0.1)))\n",
    "\n",
    "\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Dense(100, activation='softmax'))\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Glove embedding in the model\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False # we will not update this layer during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as tfb\n",
    "from keras import backend as K\n",
    "\n",
    "POS_WEIGHT = 10  # multiplier for positive targets\n",
    "\n",
    "def weighted_binary_crossentropy(target, output):\n",
    "    # transform back to logits\n",
    "    _epsilon = tfb._to_tensor(tfb.epsilon(), output.dtype.base_dtype)\n",
    "    output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "    output = tf.log(output / (1 - output))\n",
    "    # compute weighted loss\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(targets=target,\n",
    "                                                    logits=output,\n",
    "                                                    pos_weight=POS_WEIGHT)\n",
    "    return tf.reduce_mean(loss, axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 346612 samples, validate on 38513 samples\n",
      "Epoch 1/100\n",
      "346612/346612 [==============================] - 140s 403us/step - loss: 0.2930 - acc: 0.4937 - val_loss: 0.1364 - val_acc: 0.6595\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.13639, saving model to best_model.h5\n",
      "Epoch 2/100\n",
      "346612/346612 [==============================] - 131s 379us/step - loss: 0.1219 - acc: 0.6728 - val_loss: 0.1057 - val_acc: 0.7045\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.13639 to 0.10568, saving model to best_model.h5\n",
      "Epoch 3/100\n",
      "346612/346612 [==============================] - 130s 374us/step - loss: 0.1090 - acc: 0.7025 - val_loss: 0.0980 - val_acc: 0.7190\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.10568 to 0.09803, saving model to best_model.h5\n",
      "Epoch 4/100\n",
      "346612/346612 [==============================] - 131s 377us/step - loss: 0.1031 - acc: 0.7165 - val_loss: 0.0948 - val_acc: 0.7360\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.09803 to 0.09477, saving model to best_model.h5\n",
      "Epoch 5/100\n",
      "346612/346612 [==============================] - 132s 380us/step - loss: 0.0993 - acc: 0.7258 - val_loss: 0.0927 - val_acc: 0.7358\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09477 to 0.09273, saving model to best_model.h5\n",
      "Epoch 6/100\n",
      "346612/346612 [==============================] - 131s 378us/step - loss: 0.0966 - acc: 0.7320 - val_loss: 0.0905 - val_acc: 0.7385\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09273 to 0.09049, saving model to best_model.h5\n",
      "Epoch 7/100\n",
      "346612/346612 [==============================] - 131s 377us/step - loss: 0.0946 - acc: 0.7353 - val_loss: 0.0890 - val_acc: 0.7445\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.09049 to 0.08898, saving model to best_model.h5\n",
      "Epoch 8/100\n",
      "346612/346612 [==============================] - 131s 378us/step - loss: 0.0932 - acc: 0.7389 - val_loss: 0.0874 - val_acc: 0.7482\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.08898 to 0.08738, saving model to best_model.h5\n",
      "Epoch 9/100\n",
      "346612/346612 [==============================] - 129s 373us/step - loss: 0.0920 - acc: 0.7416 - val_loss: 0.0880 - val_acc: 0.7457\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.08738\n",
      "Epoch 10/100\n",
      "346612/346612 [==============================] - 130s 376us/step - loss: 0.0910 - acc: 0.7427 - val_loss: 0.0868 - val_acc: 0.7504\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.08738 to 0.08682, saving model to best_model.h5\n",
      "Epoch 11/100\n",
      "346612/346612 [==============================] - 136s 391us/step - loss: 0.0901 - acc: 0.7443 - val_loss: 0.0864 - val_acc: 0.7468\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.08682 to 0.08638, saving model to best_model.h5\n",
      "Epoch 12/100\n",
      "346612/346612 [==============================] - 136s 394us/step - loss: 0.0894 - acc: 0.7450 - val_loss: 0.0855 - val_acc: 0.7530\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.08638 to 0.08554, saving model to best_model.h5\n",
      "Epoch 13/100\n",
      "346612/346612 [==============================] - 130s 376us/step - loss: 0.0888 - acc: 0.7465 - val_loss: 0.0849 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.08554 to 0.08486, saving model to best_model.h5\n",
      "Epoch 14/100\n",
      "346612/346612 [==============================] - 131s 377us/step - loss: 0.0883 - acc: 0.7469 - val_loss: 0.0843 - val_acc: 0.7514\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.08486 to 0.08433, saving model to best_model.h5\n",
      "Epoch 15/100\n",
      "346612/346612 [==============================] - 131s 379us/step - loss: 0.0878 - acc: 0.7477 - val_loss: 0.0850 - val_acc: 0.7531\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.08433\n",
      "Epoch 16/100\n",
      "346612/346612 [==============================] - 130s 375us/step - loss: 0.0873 - acc: 0.7489 - val_loss: 0.0847 - val_acc: 0.7476\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.08433\n",
      "Epoch 17/100\n",
      "346612/346612 [==============================] - 131s 378us/step - loss: 0.0870 - acc: 0.7489 - val_loss: 0.0841 - val_acc: 0.7530\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.08433 to 0.08412, saving model to best_model.h5\n",
      "Epoch 18/100\n",
      "346612/346612 [==============================] - 130s 374us/step - loss: 0.0866 - acc: 0.7506 - val_loss: 0.0842 - val_acc: 0.7569\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.08412\n",
      "Epoch 19/100\n",
      "346612/346612 [==============================] - 130s 374us/step - loss: 0.0863 - acc: 0.7507 - val_loss: 0.0844 - val_acc: 0.7584\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.08412\n",
      "Epoch 20/100\n",
      "346612/346612 [==============================] - 136s 392us/step - loss: 0.0860 - acc: 0.7514 - val_loss: 0.0833 - val_acc: 0.7569\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.08412 to 0.08333, saving model to best_model.h5\n",
      "Epoch 21/100\n",
      "346612/346612 [==============================] - 130s 376us/step - loss: 0.0857 - acc: 0.7521 - val_loss: 0.0830 - val_acc: 0.7587\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.08333 to 0.08299, saving model to best_model.h5\n",
      "Epoch 22/100\n",
      "346612/346612 [==============================] - 132s 380us/step - loss: 0.0854 - acc: 0.7524 - val_loss: 0.0835 - val_acc: 0.7597\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.08299\n",
      "Epoch 23/100\n",
      "346612/346612 [==============================] - 138s 397us/step - loss: 0.0851 - acc: 0.7529 - val_loss: 0.0842 - val_acc: 0.7580\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.08299\n",
      "Epoch 24/100\n",
      " 16000/346612 [>.............................] - ETA: 2:04 - loss: 0.0835 - acc: 0.7544"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-743d39eb8877>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     validation_data=(x_val, y_val))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=10,\n",
    "                              verbose=1, mode='auto'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', \n",
    "                      factor=0.3, \n",
    "                      patience=5, \n",
    "                      verbose=1, \n",
    "                      mode='auto', \n",
    "                      min_delta=0.0001, \n",
    "                      cooldown=0, \n",
    "                      min_lr=0),\n",
    "    ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True, verbose=1, mode='min')\n",
    "]\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "#     loss='categorical_crossentropy',\n",
    "#     loss='binary_crossentropy',\n",
    "    loss=weighted_binary_crossentropy,\n",
    "    metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "    epochs=100,\n",
    "    callbacks=callbacks,\n",
    "    batch_size=1000,\n",
    "    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q177220', 'Q10800557', 'Q28389', 'Q486748', 'Q855091']\n"
     ]
    }
   ],
   "source": [
    "title = 'Elvis_Presley'\n",
    "idx = titles_train_train.index(title)\n",
    "input_vector = data[idx].reshape(1, maxlen)\n",
    "print(occs_train_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elvis Aaron Presley was an American singer and actor. Regarded as one of the most significant cultural icons of the 20th century, he is often referred to as the \"King of Rock and Roll\" or simply the \"King\".\n",
      "[0.9999161]\n",
      "[0.9999161]\n",
      "[0.9999161]\n",
      "[0.9999161]\n",
      "[0.9999161]\n",
      "[0.9999161]\n",
      "({'Q82955'}, {'Q82955'}, {'Q82955'}, {'Q82955'}, {'Q82955'}, {'Q82955'})\n"
     ]
    }
   ],
   "source": [
    "def predict_nn_2(model, input_vector, print_score = False):\n",
    "    \n",
    "    scores = model.predict(input_vector).reshape(100)\n",
    "    predictions1 = np.where(scores > 0.6)[0]\n",
    "    predictions2 = np.where(scores > 0.7)[0]\n",
    "    predictions3 = np.where(scores > 0.8)[0]\n",
    "    predictions4 = np.where(scores > 0.9)[0]\n",
    "    predictions5 = np.where(scores > 0.95)[0]\n",
    "    predictions6 = np.where(scores > 0.99)[0]\n",
    "    if print_score:\n",
    "        print(scores[predictions1])\n",
    "        print(scores[predictions2])\n",
    "        print(scores[predictions3])\n",
    "        print(scores[predictions4])\n",
    "        print(scores[predictions5])\n",
    "        print(scores[predictions6])\n",
    "    res1 = set(np.array(occupations)[predictions1])\n",
    "    res2 = set(np.array(occupations)[predictions2])\n",
    "    res3 = set(np.array(occupations)[predictions3])\n",
    "    res4 = set(np.array(occupations)[predictions4])\n",
    "    res5 = set(np.array(occupations)[predictions5])\n",
    "    res6 = set(np.array(occupations)[predictions6])\n",
    "    return res1, res2, res3, res4, res5, res6\n",
    "\n",
    "title = 'Elvis_Presley'\n",
    "idx = titles_train.index(title)\n",
    "print(summaries_train_train[idx])\n",
    "\n",
    "print(predict_nn_2(model, input_vector, print_score=True))\n",
    "# should be {'Q177220'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42792"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "=========================\n",
      "0.0234  :  0.0156\n",
      "0.0234  :  0.0162\n",
      "0.0234  :  0.0164\n",
      "0.0234  :  0.0162\n",
      "0.0234  :  0.0143\n",
      "0.0234  :  0.0144\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-f59e8a2b2a1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# print(evaluate_nn_2(titles_train, summaries_train, occs_train, model))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_nn_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles_train_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moccs_train_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-48-f59e8a2b2a1f>\u001b[0m in \u001b[0;36mevaluate_nn_2\u001b[0;34m(titles, input_vectors, occs, model)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0minput_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mprediction1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_nn_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-ae8333056196>\u001b[0m in \u001b[0;36mpredict_nn_2\u001b[0;34m(model, input_vector, print_score)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_nn_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpredictions1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpredictions2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate_nn_2(titles, input_vectors, occs, model):\n",
    "    nexample = len(titles)\n",
    "    accuracy1 = 0.\n",
    "    accuracy2 = 0.\n",
    "    accuracy3 = 0.\n",
    "    accuracy4 = 0.\n",
    "    accuracy5 = 0.\n",
    "    accuracy6 = 0.\n",
    "    prediction = None\n",
    "    for i in range(len(titles)):        \n",
    "        input_vector = input_vectors[i].reshape(1, -1)\n",
    "        prediction1, prediction2, prediction3, prediction4, prediction5, prediction6 = predict_nn_2(model, input_vector)\n",
    "        p1 = frozenset(prediction1)\n",
    "        p2 = frozenset(prediction2)\n",
    "        p3 = frozenset(prediction3)\n",
    "        p4 = frozenset(prediction4)\n",
    "        p5 = frozenset(prediction5)\n",
    "        p6 = frozenset(prediction6)\n",
    "        g = frozenset(occs[i])\n",
    "        accuracy1 += 1. / nexample * len(p1 & g) / len(p1 | g)\n",
    "        accuracy2 += 1. / nexample * len(p2 & g) / len(p2 | g)\n",
    "        accuracy3 += 1. / nexample * len(p3 & g) / len(p3 | g)\n",
    "        accuracy4 += 1. / nexample * len(p4 & g) / len(p4 | g)\n",
    "        accuracy5 += 1. / nexample * len(p5 & g) / len(p5 | g)\n",
    "        accuracy6 += 1. / nexample * len(p5 & g) / len(p6 | g)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"=========================\")\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy1, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy2, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy3, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy4, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy5, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy6, 4))\n",
    "    return accuracy1, accuracy2, accuracy3, accuracy4, accuracy5, accuracy6\n",
    "\n",
    "# print(evaluate_nn_2(titles_train, summaries_train, occs_train, model))\n",
    "print(evaluate_nn_2(titles_train_test, data_test, occs_train_test, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate_nn_2(titles_train_train, data, occs_train_train, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***IMPORTANT*** Output format of requested file 'results.json.gz': each line must be a json string representing a dictionnary:\n",
    "> ```{ 'title': THE_ARTICLE_NAME, 'prediction': [THE_LIST_OF_OCCUPATIONS]}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example if testset_solutions is a dictionnary: article_name (key) -> prediction_list (value) use this function:\n",
    "def export(testset_solutions):\n",
    "    with gzip.open('results.json.gz', 'wt') as output:\n",
    "        for article in testset_solutions:\n",
    "            output.write(json.dumps({'title':article, 'prediction':testset_solutions[article]}) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
