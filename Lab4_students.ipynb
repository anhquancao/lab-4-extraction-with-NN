{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import\n",
    "## Question 0 - Get common wikidata occupations\n",
    "\n",
    "> Write a sparql query that retrieves the top 100 occupations on wikidata (wikidata property P106).\n",
    "\n",
    "You may use the interface https://query.wikidata.org/ to try different queries. Here are some example sparql queries: https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT ?o (COUNT(?person) AS ?count) WHERE \n",
    "{\n",
    "   ?person wdt:P106 ?o\n",
    "}\n",
    "GROUP BY ?o\n",
    "ORDER BY DESC(?count)\n",
    "LIMIT 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following assertion should pass if your answer is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "occupations = ['Q82955', 'Q937857', 'Q36180', 'Q33999', 'Q1650915', 'Q1028181', 'Q1930187', 'Q177220', 'Q1622272', 'Q49757', 'Q36834', 'Q40348', 'Q47064', 'Q639669', 'Q10800557', 'Q201788', 'Q2526255', 'Q43845', 'Q28389', 'Q42973', 'Q10871364', 'Q39631', 'Q193391', 'Q482980', 'Q483501', 'Q11513337', 'Q3665646', 'Q12299841', 'Q19204627', 'Q16533', 'Q81096', 'Q11774891', 'Q188094', 'Q1281618', 'Q333634', 'Q189290', 'Q250867', 'Q33231', 'Q2259451', 'Q42603', 'Q628099', 'Q37226', 'Q2309784', 'Q901', 'Q2066131', 'Q6625963', 'Q10798782', 'Q2374149', 'Q170790', 'Q4610556', 'Q185351', 'Q486748', 'Q3055126', 'Q753110', 'Q4964182', 'Q169470', 'Q158852', 'Q1234713', 'Q14089670', 'Q10873124', 'Q3282637', 'Q593644', 'Q947873', 'Q13414980', 'Q131524', 'Q11338576', 'Q15117302', 'Q488205', 'Q14467526', 'Q183945', 'Q10843402', 'Q13382576', 'Q13141064', 'Q214917', 'Q855091', 'Q644687', 'Q19595175', 'Q121594', 'Q2865819', 'Q16010345', 'Q1231865', 'Q2405480', 'Q350979', 'Q3400985', 'Q13365117', 'Q10833314', 'Q3621491', 'Q15981151', 'Q212980', 'Q16145150', 'Q1792450', 'Q15296811', 'Q15627169', 'Q2306091', 'Q4263842', 'Q806798', 'Q5716684', 'Q2516866', 'Q3387717', 'Q131512']\n",
    "\n",
    "def evalSparql(query):\n",
    "    return requests.post('https://query.wikidata.org/sparql', data=query, headers={\n",
    "        'content-type': 'application/sparql-query',\n",
    "        'accept': 'application/json',\n",
    "        'user-agent': 'User:Tpt'\n",
    "    }).json()['results']['bindings']\n",
    "\n",
    "myOccupations = [val['o']['value'].replace('http://www.wikidata.org/entity/', '') \n",
    "                 for val in evalSparql(query)]\n",
    "assert(frozenset(occupations) == frozenset(myOccupations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupations labels\n",
    "\n",
    "We load the labels of the occupations from Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q82955': 'politician', 'Q121594': 'professor', 'Q177220': 'singer', 'Q169470': 'physicist', 'Q170790': 'mathematician', 'Q81096': 'engineer', 'Q201788': 'historian', 'Q188094': 'economist', 'Q212980': 'psychologist', 'Q214917': 'playwright', 'Q131524': 'entrepreneur', 'Q183945': 'record producer', 'Q193391': 'diplomat', 'Q189290': 'military officer', 'Q185351': 'jurist', 'Q350979': 'zoologist', 'Q483501': 'artist', 'Q482980': 'author', 'Q333634': 'translator', 'Q158852': 'conductor', 'Q486748': 'pianist', 'Q488205': 'singer-songwriter', 'Q250867': 'Catholic priest', 'Q593644': 'chemist', 'Q639669': 'musician', 'Q644687': 'illustrator', 'Q628099': 'association football manager', 'Q855091': 'guitarist', 'Q937857': 'association football player', 'Q947873': 'television presenter', 'Q806798': 'banker', 'Q1028181': 'painter', 'Q753110': 'songwriter', 'Q1234713': 'theologian', 'Q1281618': 'sculptor', 'Q1622272': 'university teacher', 'Q1792450': 'art historian', 'Q1650915': 'researcher', 'Q1930187': 'journalist', 'Q2306091': 'sociologist', 'Q2374149': 'botanist', 'Q2526255': 'film director', 'Q2516866': 'publisher', 'Q2066131': 'athlete', 'Q2405480': 'voice actor', 'Q1231865': 'pedagogue', 'Q2865819': 'opera singer', 'Q2259451': 'stage actor', 'Q3282637': 'film producer', 'Q3387717': 'theatre director', 'Q3055126': 'entomologist', 'Q3400985': 'academic', 'Q3665646': 'basketball player', 'Q3621491': 'archaeologist', 'Q4610556': 'model', 'Q4263842': 'literary critic', 'Q4964182': 'philosopher', 'Q5716684': 'dancer', 'Q6625963': 'novelist', 'Q10843402': 'swimmer', 'Q10833314': 'tennis player', 'Q10871364': 'baseball player', 'Q10798782': 'television actor', 'Q10873124': 'chess player', 'Q10800557': 'film actor', 'Q11338576': 'boxer', 'Q11513337': 'athletics competitor', 'Q2309784': 'sport cyclist', 'Q11774891': 'ice hockey player', 'Q12299841': 'cricketer', 'Q13141064': 'badminton player', 'Q13365117': 'handball player', 'Q13414980': 'Australian rules footballer', 'Q14089670': 'rugby union player', 'Q14467526': 'linguist', 'Q15117302': 'volleyball player', 'Q15627169': 'trade unionist', 'Q15981151': 'jazz musician', 'Q16010345': 'performer', 'Q131512': 'agriculturer', 'Q13382576': 'rower', 'Q19204627': 'American football player', 'Q15296811': 'drawer', 'Q19595175': 'amateur wrestler', 'Q16145150': 'music pedagogue', 'Q901': 'scientist', 'Q33999': 'actor', 'Q33231': 'photographer', 'Q36834': 'composer', 'Q16533': 'judge', 'Q40348': 'lawyer', 'Q36180': 'writer', 'Q42973': 'architect', 'Q43845': 'businessperson', 'Q39631': 'physician', 'Q28389': 'screenwriter', 'Q42603': 'priest', 'Q49757': 'poet', 'Q37226': 'teacher', 'Q47064': 'military personnel'}\n"
     ]
    }
   ],
   "source": [
    "occupations_label = {}\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT ?o ?oLabel \n",
    "WHERE { \n",
    "    VALUES ?o { %s } \n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\"\"\"% ' '.join('wd:' + o for o in occupations)\n",
    "\n",
    "for result in evalSparql(query):\n",
    "    occupations_label[result['o']['value'].replace('http://www.wikidata.org/entity/', '')] = result['oLabel']['value']\n",
    "\n",
    "print(occupations_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load *all* the labels of the occupations from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q82955': ['politician', 'political leader', 'polit.', 'political figure'], 'Q121594': ['professor', 'Prof.'], 'Q177220': ['singer', 'vocalist'], 'Q169470': ['physicist'], 'Q170790': ['mathematician'], 'Q81096': ['engineer'], 'Q201788': ['historian', 'historians', 'historiographer'], 'Q188094': ['economist'], 'Q212980': ['psychologist'], 'Q214917': ['playwright', 'Playwright, dramatist', 'dramatist', 'scriptwriter', 'playwrite'], 'Q131524': ['entrepreneur'], 'Q183945': ['record producer', 'music producer'], 'Q193391': ['diplomat'], 'Q189290': ['military officer', 'army officer', 'officer'], 'Q185351': ['jurist'], 'Q350979': ['zoologist', 'zooligist'], 'Q483501': ['artist'], 'Q482980': ['author'], 'Q333634': ['translator'], 'Q158852': ['conductor', 'Conducting'], 'Q486748': ['pianist'], 'Q488205': ['singer-songwriter', 'singer/songwriter', 'singer songwriter', 'singersongwriter'], 'Q250867': ['Catholic priest', 'Roman Catholic priest', 'Catholic presbyter', 'Roman Catholic presbyter'], 'Q593644': ['chemist', 'chemists'], 'Q639669': ['musician'], 'Q644687': ['illustrator'], 'Q628099': ['association football manager', 'football manager', 'association football coach', 'football coach', 'soccer coach', 'soccer manager'], 'Q855091': ['guitarist', 'guitar player'], 'Q937857': ['association football player', 'footballer', 'football player', 'association footballer', 'soccer player'], 'Q947873': ['television presenter', 'TV presenter', 'hostess', 'TV host', 'TV personality', 'host', 'television personality', 'television host'], 'Q806798': ['banker', 'Private Banker', 'private sector banker'], 'Q1028181': ['painter'], 'Q753110': ['songwriter', 'song writer'], 'Q1234713': ['theologian', 'religious scholar'], 'Q1281618': ['sculptor'], 'Q1622272': ['university teacher', 'lecturer', 'university teachers', 'college lecturer', 'college professor'], 'Q1792450': ['art historian'], 'Q1650915': ['researcher'], 'Q1930187': ['journalist', 'journo'], 'Q2306091': ['sociologist'], 'Q2374149': ['botanist', 'botany', 'plant scientist'], 'Q2526255': ['film director', 'director', 'movie director'], 'Q2516866': ['publisher'], 'Q2066131': ['athlete', 'sportsperson', 'sportsman', 'sportswoman'], 'Q2405480': ['voice actor', 'voice actress', 'voice artist'], 'Q1231865': ['pedagogue', 'educationalist'], 'Q2865819': ['opera singer'], 'Q2259451': ['stage actor', 'stage actress', 'theater actor', 'theater actress', 'theatre actor', 'theatre actress'], 'Q3282637': ['film producer', 'producer', 'movie producer'], 'Q3387717': ['theatre director', 'theater director', 'stage director'], 'Q3055126': ['entomologist'], 'Q3400985': ['academic', 'college graduates', 'university graduates'], 'Q3665646': ['basketball player', 'professional basketball player', 'basketballer'], 'Q3621491': ['archaeologist', 'archeologist'], 'Q4610556': ['model', 'fashion model', 'mannequin'], 'Q4263842': ['literary critic', 'book critic', 'literary critique'], 'Q4964182': ['philosopher'], 'Q5716684': ['dancer'], 'Q6625963': ['novelist'], 'Q10843402': ['swimmer'], 'Q10833314': ['tennis player'], 'Q10871364': ['baseball player'], 'Q10798782': ['television actor', 'actor', 'actress', 'television actress', 'TV actor', 'TV actress'], 'Q10873124': ['chess player'], 'Q10800557': ['film actor', 'film actress', 'movie actor', 'movie actress'], 'Q11338576': ['boxer', 'pugilist'], 'Q11513337': ['athletics competitor', 'track and field athlete', 'athlete (restricted sense)'], 'Q2309784': ['sport cyclist', 'racing cyclist', 'sport bicyclist', 'sport biker'], 'Q11774891': ['ice hockey player', 'hockey player'], 'Q12299841': ['cricketer', 'cricket player'], 'Q13141064': ['badminton player'], 'Q13365117': ['handball player', 'handballer'], 'Q13414980': ['Australian rules footballer', 'Australian footballer', 'Australian rules football player', 'Australian-rules football player'], 'Q14089670': ['rugby union player'], 'Q14467526': ['linguist', 'linguistic scholar'], 'Q15117302': ['volleyball player', 'volleyballer'], 'Q15627169': ['trade unionist', 'labor unionist', 'labour unionist'], 'Q15981151': ['jazz musician'], 'Q16010345': ['performer', 'performing artist', 'scenic artist'], 'Q131512': ['agriculturer', 'farmer', 'agriculturist', 'cultivator', 'grower', 'raiser'], 'Q13382576': ['rower', 'oarsman', 'oarswoman'], 'Q19204627': ['American football player', 'football player'], 'Q15296811': ['drawer', 'illustrator', 'draughtsperson', 'draughtsman', 'draftsperson', 'draftsman', 'draftswoman', 'drafter'], 'Q19595175': ['amateur wrestler', 'wrestler'], 'Q16145150': ['music pedagogue', 'music teacher'], 'Q901': ['scientist', 'natural philosopher'], 'Q33999': ['actor', 'actors', 'actresses', 'actress'], 'Q33231': ['photographer'], 'Q36834': ['composer'], 'Q16533': ['judge', 'magistrate', 'justice', 'judges', 'justices'], 'Q40348': ['lawyer', 'attorney', 'Jurisprudente'], 'Q36180': ['writer', 'author', 'writers', 'authors'], 'Q42973': ['architect'], 'Q43845': ['businessperson', 'businessman', 'dealer', 'business person', 'business woman', 'businesswoman', 'business man'], 'Q39631': ['physician', 'physicians'], 'Q28389': ['screenwriter', 'script writer', 'writer', 'screen writer', 'scriptwriter', 'scenarist', 'film writer', 'tv writer'], 'Q42603': ['priest', 'priestess', 'reverend'], 'Q49757': ['poet', 'bard', 'poetess'], 'Q37226': ['teacher', 'professor', 'educator', 'schoolmaster', 'schoolmistress', 'school teacher'], 'Q47064': ['military personnel']}\n"
     ]
    }
   ],
   "source": [
    "occupations_labels = {k: [v] for k, v in occupations_label.items()}\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT ?o ?altLabel \n",
    "WHERE {\n",
    "  VALUES ?o { %s }\n",
    "  ?o skos:altLabel ?altLabel . FILTER (lang(?altLabel) = \"en\")\n",
    "}\"\"\" % ' '.join('wd:' + o for o in occupations) \n",
    "\n",
    "for result in evalSparql(query):\n",
    "    occupations_labels[result['o']['value'].replace('http://www.wikidata.org/entity/', '')].append(result['altLabel']['value'])\n",
    "\n",
    "print(occupations_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia articles\n",
    "\n",
    "Here we load the training and the testing sets. To save memory space we use a generator that will read the file each time we iterate over the training or the testing examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "def loadJson(filename):\n",
    "    with gzip.open(filename, 'rt') as fp:\n",
    "        for line in fp:\n",
    "            yield json.loads(line)\n",
    "\n",
    "class MakeIter(object):\n",
    "    def __init__(self, generator_func, **kwargs):\n",
    "        self.generator_func = generator_func\n",
    "        self.kwargs = kwargs\n",
    "    def __iter__(self):\n",
    "        return self.generator_func(**self.kwargs)\n",
    "\n",
    "training_set = MakeIter(loadJson, filename='wiki-train.json.gz')\n",
    "testing_set = MakeIter(loadJson, filename='wiki-test.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract occupations from summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Dictionnary extraction\n",
    "\n",
    "> Using ```occupations_labels``` dictionnary, identify all occupations for each articles. Complete the function below to evaluate the accuracy of such approach. It will serve as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4842586814146957"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_occ = dict()\n",
    "for key, occs in occupations_labels.items():\n",
    "    for occ in occs:\n",
    "        label_to_occ[occ.lower()] = key\n",
    "\n",
    "def predict_dictionnary(example, occupations_labels):\n",
    "    occs = []\n",
    "    summary = example['summary'].lower()\n",
    "    labels = label_to_occ.keys()\n",
    "    for label in labels:\n",
    "        if label in summary:\n",
    "            occs.append(label_to_occ[label])\n",
    "    return occs\n",
    "    \n",
    "def evaluate_dictionnary(training_set, occupations_labels):\n",
    "    nexample = 0\n",
    "    accuracy = 0.\n",
    "    prediction = None\n",
    "    for example in training_set:\n",
    "        prediction = predict_dictionnary(example, occupations_labels)\n",
    "        p = frozenset(prediction)\n",
    "        g = frozenset(example['occupations'])\n",
    "        accuracy += 1.*len(p & g) / len(p | g)\n",
    "        nexample += 1\n",
    "    return accuracy / nexample\n",
    "\n",
    "evaluate_dictionnary(training_set, occupations_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Simple neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the articles \"summary\" and we take the average of the word vectors.\n",
    "This is done with spacy loaded with the fast text vectors.\n",
    "To do the installation/loading [takes 8-10 minutes, dl 1.2Go]\n",
    "```\n",
    "pip3 install spacy\n",
    "wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/cc.en.300.vec.gz\n",
    "python3 -m spacy init-model en /tmp/en_vectors_wiki_lg --vectors-loc cc.en.300.vec.gz\n",
    "rm cc.en.300.vec.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nlp = spacy.load('/tmp/en_vectors_wiki_lg')\n",
    "\n",
    "def vectorize(dataset, nlp):\n",
    "    result = {}\n",
    "    for example in dataset:\n",
    "        doc = nlp(example['summary'], disable=['parser', 'tagger'])\n",
    "        result[example['title']] = {}\n",
    "        result[example['title']]['vector'] = doc.vector\n",
    "        result[example['title']]['summary'] = example['summary']\n",
    "        if 'occupations' in example:\n",
    "            result[example['title']]['occupations'] = example['occupations']\n",
    "    return result\n",
    "    \n",
    "vectorized_training = vectorize(training_set, nlp)\n",
    "vectorized_testing = vectorize(testing_set, nlp)\n",
    "nlp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427798"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.45162819e-02 -2.45802402e-02 -4.59302496e-03 -4.09372151e-02\n",
      " -4.47662771e-02 -4.18604538e-03 -3.15232435e-03 -1.44802360e-02\n",
      " -1.68499984e-02 -3.69651243e-03 -1.16255814e-02  1.43651171e-02\n",
      "  2.02674349e-03 -5.88953542e-03 -2.17011590e-02  1.02302311e-02\n",
      " -2.49313917e-02 -5.65232616e-03 -2.25581434e-02  8.29069968e-03\n",
      " -1.44069805e-03  2.25197673e-02 -6.81395701e-04 -1.37232570e-02\n",
      " -1.26674427e-02 -3.35569866e-02  1.10627888e-02 -2.37208814e-03\n",
      " -2.30000000e-02  7.58616179e-02 -5.03487710e-04 -2.51116175e-02\n",
      "  9.26511642e-03 -2.52558179e-02 -1.51058156e-02 -9.51627828e-03\n",
      "  1.17523270e-02  1.22441910e-03  1.08139520e-03  3.39302444e-03\n",
      "  2.20116391e-03  1.46860480e-02 -1.43686021e-02  5.76395402e-03\n",
      "  1.74162779e-02 -4.76220921e-02 -1.72569733e-02 -1.49988411e-02\n",
      " -1.77732538e-02  1.58907007e-02 -7.23255938e-03  2.43825577e-02\n",
      " -2.73104683e-02 -3.67430188e-02 -1.48802334e-02 -1.34825567e-02\n",
      " -3.14348824e-02  1.95930228e-02 -6.68605033e-04 -9.24302172e-03\n",
      "  1.56976283e-04 -1.65674444e-02 -1.30372085e-02  6.16298130e-05\n",
      " -3.63139645e-03  2.74534873e-03 -1.62697677e-02 -4.70697694e-03\n",
      "  5.48139494e-03  4.39302297e-03  4.65523303e-02  2.29872130e-02\n",
      "  2.72058025e-02 -5.52790612e-03  2.19720937e-02 -4.41581383e-02\n",
      "  1.33255811e-03  1.20244222e-02  3.49267460e-02  3.76593024e-02\n",
      "  8.65232572e-03 -6.52325572e-03 -1.90407019e-02  1.03569757e-02\n",
      "  1.09301973e-03 -6.28488278e-03  3.98965068e-02 -3.81744131e-02\n",
      " -1.35965087e-02  1.74023230e-02 -1.48686031e-02  5.78604685e-03\n",
      " -8.59186146e-03  4.74418374e-03  1.54720917e-02 -6.42325589e-03\n",
      " -1.58430226e-02 -2.98779178e-02 -1.54255824e-02  3.28209326e-02\n",
      "  2.43825577e-02  1.32907031e-03  1.80883706e-02 -2.72825565e-02\n",
      "  9.28488653e-03 -7.39418622e-03 -7.98023026e-03  1.84244160e-02\n",
      " -9.45350039e-04 -1.16825579e-02  1.15813862e-03 -2.10464321e-04\n",
      " -3.00813979e-03  4.75407019e-02 -8.32790602e-03  4.11511678e-03\n",
      " -1.25604663e-02  8.92209262e-03  7.64534995e-03 -2.65965052e-02\n",
      "  6.58837147e-03 -1.12011610e-02 -9.68022924e-03  1.60023291e-02\n",
      "  1.61629519e-04  3.20906974e-02 -1.59848798e-02  1.14162825e-02\n",
      " -2.40430199e-02  5.39906919e-02 -4.80814092e-03  3.02209193e-03\n",
      "  5.89418598e-03 -3.94418649e-03 -2.68058274e-02 -8.98256153e-03\n",
      " -2.94616278e-02  3.90697829e-03  4.68255766e-03  3.96162830e-03\n",
      " -2.68069748e-02 -2.68395394e-02 -9.76740339e-05  5.67557989e-03\n",
      "  4.43197712e-02 -1.38953477e-02 -3.69888335e-01  1.04639539e-02\n",
      "  1.55372089e-02 -1.35093015e-02 -8.09988379e-02  2.67802346e-02\n",
      "  2.21941881e-02 -7.86627829e-03 -1.00313956e-02  1.52511625e-02\n",
      "  1.45744160e-01  4.61395411e-03  7.26162829e-03  3.14453505e-02\n",
      " -7.95465056e-03 -1.25395320e-02  6.95348764e-03 -2.48023286e-03\n",
      "  6.17325725e-03  1.26546472e-02  1.03558144e-02 -1.21616265e-02\n",
      " -1.27907039e-03 -1.99348871e-02 -9.01860371e-03  4.25581448e-03\n",
      "  7.45790750e-02  1.02186035e-02 -9.93953645e-03  1.72848776e-02\n",
      " -1.03779081e-02  1.46616297e-02 -3.75465187e-03 -2.26953458e-02\n",
      "  5.36046689e-04  6.64511696e-02 -2.53790785e-02  5.80627881e-02\n",
      " -1.42732579e-02  9.22453254e-02 -1.12825576e-02 -2.51837187e-02\n",
      "  3.90697736e-03  5.96395321e-03 -3.02476659e-02  2.63883732e-02\n",
      " -1.69488378e-02  7.39418576e-03  1.60662793e-02 -1.68313961e-02\n",
      " -8.25814065e-03 -1.36965141e-02  7.30697624e-03  1.63453538e-02\n",
      " -4.15407047e-02  1.05633713e-01  1.53325591e-02  6.63023209e-03\n",
      "  3.93279046e-02 -1.27697680e-02 -5.95697621e-03 -8.67441762e-03\n",
      "  1.58593040e-02  9.42093134e-03 -4.15697647e-03  1.34639572e-02\n",
      " -4.10383604e-02 -2.82325619e-03 -2.43790708e-02 -4.02325485e-03\n",
      "  1.65058132e-02  4.21395432e-03  1.25813941e-02  1.64744183e-02\n",
      " -2.81162816e-03  1.34813897e-02 -8.19302350e-03 -7.04767322e-03\n",
      "  1.67139638e-02  1.43581396e-02  1.20023256e-02  4.96162800e-03\n",
      "  1.76325571e-02 -7.07674446e-03 -4.24197726e-02 -2.34697610e-02\n",
      " -1.86058115e-02 -2.32790736e-03  2.98906974e-02  1.53604464e-03\n",
      "  1.95941851e-02 -2.67104693e-02 -1.12453466e-02 -2.54534930e-03\n",
      " -4.29302268e-03  3.56558077e-02 -4.36046888e-04 -8.16406980e-02\n",
      "  5.04779041e-01 -2.18813960e-02  1.15883695e-02  2.14848872e-02\n",
      "  7.80581404e-03  1.55116236e-02 -1.11523261e-02  4.61628864e-04\n",
      "  1.72918607e-02  1.43034859e-02  2.05546506e-02 -8.23488459e-03\n",
      " -3.16290706e-02 -4.83953534e-03 -1.82697661e-02  2.02907110e-03\n",
      " -3.51163093e-04  1.10220918e-02 -8.54755938e-02 -2.68255756e-03\n",
      "  1.83174424e-02  1.91116314e-02 -4.73488262e-03 -8.08255840e-03\n",
      "  1.37906978e-02 -7.76046468e-03 -2.82767452e-02 -2.99069774e-03\n",
      "  1.06569799e-02 -5.99999772e-03  1.11883730e-02  4.28720983e-03\n",
      " -3.12255807e-02 -8.07186142e-02  8.59302282e-03 -8.11744668e-03\n",
      " -5.36279054e-03  1.87046509e-02 -1.10972092e-01 -3.07988375e-02\n",
      "  9.47441999e-03 -1.03662787e-02  1.16337193e-02  3.22093032e-02\n",
      " -2.69790720e-02  2.25430205e-02 -1.49802361e-02 -1.05290683e-02\n",
      " -4.36534919e-02  6.34883530e-04 -2.83197612e-02 -1.37674408e-02\n",
      " -1.50220934e-02  1.30851150e-01 -1.22430259e-02  2.38767453e-02]\n"
     ]
    }
   ],
   "source": [
    "v = vectorized_training['George_Washington']['vector']\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the vectorized_training into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDict(d, percent):\n",
    "    split_idx = int(len(d) * percent)\n",
    "    d1 = dict(list(d.items())[: split_idx])\n",
    "    d2 = dict(list(d.items())[split_idx:])                \n",
    "\n",
    "    return d1, d2\n",
    "\n",
    "vectorized_training_test, vectorized_training_train = splitDict(vectorized_training, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "342239"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_training_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We encode the data\n",
    "import numpy as np\n",
    "\n",
    "def encode_data(vectorized_data):\n",
    "    X = np.array([vectorized_data[article]['vector'] for article in vectorized_data])\n",
    "    y = np.array([[(1 if occupation in vectorized_data[article]['occupations'] else 0)\n",
    "                        for occupation in occupations ] for article in vectorized_data])\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = encode_data(vectorized_training_train)\n",
    "X_test, y_test = encode_data(vectorized_training_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342239, 300)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342239, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using keras, define a sequential neural network with two layers. Use categorical_crossentropy as a loss function and softmax as the activation function of the output layer\n",
    "\n",
    "You can look into the documentation here: https://keras.io/getting-started/sequential-model-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=300))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "\n",
    "optimizer = Adam()\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 308015 samples, validate on 34224 samples\n",
      "Epoch 1/50\n",
      "308015/308015 [==============================] - 3s 10us/step - loss: 3.5608 - acc: 0.4881 - val_loss: 2.0247 - val_acc: 0.6362\n",
      "Epoch 2/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 2.3868 - acc: 0.6581 - val_loss: 1.7224 - val_acc: 0.6907\n",
      "Epoch 3/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 2.1847 - acc: 0.6880 - val_loss: 1.6056 - val_acc: 0.7125\n",
      "Epoch 4/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 2.0853 - acc: 0.7033 - val_loss: 1.5498 - val_acc: 0.7234\n",
      "Epoch 5/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 2.0255 - acc: 0.7115 - val_loss: 1.5037 - val_acc: 0.7321\n",
      "Epoch 6/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.9833 - acc: 0.7176 - val_loss: 1.4793 - val_acc: 0.7359\n",
      "Epoch 7/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.9517 - acc: 0.7213 - val_loss: 1.4538 - val_acc: 0.7409\n",
      "Epoch 8/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.9250 - acc: 0.7251 - val_loss: 1.4335 - val_acc: 0.7466\n",
      "Epoch 9/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.9036 - acc: 0.7273 - val_loss: 1.4188 - val_acc: 0.7452\n",
      "Epoch 10/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8846 - acc: 0.7293 - val_loss: 1.4096 - val_acc: 0.7491\n",
      "Epoch 11/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8676 - acc: 0.7324 - val_loss: 1.3920 - val_acc: 0.7537\n",
      "Epoch 12/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8522 - acc: 0.7333 - val_loss: 1.3919 - val_acc: 0.7550\n",
      "Epoch 13/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8387 - acc: 0.7349 - val_loss: 1.3776 - val_acc: 0.7560\n",
      "Epoch 14/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8256 - acc: 0.7372 - val_loss: 1.3729 - val_acc: 0.7587\n",
      "Epoch 15/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8138 - acc: 0.7383 - val_loss: 1.3733 - val_acc: 0.7553\n",
      "Epoch 16/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.8025 - acc: 0.7391 - val_loss: 1.3637 - val_acc: 0.7564\n",
      "Epoch 17/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 1.7916 - acc: 0.7408 - val_loss: 1.3510 - val_acc: 0.7603\n",
      "Epoch 18/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 1.7818 - acc: 0.7417 - val_loss: 1.3484 - val_acc: 0.7622\n",
      "Epoch 19/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7721 - acc: 0.7434 - val_loss: 1.3472 - val_acc: 0.7599\n",
      "Epoch 20/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 1.7634 - acc: 0.7436 - val_loss: 1.3448 - val_acc: 0.7615\n",
      "Epoch 21/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7554 - acc: 0.7450 - val_loss: 1.3347 - val_acc: 0.7649\n",
      "Epoch 22/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7462 - acc: 0.7463 - val_loss: 1.3367 - val_acc: 0.7639\n",
      "Epoch 23/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7388 - acc: 0.7467 - val_loss: 1.3338 - val_acc: 0.7641\n",
      "Epoch 24/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7308 - acc: 0.7481 - val_loss: 1.3379 - val_acc: 0.7646\n",
      "Epoch 25/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7240 - acc: 0.7486 - val_loss: 1.3341 - val_acc: 0.7644\n",
      "Epoch 26/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7173 - acc: 0.7494 - val_loss: 1.3279 - val_acc: 0.7634\n",
      "Epoch 27/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7097 - acc: 0.7500 - val_loss: 1.3268 - val_acc: 0.7610\n",
      "Epoch 28/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.7030 - acc: 0.7510 - val_loss: 1.3203 - val_acc: 0.7664\n",
      "Epoch 29/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6968 - acc: 0.7518 - val_loss: 1.3266 - val_acc: 0.7640\n",
      "Epoch 30/50\n",
      "308015/308015 [==============================] - 1s 5us/step - loss: 1.6907 - acc: 0.7525 - val_loss: 1.3202 - val_acc: 0.7654\n",
      "Epoch 31/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6837 - acc: 0.7533 - val_loss: 1.3224 - val_acc: 0.7615\n",
      "Epoch 32/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6777 - acc: 0.7537 - val_loss: 1.3208 - val_acc: 0.7650\n",
      "Epoch 33/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6719 - acc: 0.7541 - val_loss: 1.3257 - val_acc: 0.7629\n",
      "Epoch 34/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6654 - acc: 0.7554 - val_loss: 1.3239 - val_acc: 0.7624\n",
      "Epoch 35/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6594 - acc: 0.7557 - val_loss: 1.3201 - val_acc: 0.7618\n",
      "Epoch 36/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6540 - acc: 0.7566 - val_loss: 1.3231 - val_acc: 0.7631\n",
      "Epoch 37/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6487 - acc: 0.7568 - val_loss: 1.3197 - val_acc: 0.7663\n",
      "Epoch 38/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6427 - acc: 0.7581 - val_loss: 1.3195 - val_acc: 0.7684\n",
      "Epoch 39/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6375 - acc: 0.7579 - val_loss: 1.3150 - val_acc: 0.7657\n",
      "Epoch 40/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6323 - acc: 0.7581 - val_loss: 1.3244 - val_acc: 0.7619\n",
      "Epoch 41/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6270 - acc: 0.7595 - val_loss: 1.3234 - val_acc: 0.7617\n",
      "Epoch 42/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6214 - acc: 0.7601 - val_loss: 1.3270 - val_acc: 0.7601\n",
      "Epoch 43/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6162 - acc: 0.7604 - val_loss: 1.3214 - val_acc: 0.7638\n",
      "Epoch 44/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6115 - acc: 0.7606 - val_loss: 1.3241 - val_acc: 0.7622\n",
      "Epoch 45/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6061 - acc: 0.7616 - val_loss: 1.3275 - val_acc: 0.7599\n",
      "Epoch 46/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.6012 - acc: 0.7622 - val_loss: 1.3222 - val_acc: 0.7651\n",
      "Epoch 47/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.5964 - acc: 0.7628 - val_loss: 1.3247 - val_acc: 0.7631\n",
      "Epoch 48/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.5910 - acc: 0.7632 - val_loss: 1.3250 - val_acc: 0.7668\n",
      "Epoch 49/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.5868 - acc: 0.7636 - val_loss: 1.3247 - val_acc: 0.7598\n",
      "Epoch 50/50\n",
      "308015/308015 [==============================] - 1s 4us/step - loss: 1.5814 - acc: 0.7645 - val_loss: 1.3295 - val_acc: 0.7646\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=1024, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Complete the function predict: output the list of occupations where the corresponding neuron on the output layer of our model has a value > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q177220', 'Q639669', 'Q33999'}\n"
     ]
    }
   ],
   "source": [
    "def predict_nn(model, article_name, vectorized_dataset):\n",
    "    input_vector = vectorized_dataset[article_name]['vector'].reshape((1, 300))\n",
    "    scores = model.predict(input_vector).reshape(100)\n",
    "    predictions = np.where(scores > 0.1)[0]\n",
    "#     print(scores[predictions])\n",
    "    return set(np.array(occupations)[predictions])\n",
    "\n",
    "print(predict_nn(model, 'Elvis_Presley', vectorized_training))\n",
    "# should be {'Q177220'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nn(vectorized_training, model):\n",
    "    nexample = 0\n",
    "    accuracy = 0.\n",
    "    prediction = None\n",
    "    for article_name in vectorized_training:\n",
    "        prediction = predict_nn(model, article_name, vectorized_training)\n",
    "        p = frozenset(prediction)\n",
    "        g = frozenset(vectorized_training[article_name]['occupations'])\n",
    "        accuracy += 1.*len(p & g) / len(p | g)\n",
    "        nexample += 1\n",
    "    return accuracy / nexample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7048576643356244\n",
      "0.6662116943899452\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_nn(vectorized_training_train, model))\n",
    "print(evaluate_nn(vectorized_training_test, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Your approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, GRU, Dropout, Conv1D, MaxPooling1D, MaxPooling1D, Bidirectional, BatchNormalization, LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dataset into summaries, titles and occupations\n",
    "def parse(dataset):\n",
    "    titles = []\n",
    "    summaries = []\n",
    "    occs = []\n",
    "    for example in dataset:\n",
    "        titles.append(example['title'])\n",
    "        summaries.append(example['summary'])        \n",
    "        if 'occupations' in example:\n",
    "            occs.append(example['occupations'])\n",
    "        else:\n",
    "            occs.append([])\n",
    "    return titles, summaries, occs\n",
    "    \n",
    "titles_train, summaries_train, occs_train = parse(training_set)\n",
    "\n",
    "s = int(len(titles_train) * 0.8)\n",
    "titles_train_train, summaries_train_train, occs_train_train = titles_train[:s], summaries_train[:s], occs_train[:s]\n",
    "titles_train_test, summaries_train_test, occs_train_test = titles_train[s:], summaries_train[s:], occs_train[s:]\n",
    "\n",
    "titles_test, summaries_test, occs_test = parse(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 370295 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(titles_train_train)\n",
    "maxlen = 300\n",
    "training_samples = int(n_samples * 0.9)\n",
    "validation_samples = n_samples - training_samples\n",
    "max_words = 400000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(summaries_train_train)\n",
    "\n",
    "# convert text to sequences\n",
    "sequences =  tokenizer.texts_to_sequences(summaries_train_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(summaries_train_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found', len(word_index), 'unique tokens.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_occs_to_labels(occupations, occs_train):\n",
    "    labels = []\n",
    "    for i in range(len(occs_train)):\n",
    "        label = []\n",
    "        for occ in occupations:\n",
    "            if occ in occs_train[i]:\n",
    "                label.append(1)\n",
    "            else:\n",
    "                label.append(0)\n",
    "        labels.append(label)\n",
    "    return np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (342333, 300)\n",
      "Shape of label tensor: (342333, 100)\n"
     ]
    }
   ],
   "source": [
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "data_test = pad_sequences(sequences_test, maxlen=maxlen)\n",
    "labels = convert_occs_to_labels(occupations, occs_train_train)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# shuffle the data\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# split into training and testing set\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = 'glove.6B'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found', len(embeddings_index), 'word vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build embedding matrix to load into embedding layer\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 300, 300)          120000000 \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 300, 128)          115328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 300, 128)          512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 100, 256)          98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 100, 256)          1024      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 33, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 33, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 33, 512)           393728    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 33, 512)           2048      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 11, 512)           0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 11, 512)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 11, 128)           221568    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 11, 128)           74112     \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 11, 128)           74112     \n",
      "_________________________________________________________________\n",
      "bidirectional_8 (Bidirection (None, 128)               74112     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 100)               25700     \n",
      "=================================================================\n",
      "Total params: 121,612,772\n",
      "Trainable params: 121,607,908\n",
      "Non-trainable params: 4,864\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "\n",
    "model.add(Conv1D(64,kernel_size=3,padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv1D(128,kernel_size=3,padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv1D(256,kernel_size=3,padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "model.add(Bidirectional(GRU(64, return_sequences=True, recurrent_dropout = 0.1, dropout = 0.1)))\n",
    "\n",
    "model.add(Bidirectional(GRU(64, return_sequences=True, recurrent_dropout = 0.1, dropout = 0.1)))\n",
    "\n",
    "model.add(Bidirectional(GRU(64, return_sequences=True, recurrent_dropout = 0.1, dropout = 0.1)))\n",
    "\n",
    "model.add(Bidirectional(GRU(64, recurrent_dropout = 0.1, dropout = 0.1)))\n",
    "\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "# model.add(Dense(100, activation='softmax'))\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Glove embedding in the model\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False # we will not update this layer during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as tfb\n",
    "from keras import backend as K\n",
    "\n",
    "POS_WEIGHT = 10  # multiplier for positive targets\n",
    "\n",
    "def weighted_binary_crossentropy(target, output):\n",
    "    # transform back to logits\n",
    "    _epsilon = tfb._to_tensor(tfb.epsilon(), output.dtype.base_dtype)\n",
    "    output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "    output = tf.log(output / (1 - output))\n",
    "    # compute weighted loss\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(targets=target,\n",
    "                                                    logits=output,\n",
    "                                                    pos_weight=POS_WEIGHT)\n",
    "    return tf.reduce_mean(loss, axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 308099 samples, validate on 34234 samples\n",
      "Epoch 1/50\n",
      "308099/308099 [==============================] - 137s 446us/step - loss: 0.3043 - acc: 0.4631 - val_loss: 0.1682 - val_acc: 0.5308\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16816, saving model to best_model.h5\n",
      "Epoch 2/50\n",
      "308099/308099 [==============================] - 127s 414us/step - loss: 0.1223 - acc: 0.6662 - val_loss: 0.1303 - val_acc: 0.6647\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.16816 to 0.13028, saving model to best_model.h5\n",
      "Epoch 3/50\n",
      "308099/308099 [==============================] - 128s 414us/step - loss: 0.1044 - acc: 0.7172 - val_loss: 0.1119 - val_acc: 0.6801\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13028 to 0.11191, saving model to best_model.h5\n",
      "Epoch 4/50\n",
      "308099/308099 [==============================] - 128s 415us/step - loss: 0.0971 - acc: 0.7319 - val_loss: 0.0985 - val_acc: 0.7289\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.11191 to 0.09853, saving model to best_model.h5\n",
      "Epoch 5/50\n",
      "308099/308099 [==============================] - 128s 414us/step - loss: 0.0930 - acc: 0.7408 - val_loss: 0.0949 - val_acc: 0.7437\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09853 to 0.09489, saving model to best_model.h5\n",
      "Epoch 6/50\n",
      "308099/308099 [==============================] - 129s 417us/step - loss: 0.0902 - acc: 0.7461 - val_loss: 0.0916 - val_acc: 0.7479\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.09489 to 0.09157, saving model to best_model.h5\n",
      "Epoch 7/50\n",
      "308099/308099 [==============================] - 129s 417us/step - loss: 0.0880 - acc: 0.7498 - val_loss: 0.0925 - val_acc: 0.7315\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.09157\n",
      "Epoch 8/50\n",
      "308099/308099 [==============================] - 128s 415us/step - loss: 0.0862 - acc: 0.7526 - val_loss: 0.0868 - val_acc: 0.7609\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.09157 to 0.08683, saving model to best_model.h5\n",
      "Epoch 9/50\n",
      "308099/308099 [==============================] - 128s 414us/step - loss: 0.0845 - acc: 0.7546 - val_loss: 0.0886 - val_acc: 0.7438\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.08683\n",
      "Epoch 10/50\n",
      "308099/308099 [==============================] - 128s 415us/step - loss: 0.0834 - acc: 0.7561 - val_loss: 0.0890 - val_acc: 0.7538\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.08683\n",
      "Epoch 11/50\n",
      "308099/308099 [==============================] - 128s 416us/step - loss: 0.0823 - acc: 0.7587 - val_loss: 0.0892 - val_acc: 0.7525\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.08683\n",
      "Epoch 12/50\n",
      "308099/308099 [==============================] - 128s 415us/step - loss: 0.0812 - acc: 0.7593 - val_loss: 0.0864 - val_acc: 0.7599\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.08683 to 0.08644, saving model to best_model.h5\n",
      "Epoch 13/50\n",
      "308099/308099 [==============================] - 128s 416us/step - loss: 0.0802 - acc: 0.7603 - val_loss: 0.0839 - val_acc: 0.7635\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.08644 to 0.08388, saving model to best_model.h5\n",
      "Epoch 14/50\n",
      "308099/308099 [==============================] - 127s 414us/step - loss: 0.0796 - acc: 0.7596 - val_loss: 0.0865 - val_acc: 0.7567\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.08388\n",
      "Epoch 15/50\n",
      "308099/308099 [==============================] - 128s 415us/step - loss: 0.0786 - acc: 0.7611 - val_loss: 0.0866 - val_acc: 0.7589\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.08388\n",
      "Epoch 16/50\n",
      "308099/308099 [==============================] - 127s 414us/step - loss: 0.0779 - acc: 0.7621 - val_loss: 0.0942 - val_acc: 0.7460\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.08388\n",
      "Epoch 17/50\n",
      "308099/308099 [==============================] - 128s 415us/step - loss: 0.0773 - acc: 0.7627 - val_loss: 0.3040 - val_acc: 0.2951\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.08388\n",
      "Epoch 18/50\n",
      "308099/308099 [==============================] - 128s 415us/step - loss: 0.0768 - acc: 0.7633 - val_loss: 0.2479 - val_acc: 0.3346\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.08388\n",
      "Epoch 19/50\n",
      "308099/308099 [==============================] - 128s 415us/step - loss: 0.0736 - acc: 0.7674 - val_loss: 0.0801 - val_acc: 0.7632\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.08388 to 0.08008, saving model to best_model.h5\n",
      "Epoch 20/50\n",
      "308099/308099 [==============================] - 128s 414us/step - loss: 0.0723 - acc: 0.7687 - val_loss: 0.0799 - val_acc: 0.7601\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.08008 to 0.07992, saving model to best_model.h5\n",
      "Epoch 21/50\n",
      "308099/308099 [==============================] - 128s 414us/step - loss: 0.0717 - acc: 0.7691 - val_loss: 0.0800 - val_acc: 0.7611\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.07992\n",
      "Epoch 22/50\n",
      "308099/308099 [==============================] - 128s 416us/step - loss: 0.0713 - acc: 0.7695 - val_loss: 0.0799 - val_acc: 0.7612\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.07992 to 0.07989, saving model to best_model.h5\n",
      "Epoch 23/50\n",
      "308099/308099 [==============================] - 128s 415us/step - loss: 0.0710 - acc: 0.7702 - val_loss: 0.0801 - val_acc: 0.7615\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.07989\n",
      "Epoch 24/50\n",
      "308099/308099 [==============================] - 128s 415us/step - loss: 0.0706 - acc: 0.7698 - val_loss: 0.0803 - val_acc: 0.7622\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.07989\n",
      "Epoch 25/50\n",
      "308099/308099 [==============================] - 128s 414us/step - loss: 0.0703 - acc: 0.7697 - val_loss: 0.0803 - val_acc: 0.7646\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.07989\n",
      "Epoch 26/50\n",
      "308099/308099 [==============================] - 128s 414us/step - loss: 0.0699 - acc: 0.7723 - val_loss: 0.0803 - val_acc: 0.7632\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.07989\n",
      "Epoch 27/50\n",
      "308099/308099 [==============================] - 128s 414us/step - loss: 0.0699 - acc: 0.7709 - val_loss: 0.0802 - val_acc: 0.7626\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.07989\n",
      "Epoch 28/50\n",
      "308099/308099 [==============================] - 131s 425us/step - loss: 0.0699 - acc: 0.7710 - val_loss: 0.0802 - val_acc: 0.7625\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.07989\n",
      "Epoch 29/50\n",
      "308099/308099 [==============================] - 128s 415us/step - loss: 0.0698 - acc: 0.7708 - val_loss: 0.0802 - val_acc: 0.7622\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.07989\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=7,\n",
    "                              verbose=0, mode='auto'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', \n",
    "                      factor=0.1, \n",
    "                      patience=5, \n",
    "                      verbose=1, \n",
    "                      mode='auto', \n",
    "                      min_delta=0.0001, \n",
    "                      cooldown=0, \n",
    "                      min_lr=0),\n",
    "    ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True, verbose=1, mode='min')\n",
    "]\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "#     loss='categorical_crossentropy',\n",
    "#     loss='binary_crossentropy',\n",
    "    loss=weighted_binary_crossentropy,\n",
    "    metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    "    batch_size=1000,\n",
    "    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"best_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q177220', 'Q10800557', 'Q28389', 'Q486748', 'Q855091']\n"
     ]
    }
   ],
   "source": [
    "title = 'Elvis_Presley'\n",
    "idx = titles_train_train.index(title)\n",
    "input_vector = data[idx].reshape(1, maxlen)\n",
    "print(occs_train_train[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elvis Aaron Presley was an American singer and actor. Regarded as one of the most significant cultural icons of the 20th century, he is often referred to as the \"King of Rock and Roll\" or simply the \"King\".\n",
      "[0.63429046 0.60777783 0.95774215 0.5449958 ]\n",
      "[0.63429046 0.60777783 0.95774215]\n",
      "[0.95774215]\n",
      "[0.95774215]\n",
      "[0.95774215]\n",
      "[0.95774215]\n",
      "({'Q82955', 'Q40348', 'Q193391', 'Q16533'}, {'Q82955', 'Q40348', 'Q193391'}, {'Q193391'}, {'Q193391'}, {'Q193391'}, {'Q193391'})\n"
     ]
    }
   ],
   "source": [
    "def predict_nn_2(model, input_vector, print_score = False):\n",
    "    \n",
    "    scores = model.predict(input_vector).reshape(100)\n",
    "    predictions1 = np.where(scores > 0.5)[0]\n",
    "    predictions2 = np.where(scores > 0.6)[0]\n",
    "    predictions3 = np.where(scores > 0.7)[0]\n",
    "    predictions4 = np.where(scores > 0.8)[0]\n",
    "    predictions5 = np.where(scores > 0.9)[0]\n",
    "    predictions6 = np.where(scores > 0.95)[0]\n",
    "    if print_score:\n",
    "        print(scores[predictions1])\n",
    "        print(scores[predictions2])\n",
    "        print(scores[predictions3])\n",
    "        print(scores[predictions4])\n",
    "        print(scores[predictions5])\n",
    "        print(scores[predictions6])\n",
    "    res1 = set(np.array(occupations)[predictions1])\n",
    "    res2 = set(np.array(occupations)[predictions2])\n",
    "    res3 = set(np.array(occupations)[predictions3])\n",
    "    res4 = set(np.array(occupations)[predictions4])\n",
    "    res5 = set(np.array(occupations)[predictions5])\n",
    "    res6 = set(np.array(occupations)[predictions6])\n",
    "    return res1, res2, res3, res4, res5, res6\n",
    "\n",
    "title = 'Elvis_Presley'\n",
    "idx = titles_train.index(title)\n",
    "print(summaries_train_train[idx])\n",
    "\n",
    "print(predict_nn_2(model, input_vector, print_score=True))\n",
    "# should be {'Q177220'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85584"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "0.0  :  0.0\n",
      "=========================\n",
      "0.0117  :  0.0075\n",
      "0.0117  :  0.008\n",
      "0.0117  :  0.0083\n",
      "0.0117  :  0.0087\n",
      "0.0117  :  0.0087\n",
      "0.0117  :  0.009\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-f59e8a2b2a1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# print(evaluate_nn_2(titles_train, summaries_train, occs_train, model))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_nn_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles_train_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moccs_train_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-f59e8a2b2a1f>\u001b[0m in \u001b[0;36mevaluate_nn_2\u001b[0;34m(titles, input_vectors, occs, model)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0minput_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_vectors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mprediction1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_nn_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-efcf6bd6c484>\u001b[0m in \u001b[0;36mpredict_nn_2\u001b[0;34m(model, input_vector, print_score)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_nn_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpredictions1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpredictions2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate_nn_2(titles, input_vectors, occs, model):\n",
    "    nexample = len(titles)\n",
    "    accuracy1 = 0.\n",
    "    accuracy2 = 0.\n",
    "    accuracy3 = 0.\n",
    "    accuracy4 = 0.\n",
    "    accuracy5 = 0.\n",
    "    accuracy6 = 0.\n",
    "    prediction = None\n",
    "    for i in range(len(titles)):        \n",
    "        input_vector = input_vectors[i].reshape(1, -1)\n",
    "        prediction1, prediction2, prediction3, prediction4, prediction5, prediction6 = predict_nn_2(model, input_vector)\n",
    "        p1 = frozenset(prediction1)\n",
    "        p2 = frozenset(prediction2)\n",
    "        p3 = frozenset(prediction3)\n",
    "        p4 = frozenset(prediction4)\n",
    "        p5 = frozenset(prediction5)\n",
    "        p6 = frozenset(prediction6)\n",
    "        g = frozenset(occs[i])\n",
    "        accuracy1 += 1. / nexample * len(p1 & g) / len(p1 | g)\n",
    "        accuracy2 += 1. / nexample * len(p2 & g) / len(p2 | g)\n",
    "        accuracy3 += 1. / nexample * len(p3 & g) / len(p3 | g)\n",
    "        accuracy4 += 1. / nexample * len(p4 & g) / len(p4 | g)\n",
    "        accuracy5 += 1. / nexample * len(p5 & g) / len(p5 | g)\n",
    "        accuracy6 += 1. / nexample * len(p5 & g) / len(p6 | g)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"=========================\")\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy1, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy2, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy3, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy4, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy5, 4))\n",
    "            print(round(i / nexample, 4), \" : \", round(accuracy6, 4))\n",
    "    return accuracy1, accuracy2, accuracy3, accuracy4, accuracy5, accuracy6\n",
    "\n",
    "# print(evaluate_nn_2(titles_train, summaries_train, occs_train, model))\n",
    "print(evaluate_nn_2(titles_train_test, data_test, occs_train_test, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_7986.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate_nn_2(titles_train_train, data, occs_train_train, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***IMPORTANT*** Output format of requested file 'results.json.gz': each line must be a json string representing a dictionnary:\n",
    "> ```{ 'title': THE_ARTICLE_NAME, 'prediction': [THE_LIST_OF_OCCUPATIONS]}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example if testset_solutions is a dictionnary: article_name (key) -> prediction_list (value) use this function:\n",
    "def export(testset_solutions):\n",
    "    with gzip.open('results.json.gz', 'wt') as output:\n",
    "        for article in testset_solutions:\n",
    "            output.write(json.dumps({'title':article, 'prediction':testset_solutions[article]}) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
