{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import\n",
    "## Question 0 - Get common wikidata occupations\n",
    "\n",
    "> Write a sparql query that retrieves the top 100 occupations on wikidata (wikidata property P106).\n",
    "\n",
    "You may use the interface https://query.wikidata.org/ to try different queries. Here are some example sparql queries: https://www.wikidata.org/wiki/Wikidata:SPARQL_query_service/queries/examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT ?o (COUNT(?person) AS ?count) WHERE \n",
    "{\n",
    "   ?person wdt:P106 ?o\n",
    "}\n",
    "GROUP BY ?o\n",
    "ORDER BY DESC(?count)\n",
    "LIMIT 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following assertion should pass if your answer is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "occupations = ['Q82955', 'Q937857', 'Q36180', 'Q33999', 'Q1650915', 'Q1028181', 'Q1930187', 'Q177220', 'Q1622272', 'Q49757', 'Q36834', 'Q40348', 'Q47064', 'Q639669', 'Q10800557', 'Q201788', 'Q2526255', 'Q43845', 'Q28389', 'Q42973', 'Q10871364', 'Q39631', 'Q193391', 'Q482980', 'Q483501', 'Q11513337', 'Q3665646', 'Q12299841', 'Q19204627', 'Q16533', 'Q81096', 'Q11774891', 'Q188094', 'Q1281618', 'Q333634', 'Q189290', 'Q250867', 'Q33231', 'Q2259451', 'Q42603', 'Q628099', 'Q37226', 'Q2309784', 'Q901', 'Q2066131', 'Q6625963', 'Q10798782', 'Q2374149', 'Q170790', 'Q4610556', 'Q185351', 'Q486748', 'Q3055126', 'Q753110', 'Q4964182', 'Q169470', 'Q158852', 'Q1234713', 'Q14089670', 'Q10873124', 'Q3282637', 'Q593644', 'Q947873', 'Q13414980', 'Q131524', 'Q11338576', 'Q15117302', 'Q488205', 'Q14467526', 'Q183945', 'Q10843402', 'Q13382576', 'Q13141064', 'Q214917', 'Q855091', 'Q644687', 'Q19595175', 'Q121594', 'Q2865819', 'Q16010345', 'Q1231865', 'Q2405480', 'Q350979', 'Q3400985', 'Q13365117', 'Q10833314', 'Q3621491', 'Q15981151', 'Q212980', 'Q16145150', 'Q1792450', 'Q15296811', 'Q15627169', 'Q2306091', 'Q4263842', 'Q806798', 'Q5716684', 'Q2516866', 'Q3387717', 'Q131512']\n",
    "\n",
    "def evalSparql(query):\n",
    "    return requests.post('https://query.wikidata.org/sparql', data=query, headers={\n",
    "        'content-type': 'application/sparql-query',\n",
    "        'accept': 'application/json',\n",
    "        'user-agent': 'User:Tpt'\n",
    "    }).json()['results']['bindings']\n",
    "\n",
    "myOccupations = [val['o']['value'].replace('http://www.wikidata.org/entity/', '') \n",
    "                 for val in evalSparql(query)]\n",
    "assert(frozenset(occupations) == frozenset(myOccupations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Occupations labels\n",
    "\n",
    "We load the labels of the occupations from Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q82955': 'politician', 'Q121594': 'professor', 'Q177220': 'singer', 'Q169470': 'physicist', 'Q170790': 'mathematician', 'Q81096': 'engineer', 'Q201788': 'historian', 'Q188094': 'economist', 'Q212980': 'psychologist', 'Q214917': 'playwright', 'Q131524': 'entrepreneur', 'Q183945': 'record producer', 'Q193391': 'diplomat', 'Q189290': 'military officer', 'Q185351': 'jurist', 'Q350979': 'zoologist', 'Q483501': 'artist', 'Q482980': 'author', 'Q333634': 'translator', 'Q158852': 'conductor', 'Q486748': 'pianist', 'Q488205': 'singer-songwriter', 'Q250867': 'Catholic priest', 'Q593644': 'chemist', 'Q639669': 'musician', 'Q644687': 'illustrator', 'Q628099': 'association football manager', 'Q855091': 'guitarist', 'Q937857': 'association football player', 'Q947873': 'television presenter', 'Q806798': 'banker', 'Q1028181': 'painter', 'Q753110': 'songwriter', 'Q1234713': 'theologian', 'Q1281618': 'sculptor', 'Q1622272': 'university teacher', 'Q1792450': 'art historian', 'Q1650915': 'researcher', 'Q1930187': 'journalist', 'Q2306091': 'sociologist', 'Q2374149': 'botanist', 'Q2526255': 'film director', 'Q2516866': 'publisher', 'Q2066131': 'athlete', 'Q2405480': 'voice actor', 'Q1231865': 'pedagogue', 'Q2865819': 'opera singer', 'Q2259451': 'stage actor', 'Q3282637': 'film producer', 'Q3387717': 'theatre director', 'Q3055126': 'entomologist', 'Q3400985': 'academic', 'Q3665646': 'basketball player', 'Q3621491': 'archaeologist', 'Q4610556': 'model', 'Q4263842': 'literary critic', 'Q4964182': 'philosopher', 'Q5716684': 'dancer', 'Q6625963': 'novelist', 'Q10843402': 'swimmer', 'Q10833314': 'tennis player', 'Q10871364': 'baseball player', 'Q10798782': 'television actor', 'Q10873124': 'chess player', 'Q10800557': 'film actor', 'Q11338576': 'boxer', 'Q11513337': 'athletics competitor', 'Q2309784': 'sport cyclist', 'Q11774891': 'ice hockey player', 'Q12299841': 'cricketer', 'Q13141064': 'badminton player', 'Q13365117': 'handball player', 'Q13414980': 'Australian rules footballer', 'Q14089670': 'rugby union player', 'Q14467526': 'linguist', 'Q15117302': 'volleyball player', 'Q15627169': 'trade unionist', 'Q15981151': 'jazz musician', 'Q16010345': 'performer', 'Q131512': 'agriculturer', 'Q13382576': 'rower', 'Q19204627': 'American football player', 'Q15296811': 'drawer', 'Q19595175': 'amateur wrestler', 'Q16145150': 'music pedagogue', 'Q901': 'scientist', 'Q33999': 'actor', 'Q33231': 'photographer', 'Q36834': 'composer', 'Q16533': 'judge', 'Q40348': 'lawyer', 'Q36180': 'writer', 'Q42973': 'architect', 'Q43845': 'businessperson', 'Q39631': 'physician', 'Q28389': 'screenwriter', 'Q42603': 'priest', 'Q49757': 'poet', 'Q37226': 'teacher', 'Q47064': 'military personnel'}\n"
     ]
    }
   ],
   "source": [
    "occupations_label = {}\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT ?o ?oLabel \n",
    "WHERE { \n",
    "    VALUES ?o { %s } \n",
    "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
    "}\"\"\"% ' '.join('wd:' + o for o in occupations)\n",
    "\n",
    "for result in evalSparql(query):\n",
    "    occupations_label[result['o']['value'].replace('http://www.wikidata.org/entity/', '')] = result['oLabel']['value']\n",
    "\n",
    "print(occupations_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load *all* the labels of the occupations from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q82955': ['politician', 'political leader', 'polit.', 'political figure'], 'Q121594': ['professor', 'Prof.'], 'Q177220': ['singer', 'vocalist'], 'Q169470': ['physicist'], 'Q170790': ['mathematician'], 'Q81096': ['engineer'], 'Q201788': ['historian', 'historians', 'historiographer'], 'Q188094': ['economist'], 'Q212980': ['psychologist'], 'Q214917': ['playwright', 'Playwright, dramatist', 'dramatist', 'scriptwriter', 'playwrite'], 'Q131524': ['entrepreneur'], 'Q183945': ['record producer', 'music producer'], 'Q193391': ['diplomat'], 'Q189290': ['military officer', 'army officer', 'officer'], 'Q185351': ['jurist'], 'Q350979': ['zoologist', 'zooligist'], 'Q483501': ['artist'], 'Q482980': ['author'], 'Q333634': ['translator'], 'Q158852': ['conductor', 'Conducting'], 'Q486748': ['pianist'], 'Q488205': ['singer-songwriter', 'singer/songwriter', 'singer songwriter', 'singersongwriter'], 'Q250867': ['Catholic priest', 'Roman Catholic priest', 'Catholic presbyter', 'Roman Catholic presbyter'], 'Q593644': ['chemist', 'chemists'], 'Q639669': ['musician'], 'Q644687': ['illustrator'], 'Q628099': ['association football manager', 'football manager', 'association football coach', 'football coach', 'soccer coach', 'soccer manager'], 'Q855091': ['guitarist', 'guitar player'], 'Q937857': ['association football player', 'footballer', 'football player', 'association footballer', 'soccer player'], 'Q947873': ['television presenter', 'TV presenter', 'hostess', 'TV host', 'TV personality', 'host', 'television personality', 'television host'], 'Q806798': ['banker', 'Private Banker', 'private sector banker'], 'Q1028181': ['painter'], 'Q753110': ['songwriter', 'song writer'], 'Q1234713': ['theologian', 'religious scholar'], 'Q1281618': ['sculptor'], 'Q1622272': ['university teacher', 'lecturer', 'university teachers', 'college lecturer', 'college professor'], 'Q1792450': ['art historian'], 'Q1650915': ['researcher'], 'Q1930187': ['journalist', 'journo'], 'Q2306091': ['sociologist'], 'Q2374149': ['botanist', 'botany', 'plant scientist'], 'Q2526255': ['film director', 'director', 'movie director'], 'Q2516866': ['publisher'], 'Q2066131': ['athlete', 'sportsperson', 'sportsman', 'sportswoman'], 'Q2405480': ['voice actor', 'voice artist', 'voice actress'], 'Q1231865': ['pedagogue', 'educationalist'], 'Q2865819': ['opera singer'], 'Q2259451': ['stage actor', 'stage actress', 'theater actor', 'theater actress', 'theatre actor', 'theatre actress'], 'Q3282637': ['film producer', 'producer', 'movie producer'], 'Q3387717': ['theatre director', 'theater director', 'stage director'], 'Q3055126': ['entomologist'], 'Q3400985': ['academic', 'college graduates', 'university graduates'], 'Q3665646': ['basketball player', 'professional basketball player', 'basketballer'], 'Q3621491': ['archaeologist', 'archeologist'], 'Q4610556': ['model', 'fashion model', 'mannequin'], 'Q4263842': ['literary critic', 'book critic', 'literary critique'], 'Q4964182': ['philosopher'], 'Q5716684': ['dancer'], 'Q6625963': ['novelist'], 'Q10843402': ['swimmer'], 'Q10833314': ['tennis player'], 'Q10871364': ['baseball player'], 'Q10798782': ['television actor', 'actor', 'actress', 'television actress', 'TV actor', 'TV actress'], 'Q10873124': ['chess player'], 'Q10800557': ['film actor', 'film actress', 'movie actor', 'movie actress'], 'Q11338576': ['boxer', 'pugilist'], 'Q11513337': ['athletics competitor', 'track and field athlete', 'athlete (restricted sense)'], 'Q2309784': ['sport cyclist', 'racing cyclist', 'sport bicyclist', 'sport biker'], 'Q11774891': ['ice hockey player', 'hockey player'], 'Q12299841': ['cricketer', 'cricket player'], 'Q13141064': ['badminton player'], 'Q13365117': ['handball player', 'handballer'], 'Q13414980': ['Australian rules footballer', 'Australian footballer', 'Australian rules football player', 'Australian-rules football player'], 'Q14089670': ['rugby union player'], 'Q14467526': ['linguist', 'linguistic scholar'], 'Q15117302': ['volleyball player', 'volleyballer'], 'Q15627169': ['trade unionist', 'labor unionist', 'labour unionist'], 'Q15981151': ['jazz musician'], 'Q16010345': ['performer', 'performing artist', 'scenic artist'], 'Q131512': ['agriculturer', 'farmer', 'agriculturist', 'cultivator', 'grower', 'raiser'], 'Q13382576': ['rower', 'oarsman', 'oarswoman'], 'Q19204627': ['American football player', 'football player'], 'Q15296811': ['drawer', 'illustrator', 'draughtsperson', 'draughtsman', 'draftsperson', 'draftsman', 'draftswoman', 'drafter'], 'Q19595175': ['amateur wrestler', 'wrestler'], 'Q16145150': ['music pedagogue', 'music teacher'], 'Q901': ['scientist', 'natural philosopher'], 'Q33999': ['actor', 'actress', 'actors', 'actresses'], 'Q33231': ['photographer'], 'Q36834': ['composer'], 'Q16533': ['judge', 'magistrate', 'justice', 'judges', 'justices'], 'Q40348': ['lawyer', 'attorney', 'Jurisprudente'], 'Q36180': ['writer', 'author', 'writers', 'authors'], 'Q42973': ['architect'], 'Q43845': ['businessperson', 'businessman', 'dealer', 'business person', 'business woman', 'businesswoman', 'business man'], 'Q39631': ['physician', 'physicians'], 'Q28389': ['screenwriter', 'writer', 'screen writer', 'scriptwriter', 'scenarist', 'film writer', 'tv writer', 'script writer'], 'Q42603': ['priest', 'priestess', 'reverend'], 'Q49757': ['poet', 'bard', 'poetess'], 'Q37226': ['teacher', 'professor', 'educator', 'schoolmaster', 'schoolmistress', 'school teacher'], 'Q47064': ['military personnel']}\n"
     ]
    }
   ],
   "source": [
    "occupations_labels = {k: [v] for k, v in occupations_label.items()}\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT ?o ?altLabel \n",
    "WHERE {\n",
    "  VALUES ?o { %s }\n",
    "  ?o skos:altLabel ?altLabel . FILTER (lang(?altLabel) = \"en\")\n",
    "}\"\"\" % ' '.join('wd:' + o for o in occupations) \n",
    "\n",
    "for result in evalSparql(query):\n",
    "    occupations_labels[result['o']['value'].replace('http://www.wikidata.org/entity/', '')].append(result['altLabel']['value'])\n",
    "\n",
    "print(occupations_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wikipedia articles\n",
    "\n",
    "Here we load the training and the testing sets. To save memory space we use a generator that will read the file each time we iterate over the training or the testing examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "\n",
    "def loadJson(filename):\n",
    "    with gzip.open(filename, 'rt') as fp:\n",
    "        for line in fp:\n",
    "            yield json.loads(line)\n",
    "\n",
    "class MakeIter(object):\n",
    "    def __init__(self, generator_func, **kwargs):\n",
    "        self.generator_func = generator_func\n",
    "        self.kwargs = kwargs\n",
    "    def __iter__(self):\n",
    "        return self.generator_func(**self.kwargs)\n",
    "\n",
    "training_set = MakeIter(loadJson, filename='wiki-train.json.gz')\n",
    "testing_set = MakeIter(loadJson, filename='wiki-test.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract occupations from summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Dictionnary extraction\n",
    "\n",
    "> Using ```occupations_labels``` dictionnary, identify all occupations for each articles. Complete the function below to evaluate the accuracy of such approach. It will serve as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4842586814146957"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_occ = dict()\n",
    "for key, occs in occupations_labels.items():\n",
    "    for occ in occs:\n",
    "        label_to_occ[occ.lower()] = key\n",
    "\n",
    "def predict_dictionnary(example, occupations_labels):\n",
    "    occs = []\n",
    "    summary = example['summary'].lower()\n",
    "    labels = label_to_occ.keys()\n",
    "    for label in labels:\n",
    "        if label in summary:\n",
    "            occs.append(label_to_occ[label])\n",
    "    return occs\n",
    "    \n",
    "def evaluate_dictionnary(training_set, occupations_labels):\n",
    "    nexample = 0\n",
    "    accuracy = 0.\n",
    "    prediction = None\n",
    "    for example in training_set:\n",
    "        prediction = predict_dictionnary(example, occupations_labels)\n",
    "        p = frozenset(prediction)\n",
    "        g = frozenset(example['occupations'])\n",
    "        accuracy += 1.*len(p & g) / len(p | g)\n",
    "        nexample += 1\n",
    "    return accuracy / nexample\n",
    "\n",
    "evaluate_dictionnary(training_set, occupations_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Simple neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the articles \"summary\" and we take the average of the word vectors.\n",
    "This is done with spacy loaded with the fast text vectors.\n",
    "To do the installation/loading [takes 8-10 minutes, dl 1.2Go]\n",
    "```\n",
    "pip3 install spacy\n",
    "wget https://s3-us-west-1.amazonaws.com/fasttext-vectors/cc.en.300.vec.gz\n",
    "python3 -m spacy init-model en /tmp/en_vectors_wiki_lg --vectors-loc cc.en.300.vec.gz\n",
    "rm cc.en.300.vec.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nlp = spacy.load('/tmp/en_vectors_wiki_lg')\n",
    "\n",
    "def vectorize(dataset, nlp):\n",
    "    result = {}\n",
    "    for example in dataset:\n",
    "        doc = nlp(example['summary'], disable=['parser', 'tagger'])\n",
    "        result[example['title']] = {}\n",
    "        result[example['title']]['vector'] = doc.vector\n",
    "        result[example['title']]['summary'] = example['summary']\n",
    "        if 'occupations' in example:\n",
    "            result[example['title']]['occupations'] = example['occupations']\n",
    "    return result\n",
    "    \n",
    "vectorized_training = vectorize(training_set, nlp)\n",
    "vectorized_testing = vectorize(testing_set, nlp)\n",
    "nlp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "427798"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.45162819e-02 -2.45802402e-02 -4.59302496e-03 -4.09372151e-02\n",
      " -4.47662771e-02 -4.18604538e-03 -3.15232435e-03 -1.44802360e-02\n",
      " -1.68499984e-02 -3.69651243e-03 -1.16255814e-02  1.43651171e-02\n",
      "  2.02674349e-03 -5.88953542e-03 -2.17011590e-02  1.02302311e-02\n",
      " -2.49313917e-02 -5.65232616e-03 -2.25581434e-02  8.29069968e-03\n",
      " -1.44069805e-03  2.25197673e-02 -6.81395701e-04 -1.37232570e-02\n",
      " -1.26674427e-02 -3.35569866e-02  1.10627888e-02 -2.37208814e-03\n",
      " -2.30000000e-02  7.58616179e-02 -5.03487710e-04 -2.51116175e-02\n",
      "  9.26511642e-03 -2.52558179e-02 -1.51058156e-02 -9.51627828e-03\n",
      "  1.17523270e-02  1.22441910e-03  1.08139520e-03  3.39302444e-03\n",
      "  2.20116391e-03  1.46860480e-02 -1.43686021e-02  5.76395402e-03\n",
      "  1.74162779e-02 -4.76220921e-02 -1.72569733e-02 -1.49988411e-02\n",
      " -1.77732538e-02  1.58907007e-02 -7.23255938e-03  2.43825577e-02\n",
      " -2.73104683e-02 -3.67430188e-02 -1.48802334e-02 -1.34825567e-02\n",
      " -3.14348824e-02  1.95930228e-02 -6.68605033e-04 -9.24302172e-03\n",
      "  1.56976283e-04 -1.65674444e-02 -1.30372085e-02  6.16298130e-05\n",
      " -3.63139645e-03  2.74534873e-03 -1.62697677e-02 -4.70697694e-03\n",
      "  5.48139494e-03  4.39302297e-03  4.65523303e-02  2.29872130e-02\n",
      "  2.72058025e-02 -5.52790612e-03  2.19720937e-02 -4.41581383e-02\n",
      "  1.33255811e-03  1.20244222e-02  3.49267460e-02  3.76593024e-02\n",
      "  8.65232572e-03 -6.52325572e-03 -1.90407019e-02  1.03569757e-02\n",
      "  1.09301973e-03 -6.28488278e-03  3.98965068e-02 -3.81744131e-02\n",
      " -1.35965087e-02  1.74023230e-02 -1.48686031e-02  5.78604685e-03\n",
      " -8.59186146e-03  4.74418374e-03  1.54720917e-02 -6.42325589e-03\n",
      " -1.58430226e-02 -2.98779178e-02 -1.54255824e-02  3.28209326e-02\n",
      "  2.43825577e-02  1.32907031e-03  1.80883706e-02 -2.72825565e-02\n",
      "  9.28488653e-03 -7.39418622e-03 -7.98023026e-03  1.84244160e-02\n",
      " -9.45350039e-04 -1.16825579e-02  1.15813862e-03 -2.10464321e-04\n",
      " -3.00813979e-03  4.75407019e-02 -8.32790602e-03  4.11511678e-03\n",
      " -1.25604663e-02  8.92209262e-03  7.64534995e-03 -2.65965052e-02\n",
      "  6.58837147e-03 -1.12011610e-02 -9.68022924e-03  1.60023291e-02\n",
      "  1.61629519e-04  3.20906974e-02 -1.59848798e-02  1.14162825e-02\n",
      " -2.40430199e-02  5.39906919e-02 -4.80814092e-03  3.02209193e-03\n",
      "  5.89418598e-03 -3.94418649e-03 -2.68058274e-02 -8.98256153e-03\n",
      " -2.94616278e-02  3.90697829e-03  4.68255766e-03  3.96162830e-03\n",
      " -2.68069748e-02 -2.68395394e-02 -9.76740339e-05  5.67557989e-03\n",
      "  4.43197712e-02 -1.38953477e-02 -3.69888335e-01  1.04639539e-02\n",
      "  1.55372089e-02 -1.35093015e-02 -8.09988379e-02  2.67802346e-02\n",
      "  2.21941881e-02 -7.86627829e-03 -1.00313956e-02  1.52511625e-02\n",
      "  1.45744160e-01  4.61395411e-03  7.26162829e-03  3.14453505e-02\n",
      " -7.95465056e-03 -1.25395320e-02  6.95348764e-03 -2.48023286e-03\n",
      "  6.17325725e-03  1.26546472e-02  1.03558144e-02 -1.21616265e-02\n",
      " -1.27907039e-03 -1.99348871e-02 -9.01860371e-03  4.25581448e-03\n",
      "  7.45790750e-02  1.02186035e-02 -9.93953645e-03  1.72848776e-02\n",
      " -1.03779081e-02  1.46616297e-02 -3.75465187e-03 -2.26953458e-02\n",
      "  5.36046689e-04  6.64511696e-02 -2.53790785e-02  5.80627881e-02\n",
      " -1.42732579e-02  9.22453254e-02 -1.12825576e-02 -2.51837187e-02\n",
      "  3.90697736e-03  5.96395321e-03 -3.02476659e-02  2.63883732e-02\n",
      " -1.69488378e-02  7.39418576e-03  1.60662793e-02 -1.68313961e-02\n",
      " -8.25814065e-03 -1.36965141e-02  7.30697624e-03  1.63453538e-02\n",
      " -4.15407047e-02  1.05633713e-01  1.53325591e-02  6.63023209e-03\n",
      "  3.93279046e-02 -1.27697680e-02 -5.95697621e-03 -8.67441762e-03\n",
      "  1.58593040e-02  9.42093134e-03 -4.15697647e-03  1.34639572e-02\n",
      " -4.10383604e-02 -2.82325619e-03 -2.43790708e-02 -4.02325485e-03\n",
      "  1.65058132e-02  4.21395432e-03  1.25813941e-02  1.64744183e-02\n",
      " -2.81162816e-03  1.34813897e-02 -8.19302350e-03 -7.04767322e-03\n",
      "  1.67139638e-02  1.43581396e-02  1.20023256e-02  4.96162800e-03\n",
      "  1.76325571e-02 -7.07674446e-03 -4.24197726e-02 -2.34697610e-02\n",
      " -1.86058115e-02 -2.32790736e-03  2.98906974e-02  1.53604464e-03\n",
      "  1.95941851e-02 -2.67104693e-02 -1.12453466e-02 -2.54534930e-03\n",
      " -4.29302268e-03  3.56558077e-02 -4.36046888e-04 -8.16406980e-02\n",
      "  5.04779041e-01 -2.18813960e-02  1.15883695e-02  2.14848872e-02\n",
      "  7.80581404e-03  1.55116236e-02 -1.11523261e-02  4.61628864e-04\n",
      "  1.72918607e-02  1.43034859e-02  2.05546506e-02 -8.23488459e-03\n",
      " -3.16290706e-02 -4.83953534e-03 -1.82697661e-02  2.02907110e-03\n",
      " -3.51163093e-04  1.10220918e-02 -8.54755938e-02 -2.68255756e-03\n",
      "  1.83174424e-02  1.91116314e-02 -4.73488262e-03 -8.08255840e-03\n",
      "  1.37906978e-02 -7.76046468e-03 -2.82767452e-02 -2.99069774e-03\n",
      "  1.06569799e-02 -5.99999772e-03  1.11883730e-02  4.28720983e-03\n",
      " -3.12255807e-02 -8.07186142e-02  8.59302282e-03 -8.11744668e-03\n",
      " -5.36279054e-03  1.87046509e-02 -1.10972092e-01 -3.07988375e-02\n",
      "  9.47441999e-03 -1.03662787e-02  1.16337193e-02  3.22093032e-02\n",
      " -2.69790720e-02  2.25430205e-02 -1.49802361e-02 -1.05290683e-02\n",
      " -4.36534919e-02  6.34883530e-04 -2.83197612e-02 -1.37674408e-02\n",
      " -1.50220934e-02  1.30851150e-01 -1.22430259e-02  2.38767453e-02]\n"
     ]
    }
   ],
   "source": [
    "v = vectorized_training['George_Washington']['vector']\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the vectorized_training into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitDict(d, percent):\n",
    "    split_idx = int(len(d) * percent)\n",
    "    d1 = dict(list(d.items())[: split_idx])\n",
    "    d2 = dict(list(d.items())[split_idx:])                \n",
    "\n",
    "    return d1, d2\n",
    "\n",
    "vectorized_training_test, vectorized_training_train = splitDict(vectorized_training, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385019"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_training_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We encode the data\n",
    "import numpy as np\n",
    "\n",
    "def encode_data(vectorized_data):\n",
    "    X = np.array([vectorized_data[article]['vector'] for article in vectorized_data])\n",
    "    y = np.array([[(1 if occupation in vectorized_data[article]['occupations'] else 0)\n",
    "                        for occupation in occupations ] for article in vectorized_data])\n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = encode_data(vectorized_training_train)\n",
    "X_test, y_test = encode_data(vectorized_training_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385019, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385019, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using keras, define a sequential neural network with two layers. Use categorical_crossentropy as a loss function and softmax as the activation function of the output layer\n",
    "\n",
    "You can look into the documentation here: https://keras.io/getting-started/sequential-model-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_dim=300))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(100, activation='softmax'))\n",
    "\n",
    "optimizer = Adam()\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 346517 samples, validate on 38502 samples\n",
      "Epoch 1/50\n",
      "346517/346517 [==============================] - 3s 10us/step - loss: 3.4467 - acc: 0.5043 - val_loss: 2.0377 - val_acc: 0.6464\n",
      "Epoch 2/50\n",
      "346517/346517 [==============================] - 1s 4us/step - loss: 2.3634 - acc: 0.6634 - val_loss: 1.7669 - val_acc: 0.6960\n",
      "Epoch 3/50\n",
      "346517/346517 [==============================] - 1s 4us/step - loss: 2.1769 - acc: 0.6908 - val_loss: 1.6653 - val_acc: 0.7075\n",
      "Epoch 4/50\n",
      "346517/346517 [==============================] - 1s 4us/step - loss: 2.0829 - acc: 0.7043 - val_loss: 1.6013 - val_acc: 0.7304\n",
      "Epoch 5/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 2.0235 - acc: 0.7131 - val_loss: 1.5702 - val_acc: 0.7286\n",
      "Epoch 6/50\n",
      "346517/346517 [==============================] - 1s 4us/step - loss: 1.9830 - acc: 0.7182 - val_loss: 1.5240 - val_acc: 0.7393\n",
      "Epoch 7/50\n",
      "346517/346517 [==============================] - 1s 4us/step - loss: 1.9497 - acc: 0.7229 - val_loss: 1.5029 - val_acc: 0.7402\n",
      "Epoch 8/50\n",
      "346517/346517 [==============================] - 1s 4us/step - loss: 1.9241 - acc: 0.7258 - val_loss: 1.4859 - val_acc: 0.7425\n",
      "Epoch 9/50\n",
      "346517/346517 [==============================] - 1s 4us/step - loss: 1.9023 - acc: 0.7281 - val_loss: 1.4677 - val_acc: 0.7461\n",
      "Epoch 10/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.8829 - acc: 0.7305 - val_loss: 1.4566 - val_acc: 0.7509\n",
      "Epoch 11/50\n",
      "346517/346517 [==============================] - 2s 5us/step - loss: 1.8666 - acc: 0.7323 - val_loss: 1.4465 - val_acc: 0.7502\n",
      "Epoch 12/50\n",
      "346517/346517 [==============================] - 2s 5us/step - loss: 1.8514 - acc: 0.7345 - val_loss: 1.4339 - val_acc: 0.7532\n",
      "Epoch 13/50\n",
      "346517/346517 [==============================] - 2s 5us/step - loss: 1.8370 - acc: 0.7362 - val_loss: 1.4291 - val_acc: 0.7535\n",
      "Epoch 14/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.8246 - acc: 0.7374 - val_loss: 1.4160 - val_acc: 0.7552\n",
      "Epoch 15/50\n",
      "346517/346517 [==============================] - 2s 5us/step - loss: 1.8132 - acc: 0.7388 - val_loss: 1.4080 - val_acc: 0.7581\n",
      "Epoch 16/50\n",
      "346517/346517 [==============================] - 2s 5us/step - loss: 1.8024 - acc: 0.7403 - val_loss: 1.4163 - val_acc: 0.7557\n",
      "Epoch 17/50\n",
      "346517/346517 [==============================] - 2s 5us/step - loss: 1.7917 - acc: 0.7414 - val_loss: 1.4085 - val_acc: 0.7544\n",
      "Epoch 18/50\n",
      "346517/346517 [==============================] - 2s 5us/step - loss: 1.7821 - acc: 0.7423 - val_loss: 1.4025 - val_acc: 0.7545\n",
      "Epoch 19/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.7740 - acc: 0.7429 - val_loss: 1.3982 - val_acc: 0.7584\n",
      "Epoch 20/50\n",
      "346517/346517 [==============================] - 2s 5us/step - loss: 1.7649 - acc: 0.7439 - val_loss: 1.3946 - val_acc: 0.7592\n",
      "Epoch 21/50\n",
      "346517/346517 [==============================] - 2s 5us/step - loss: 1.7563 - acc: 0.7448 - val_loss: 1.3871 - val_acc: 0.7605\n",
      "Epoch 22/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.7481 - acc: 0.7461 - val_loss: 1.3878 - val_acc: 0.7625\n",
      "Epoch 23/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.7409 - acc: 0.7467 - val_loss: 1.3825 - val_acc: 0.7643\n",
      "Epoch 24/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.7330 - acc: 0.7479 - val_loss: 1.3799 - val_acc: 0.7629\n",
      "Epoch 25/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.7271 - acc: 0.7483 - val_loss: 1.3819 - val_acc: 0.7605\n",
      "Epoch 26/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.7201 - acc: 0.7489 - val_loss: 1.3808 - val_acc: 0.7642\n",
      "Epoch 27/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.7133 - acc: 0.7503 - val_loss: 1.3844 - val_acc: 0.7561\n",
      "Epoch 28/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.7063 - acc: 0.7508 - val_loss: 1.3811 - val_acc: 0.7571\n",
      "Epoch 29/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.6999 - acc: 0.7510 - val_loss: 1.3794 - val_acc: 0.7585\n",
      "Epoch 30/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.6947 - acc: 0.7521 - val_loss: 1.3769 - val_acc: 0.7628\n",
      "Epoch 31/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.6886 - acc: 0.7528 - val_loss: 1.3694 - val_acc: 0.7621\n",
      "Epoch 32/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.6817 - acc: 0.7533 - val_loss: 1.3698 - val_acc: 0.7621\n",
      "Epoch 33/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.6761 - acc: 0.7540 - val_loss: 1.3692 - val_acc: 0.7578\n",
      "Epoch 34/50\n",
      "346517/346517 [==============================] - 2s 5us/step - loss: 1.6711 - acc: 0.7550 - val_loss: 1.3703 - val_acc: 0.7580\n",
      "Epoch 35/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.6648 - acc: 0.7552 - val_loss: 1.3704 - val_acc: 0.7597\n",
      "Epoch 36/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.6593 - acc: 0.7561 - val_loss: 1.3807 - val_acc: 0.7574\n",
      "Epoch 37/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.6547 - acc: 0.7569 - val_loss: 1.3721 - val_acc: 0.7633\n",
      "Epoch 38/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.6492 - acc: 0.7571 - val_loss: 1.3738 - val_acc: 0.7649\n",
      "Epoch 39/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.6439 - acc: 0.7576 - val_loss: 1.3774 - val_acc: 0.7571\n",
      "Epoch 40/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.6391 - acc: 0.7583 - val_loss: 1.3699 - val_acc: 0.7669\n",
      "Epoch 41/50\n",
      "346517/346517 [==============================] - 2s 5us/step - loss: 1.6346 - acc: 0.7587 - val_loss: 1.3758 - val_acc: 0.7593\n",
      "Epoch 42/50\n",
      "346517/346517 [==============================] - 2s 5us/step - loss: 1.6296 - acc: 0.7596 - val_loss: 1.3723 - val_acc: 0.7653\n",
      "Epoch 43/50\n",
      "346517/346517 [==============================] - 2s 5us/step - loss: 1.6234 - acc: 0.7601 - val_loss: 1.3782 - val_acc: 0.7545\n",
      "Epoch 44/50\n",
      "346517/346517 [==============================] - 2s 5us/step - loss: 1.6192 - acc: 0.7606 - val_loss: 1.3727 - val_acc: 0.7608\n",
      "Epoch 45/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.6146 - acc: 0.7607 - val_loss: 1.3785 - val_acc: 0.7605\n",
      "Epoch 46/50\n",
      "346517/346517 [==============================] - 2s 5us/step - loss: 1.6098 - acc: 0.7614 - val_loss: 1.3792 - val_acc: 0.7604\n",
      "Epoch 47/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.6046 - acc: 0.7622 - val_loss: 1.3768 - val_acc: 0.7589\n",
      "Epoch 48/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.6009 - acc: 0.7625 - val_loss: 1.3747 - val_acc: 0.7607\n",
      "Epoch 49/50\n",
      "346517/346517 [==============================] - 2s 4us/step - loss: 1.5958 - acc: 0.7631 - val_loss: 1.3762 - val_acc: 0.7595\n",
      "Epoch 50/50\n",
      "346517/346517 [==============================] - 2s 5us/step - loss: 1.5913 - acc: 0.7638 - val_loss: 1.3802 - val_acc: 0.7595\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, batch_size=1024, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Complete the function predict: output the list of occupations where the corresponding neuron on the output layer of our model has a value > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Q639669', 'Q33999', 'Q177220'}\n"
     ]
    }
   ],
   "source": [
    "def predict_nn(model, article_name, vectorized_dataset):\n",
    "    input_vector = vectorized_dataset[article_name]['vector'].reshape((1, 300))\n",
    "    scores = model.predict(input_vector).reshape(100)\n",
    "    predictions = np.where(scores > 0.1)[0]\n",
    "#     print(scores[predictions])\n",
    "    return set(np.array(occupations)[predictions])\n",
    "\n",
    "print(predict_nn(model, 'Elvis_Presley', vectorized_training))\n",
    "# should be {'Q177220'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_nn(vectorized_training, model):\n",
    "    nexample = 0\n",
    "    accuracy = 0.\n",
    "    prediction = None\n",
    "    for article_name in vectorized_training:\n",
    "        prediction = predict_nn(model, article_name, vectorized_training)\n",
    "        p = frozenset(prediction)\n",
    "        g = frozenset(vectorized_training[article_name]['occupations'])\n",
    "        accuracy += 1.*len(p & g) / len(p | g)\n",
    "        nexample += 1\n",
    "    return accuracy / nexample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.698508658729491\n",
      "0.6544649581232717\n"
     ]
    }
   ],
   "source": [
    "print(evaluate_nn(vectorized_training_train, model))\n",
    "print(evaluate_nn(vectorized_training_test, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 Your approach: CNN + BiRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, GRU, Dropout, Conv1D, MaxPooling1D, MaxPooling1D, Bidirectional, BatchNormalization, LSTM\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the dataset into summaries, titles and occupations\n",
    "def parse(dataset):\n",
    "    titles = []\n",
    "    summaries = []\n",
    "    occs = []\n",
    "    ml = 0 \n",
    "    for example in dataset:\n",
    "        titles.append(example['title'])\n",
    "        summaries.append(example['summary'].lower())        \n",
    "        ml = max(len(example['summary']), ml)\n",
    "        if 'occupations' in example:\n",
    "            occs.append(example['occupations'])\n",
    "        else:\n",
    "            occs.append([])\n",
    "    return titles, summaries, occs\n",
    "    \n",
    "titles_train, summaries_train, occs_train = parse(training_set)\n",
    "titles_test, summaries_test, occs_test = parse(testing_set)\n",
    "\n",
    "# split the training_set into train and test set \n",
    "s = int(len(titles_train) * 0.8)\n",
    "titles_train_train, summaries_train_train, occs_train_train = titles_train[:s], summaries_train[:s], occs_train[:s]\n",
    "titles_train_test, summaries_train_test, occs_train_test = titles_train[s:], summaries_train[s:], occs_train[s:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 370295 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "n_samples = len(titles_train_train)\n",
    "maxlen = 300\n",
    "training_samples = int(n_samples * 0.9)\n",
    "validation_samples = n_samples - training_samples\n",
    "max_words = 100000 # we only use the most 100,000 common words\n",
    "\n",
    "# Tokenize the text\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(summaries_train_train)\n",
    "\n",
    "# convert text to sequences\n",
    "sequences =  tokenizer.texts_to_sequences(summaries_train_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(summaries_train_test)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found', len(word_index), 'unique tokens.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_occs_to_labels(occupations, occs_train):\n",
    "    labels = []\n",
    "    for i in range(len(occs_train)):\n",
    "        label = []\n",
    "        \n",
    "        for occ in occupations:\n",
    "            if occ in occs_train[i]:\n",
    "                label.append(1)\n",
    "            else:\n",
    "                label.append(0)\n",
    "        labels.append(label)\n",
    "    return np.array(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (342333, 300)\n",
      "Shape of label tensor: (342333, 100)\n"
     ]
    }
   ],
   "source": [
    "# pad the sequences so all the sequences have the same length\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "data_test = pad_sequences(sequences_test, maxlen=maxlen)\n",
    "\n",
    "# encode the occupations\n",
    "labels = convert_occs_to_labels(occupations, occs_train_train)\n",
    "\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# shuffle the data\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "# split into training and validation set\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir = 'glove.6B'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.42B.300d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found', len(embeddings_index), 'word vectors.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build embedding matrix to load into embedding layer\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 300, 300)          30000000  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 300, 64)           57664     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300, 64)           256       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 100, 128)          24704     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100, 128)          512       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 33, 128)           0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 33, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 33, 256)           98560     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 33, 256)           1024      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 11, 256)           0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 11, 256)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 11, 128)           123264    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 11, 128)           74112     \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 11, 128)           74112     \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 128)               74112     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               66048     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 100)               25700     \n",
      "=================================================================\n",
      "Total params: 31,085,988\n",
      "Trainable params: 31,082,020\n",
      "Non-trainable params: 3,968\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "\n",
    "model.add(Conv1D(64,kernel_size=3,padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv1D(128,kernel_size=3,padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Conv1D(256,kernel_size=3,padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "model.add(Bidirectional(GRU(64, return_sequences=True, recurrent_dropout = 0.1, dropout = 0.1)))\n",
    "\n",
    "model.add(Bidirectional(GRU(64, return_sequences=True, recurrent_dropout = 0.1, dropout = 0.1)))\n",
    "\n",
    "model.add(Bidirectional(GRU(64, return_sequences=True, recurrent_dropout = 0.1, dropout = 0.1)))\n",
    "\n",
    "model.add(Bidirectional(GRU(64, recurrent_dropout = 0.1, dropout = 0.1)))\n",
    "\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(256,activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "# model.add(Dense(100, activation='softmax'))\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the embedding in the model\n",
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False # we will not train this layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"model3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras.backend.tensorflow_backend as tfb\n",
    "from keras import backend as K\n",
    "\n",
    "POS_WEIGHT = 10  # multiplier for positive targets\n",
    "\n",
    "def weighted_binary_crossentropy(target, output):\n",
    "    # transform back to logits\n",
    "    _epsilon = tfb._to_tensor(tfb.epsilon(), output.dtype.base_dtype)\n",
    "    output = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\n",
    "    output = tf.log(output / (1 - output))\n",
    "    # compute weighted loss\n",
    "    loss = tf.nn.weighted_cross_entropy_with_logits(targets=target,\n",
    "                                                    logits=output,\n",
    "                                                    pos_weight=POS_WEIGHT)\n",
    "    return tf.reduce_mean(loss, axis=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 308099 samples, validate on 34234 samples\n",
      "Epoch 1/200\n",
      "308099/308099 [==============================] - 81s 263us/step - loss: 0.0757 - acc: 0.7635 - val_loss: 0.0704 - val_acc: 0.7675\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.07041, saving model to best_model.h5\n",
      "Epoch 2/200\n",
      "308099/308099 [==============================] - 71s 230us/step - loss: 0.0750 - acc: 0.7643 - val_loss: 0.0723 - val_acc: 0.7710\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.07041\n",
      "Epoch 3/200\n",
      "308099/308099 [==============================] - 70s 227us/step - loss: 0.0747 - acc: 0.7652 - val_loss: 0.0720 - val_acc: 0.7705\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.07041\n",
      "Epoch 4/200\n",
      "308099/308099 [==============================] - 70s 228us/step - loss: 0.0744 - acc: 0.7651 - val_loss: 0.0743 - val_acc: 0.7698\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.07041\n",
      "Epoch 5/200\n",
      "308099/308099 [==============================] - 70s 228us/step - loss: 0.0742 - acc: 0.7658 - val_loss: 0.0723 - val_acc: 0.7750\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.07041\n",
      "Epoch 6/200\n",
      "308099/308099 [==============================] - 70s 228us/step - loss: 0.0739 - acc: 0.7657 - val_loss: 0.0729 - val_acc: 0.7663\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.07041\n",
      "Epoch 7/200\n",
      "308099/308099 [==============================] - 70s 227us/step - loss: 0.0725 - acc: 0.7667 - val_loss: 0.0695 - val_acc: 0.7765\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.07041 to 0.06947, saving model to best_model.h5\n",
      "Epoch 8/200\n",
      "308099/308099 [==============================] - 70s 226us/step - loss: 0.0717 - acc: 0.7675 - val_loss: 0.0692 - val_acc: 0.7772\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.06947 to 0.06921, saving model to best_model.h5\n",
      "Epoch 9/200\n",
      "308099/308099 [==============================] - 70s 227us/step - loss: 0.0714 - acc: 0.7677 - val_loss: 0.0692 - val_acc: 0.7761\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.06921 to 0.06921, saving model to best_model.h5\n",
      "Epoch 10/200\n",
      "308099/308099 [==============================] - 70s 226us/step - loss: 0.0713 - acc: 0.7674 - val_loss: 0.0691 - val_acc: 0.7755\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.06921 to 0.06908, saving model to best_model.h5\n",
      "Epoch 11/200\n",
      "308099/308099 [==============================] - 70s 227us/step - loss: 0.0711 - acc: 0.7674 - val_loss: 0.0691 - val_acc: 0.7761\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.06908\n",
      "Epoch 12/200\n",
      "308099/308099 [==============================] - 70s 226us/step - loss: 0.0710 - acc: 0.7675 - val_loss: 0.0690 - val_acc: 0.7746\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.06908 to 0.06896, saving model to best_model.h5\n",
      "Epoch 13/200\n",
      "308099/308099 [==============================] - 70s 226us/step - loss: 0.0709 - acc: 0.7676 - val_loss: 0.0691 - val_acc: 0.7764\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.06896\n",
      "Epoch 14/200\n",
      "308099/308099 [==============================] - 70s 227us/step - loss: 0.0709 - acc: 0.7677 - val_loss: 0.0690 - val_acc: 0.7757\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.06896\n",
      "Epoch 15/200\n",
      "308099/308099 [==============================] - 70s 227us/step - loss: 0.0707 - acc: 0.7687 - val_loss: 0.0690 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.06896 to 0.06895, saving model to best_model.h5\n",
      "Epoch 16/200\n",
      "308099/308099 [==============================] - 70s 227us/step - loss: 0.0705 - acc: 0.7681 - val_loss: 0.0691 - val_acc: 0.7758\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.06895\n",
      "Epoch 17/200\n",
      "308099/308099 [==============================] - 70s 227us/step - loss: 0.0706 - acc: 0.7683 - val_loss: 0.0690 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.06895\n",
      "Epoch 18/200\n",
      "308099/308099 [==============================] - 70s 226us/step - loss: 0.0704 - acc: 0.7695 - val_loss: 0.0689 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.06895 to 0.06895, saving model to best_model.h5\n",
      "Epoch 19/200\n",
      "308099/308099 [==============================] - 70s 227us/step - loss: 0.0705 - acc: 0.7691 - val_loss: 0.0690 - val_acc: 0.7761\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.06895\n",
      "Epoch 20/200\n",
      "308099/308099 [==============================] - 70s 226us/step - loss: 0.0704 - acc: 0.7690 - val_loss: 0.0690 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.06895\n",
      "Epoch 21/200\n",
      "308099/308099 [==============================] - 70s 227us/step - loss: 0.0703 - acc: 0.7691 - val_loss: 0.0690 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.06895\n",
      "Epoch 22/200\n",
      "308099/308099 [==============================] - 70s 226us/step - loss: 0.0704 - acc: 0.7688 - val_loss: 0.0689 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.06895\n",
      "Epoch 23/200\n",
      "308099/308099 [==============================] - 70s 227us/step - loss: 0.0702 - acc: 0.7684 - val_loss: 0.0690 - val_acc: 0.7763\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.06895\n",
      "Epoch 24/200\n",
      "308099/308099 [==============================] - 70s 227us/step - loss: 0.0703 - acc: 0.7687 - val_loss: 0.0690 - val_acc: 0.7761\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.06895\n",
      "Epoch 25/200\n",
      "308099/308099 [==============================] - 70s 227us/step - loss: 0.0703 - acc: 0.7689 - val_loss: 0.0690 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.06895\n",
      "Epoch 26/200\n",
      "308099/308099 [==============================] - 70s 226us/step - loss: 0.0703 - acc: 0.7687 - val_loss: 0.0690 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.06895\n",
      "Epoch 27/200\n",
      "308099/308099 [==============================] - 71s 230us/step - loss: 0.0704 - acc: 0.7681 - val_loss: 0.0690 - val_acc: 0.7761\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.06895\n",
      "Epoch 28/200\n",
      "308099/308099 [==============================] - 71s 231us/step - loss: 0.0704 - acc: 0.7687 - val_loss: 0.0690 - val_acc: 0.7762\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.06895\n",
      "Epoch 00028: early stopping\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss',\n",
    "                              min_delta=0,\n",
    "                              patience=10,\n",
    "                              verbose=1, mode='auto'),\n",
    "    ReduceLROnPlateau(monitor='val_loss', \n",
    "                      factor=0.1, \n",
    "                      patience=5, \n",
    "                      verbose=1, \n",
    "                      mode='auto', \n",
    "                      min_delta=0.0001, \n",
    "                      cooldown=0, \n",
    "                      min_lr=0),\n",
    "    ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True, verbose=1, mode='min')\n",
    "]\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "    loss=weighted_binary_crossentropy, # give more weight to the 1-label\n",
    "    metrics=['acc'])\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "    epochs=200,\n",
    "    callbacks=callbacks,\n",
    "    batch_size=2000,\n",
    "    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn_2(model, input_vector, print_score = False):\n",
    "    \n",
    "    scores = model.predict(input_vector).reshape(100)\n",
    "    predictions1 = np.where(scores > 0.6)[0]\n",
    "    predictions2 = np.where(scores > 0.7)[0]\n",
    "    predictions3 = np.where(scores > 0.8)[0]\n",
    "    predictions4 = np.where(scores > 0.9)[0]\n",
    "    predictions5 = np.where(scores > 0.95)[0]\n",
    "    predictions6 = np.where(scores > 0.99)[0]\n",
    "    if print_score:\n",
    "        print(scores[predictions1])\n",
    "        print(scores[predictions2])\n",
    "        print(scores[predictions3])\n",
    "        print(scores[predictions4])\n",
    "        print(scores[predictions5])\n",
    "        print(scores[predictions6])\n",
    "    res1 = set(np.array(occupations)[predictions1])\n",
    "    res2 = set(np.array(occupations)[predictions2])\n",
    "    res3 = set(np.array(occupations)[predictions3])\n",
    "    res4 = set(np.array(occupations)[predictions4])\n",
    "    res5 = set(np.array(occupations)[predictions5])\n",
    "    res6 = set(np.array(occupations)[predictions6])\n",
    "    return res1, res2, res3, res4, res5, res6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85584"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(titles_train_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "0.0\n",
      "0.6 :  0.0\n",
      "0.7 :  0.0\n",
      "0.8 :  0.0\n",
      "0.9 :  0.0\n",
      "0.95 :  0.0\n",
      "0.99 :  0.0\n",
      "=========================\n",
      "0.0117\n",
      "0.6 :  0.0081\n",
      "0.7 :  0.0085\n",
      "0.8 :  0.0088\n",
      "0.9 :  0.0088\n",
      "0.95 :  0.0084\n",
      "0.99 :  0.0064\n",
      "=========================\n",
      "0.0234\n",
      "0.6 :  0.0162\n",
      "0.7 :  0.0171\n",
      "0.8 :  0.0176\n",
      "0.9 :  0.0176\n",
      "0.95 :  0.0168\n",
      "0.99 :  0.0129\n",
      "=========================\n",
      "0.0351\n",
      "0.6 :  0.0242\n",
      "0.7 :  0.0254\n",
      "0.8 :  0.0262\n",
      "0.9 :  0.0265\n",
      "0.95 :  0.0251\n",
      "0.99 :  0.0191\n",
      "=========================\n",
      "0.0467\n",
      "0.6 :  0.0323\n",
      "0.7 :  0.0338\n",
      "0.8 :  0.0351\n",
      "0.9 :  0.0354\n",
      "0.95 :  0.0337\n",
      "0.99 :  0.0258\n",
      "=========================\n",
      "0.0584\n",
      "0.6 :  0.0402\n",
      "0.7 :  0.042\n",
      "0.8 :  0.0437\n",
      "0.9 :  0.0441\n",
      "0.95 :  0.0421\n",
      "0.99 :  0.032\n",
      "=========================\n",
      "0.0701\n",
      "0.6 :  0.0482\n",
      "0.7 :  0.0505\n",
      "0.8 :  0.0525\n",
      "0.9 :  0.0531\n",
      "0.95 :  0.0506\n",
      "0.99 :  0.0383\n",
      "=========================\n",
      "0.0818\n",
      "0.6 :  0.0566\n",
      "0.7 :  0.0592\n",
      "0.8 :  0.0615\n",
      "0.9 :  0.0622\n",
      "0.95 :  0.0593\n",
      "0.99 :  0.0453\n",
      "=========================\n",
      "0.0935\n",
      "0.6 :  0.0646\n",
      "0.7 :  0.0676\n",
      "0.8 :  0.0701\n",
      "0.9 :  0.071\n",
      "0.95 :  0.0679\n",
      "0.99 :  0.0515\n",
      "=========================\n",
      "0.1052\n",
      "0.6 :  0.0728\n",
      "0.7 :  0.0761\n",
      "0.8 :  0.079\n",
      "0.9 :  0.08\n",
      "0.95 :  0.0765\n",
      "0.99 :  0.0583\n",
      "=========================\n",
      "0.1168\n",
      "0.6 :  0.0809\n",
      "0.7 :  0.0846\n",
      "0.8 :  0.0877\n",
      "0.9 :  0.0887\n",
      "0.95 :  0.0847\n",
      "0.99 :  0.0648\n",
      "=========================\n",
      "0.1285\n",
      "0.6 :  0.0891\n",
      "0.7 :  0.0932\n",
      "0.8 :  0.0966\n",
      "0.9 :  0.0977\n",
      "0.95 :  0.0934\n",
      "0.99 :  0.0718\n",
      "=========================\n",
      "0.1402\n",
      "0.6 :  0.0974\n",
      "0.7 :  0.1018\n",
      "0.8 :  0.1055\n",
      "0.9 :  0.1067\n",
      "0.95 :  0.102\n",
      "0.99 :  0.0786\n",
      "=========================\n",
      "0.1519\n",
      "0.6 :  0.1054\n",
      "0.7 :  0.1102\n",
      "0.8 :  0.1143\n",
      "0.9 :  0.1155\n",
      "0.95 :  0.1105\n",
      "0.99 :  0.0851\n",
      "=========================\n",
      "0.1636\n",
      "0.6 :  0.1136\n",
      "0.7 :  0.1188\n",
      "0.8 :  0.1231\n",
      "0.9 :  0.1245\n",
      "0.95 :  0.1191\n",
      "0.99 :  0.0919\n",
      "=========================\n",
      "0.1753\n",
      "0.6 :  0.1217\n",
      "0.7 :  0.1272\n",
      "0.8 :  0.1319\n",
      "0.9 :  0.1335\n",
      "0.95 :  0.1277\n",
      "0.99 :  0.0987\n",
      "=========================\n",
      "0.187\n",
      "0.6 :  0.13\n",
      "0.7 :  0.1359\n",
      "0.8 :  0.1408\n",
      "0.9 :  0.1426\n",
      "0.95 :  0.1365\n",
      "0.99 :  0.1056\n",
      "=========================\n",
      "0.1986\n",
      "0.6 :  0.138\n",
      "0.7 :  0.1443\n",
      "0.8 :  0.1496\n",
      "0.9 :  0.1515\n",
      "0.95 :  0.145\n",
      "0.99 :  0.1121\n",
      "=========================\n",
      "0.2103\n",
      "0.6 :  0.1466\n",
      "0.7 :  0.1532\n",
      "0.8 :  0.1586\n",
      "0.9 :  0.1607\n",
      "0.95 :  0.1539\n",
      "0.99 :  0.1194\n",
      "=========================\n",
      "0.222\n",
      "0.6 :  0.1543\n",
      "0.7 :  0.1612\n",
      "0.8 :  0.167\n",
      "0.9 :  0.1692\n",
      "0.95 :  0.162\n",
      "0.99 :  0.1253\n",
      "=========================\n",
      "0.2337\n",
      "0.6 :  0.1627\n",
      "0.7 :  0.17\n",
      "0.8 :  0.176\n",
      "0.9 :  0.1785\n",
      "0.95 :  0.171\n",
      "0.99 :  0.1324\n",
      "=========================\n",
      "0.2454\n",
      "0.6 :  0.1712\n",
      "0.7 :  0.1789\n",
      "0.8 :  0.1851\n",
      "0.9 :  0.1878\n",
      "0.95 :  0.1799\n",
      "0.99 :  0.1397\n",
      "=========================\n",
      "0.2571\n",
      "0.6 :  0.1802\n",
      "0.7 :  0.1883\n",
      "0.8 :  0.1948\n",
      "0.9 :  0.1975\n",
      "0.95 :  0.1893\n",
      "0.99 :  0.1477\n",
      "=========================\n",
      "0.2687\n",
      "0.6 :  0.1883\n",
      "0.7 :  0.1968\n",
      "0.8 :  0.2036\n",
      "0.9 :  0.2065\n",
      "0.95 :  0.1979\n",
      "0.99 :  0.1544\n",
      "=========================\n",
      "0.2804\n",
      "0.6 :  0.1967\n",
      "0.7 :  0.2055\n",
      "0.8 :  0.2126\n",
      "0.9 :  0.2156\n",
      "0.95 :  0.2066\n",
      "0.99 :  0.1614\n",
      "=========================\n",
      "0.2921\n",
      "0.6 :  0.2051\n",
      "0.7 :  0.2143\n",
      "0.8 :  0.2217\n",
      "0.9 :  0.2248\n",
      "0.95 :  0.2154\n",
      "0.99 :  0.1685\n",
      "=========================\n",
      "0.3038\n",
      "0.6 :  0.2136\n",
      "0.7 :  0.2232\n",
      "0.8 :  0.2308\n",
      "0.9 :  0.2341\n",
      "0.95 :  0.2244\n",
      "0.99 :  0.1757\n",
      "=========================\n",
      "0.3155\n",
      "0.6 :  0.2226\n",
      "0.7 :  0.2324\n",
      "0.8 :  0.2403\n",
      "0.9 :  0.2437\n",
      "0.95 :  0.2339\n",
      "0.99 :  0.1836\n",
      "=========================\n",
      "0.3272\n",
      "0.6 :  0.2311\n",
      "0.7 :  0.2413\n",
      "0.8 :  0.2494\n",
      "0.9 :  0.2531\n",
      "0.95 :  0.2428\n",
      "0.99 :  0.191\n",
      "=========================\n",
      "0.3388\n",
      "0.6 :  0.2397\n",
      "0.7 :  0.2501\n",
      "0.8 :  0.2585\n",
      "0.9 :  0.2625\n",
      "0.95 :  0.2518\n",
      "0.99 :  0.1988\n",
      "=========================\n",
      "0.3505\n",
      "0.6 :  0.2481\n",
      "0.7 :  0.2588\n",
      "0.8 :  0.2675\n",
      "0.9 :  0.2715\n",
      "0.95 :  0.2607\n",
      "0.99 :  0.2063\n",
      "=========================\n",
      "0.3622\n",
      "0.6 :  0.2561\n",
      "0.7 :  0.2671\n",
      "0.8 :  0.2762\n",
      "0.9 :  0.2802\n",
      "0.95 :  0.2691\n",
      "0.99 :  0.2136\n",
      "=========================\n",
      "0.3739\n",
      "0.6 :  0.2647\n",
      "0.7 :  0.2761\n",
      "0.8 :  0.2855\n",
      "0.9 :  0.2896\n",
      "0.95 :  0.2781\n",
      "0.99 :  0.2215\n",
      "=========================\n",
      "0.3856\n",
      "0.6 :  0.2725\n",
      "0.7 :  0.2841\n",
      "0.8 :  0.2937\n",
      "0.9 :  0.2976\n",
      "0.95 :  0.2857\n",
      "0.99 :  0.2275\n",
      "=========================\n",
      "0.3973\n",
      "0.6 :  0.2793\n",
      "0.7 :  0.2911\n",
      "0.8 :  0.3008\n",
      "0.9 :  0.3041\n",
      "0.95 :  0.2913\n",
      "0.99 :  0.2308\n",
      "=========================\n",
      "0.409\n",
      "0.6 :  0.2868\n",
      "0.7 :  0.2988\n",
      "0.8 :  0.3087\n",
      "0.9 :  0.3115\n",
      "0.95 :  0.2978\n",
      "0.99 :  0.2351\n",
      "=========================\n",
      "0.4206\n",
      "0.6 :  0.2945\n",
      "0.7 :  0.3067\n",
      "0.8 :  0.3166\n",
      "0.9 :  0.319\n",
      "0.95 :  0.3046\n",
      "0.99 :  0.24\n",
      "=========================\n",
      "0.4323\n",
      "0.6 :  0.3023\n",
      "0.7 :  0.3148\n",
      "0.8 :  0.3248\n",
      "0.9 :  0.3268\n",
      "0.95 :  0.3117\n",
      "0.99 :  0.245\n",
      "=========================\n",
      "0.444\n",
      "0.6 :  0.3103\n",
      "0.7 :  0.3229\n",
      "0.8 :  0.3331\n",
      "0.9 :  0.3347\n",
      "0.95 :  0.3191\n",
      "0.99 :  0.2506\n",
      "=========================\n",
      "0.4557\n",
      "0.6 :  0.3183\n",
      "0.7 :  0.3312\n",
      "0.8 :  0.3415\n",
      "0.9 :  0.3429\n",
      "0.95 :  0.3268\n",
      "0.99 :  0.2563\n",
      "=========================\n",
      "0.4674\n",
      "0.6 :  0.3261\n",
      "0.7 :  0.3392\n",
      "0.8 :  0.3497\n",
      "0.9 :  0.351\n",
      "0.95 :  0.3343\n",
      "0.99 :  0.2621\n",
      "=========================\n",
      "0.4791\n",
      "0.6 :  0.3341\n",
      "0.7 :  0.3476\n",
      "0.8 :  0.3581\n",
      "0.9 :  0.3591\n",
      "0.95 :  0.3418\n",
      "0.99 :  0.2675\n",
      "=========================\n",
      "0.4907\n",
      "0.6 :  0.342\n",
      "0.7 :  0.3557\n",
      "0.8 :  0.3663\n",
      "0.9 :  0.3671\n",
      "0.95 :  0.3493\n",
      "0.99 :  0.2732\n",
      "=========================\n",
      "0.5024\n",
      "0.6 :  0.3498\n",
      "0.7 :  0.3637\n",
      "0.8 :  0.3747\n",
      "0.9 :  0.3754\n",
      "0.95 :  0.3569\n",
      "0.99 :  0.279\n",
      "=========================\n",
      "0.5141\n",
      "0.6 :  0.3578\n",
      "0.7 :  0.372\n",
      "0.8 :  0.3831\n",
      "0.9 :  0.3837\n",
      "0.95 :  0.3648\n",
      "0.99 :  0.2851\n",
      "=========================\n",
      "0.5258\n",
      "0.6 :  0.3656\n",
      "0.7 :  0.3802\n",
      "0.8 :  0.3915\n",
      "0.9 :  0.3921\n",
      "0.95 :  0.3726\n",
      "0.99 :  0.2908\n",
      "=========================\n",
      "0.5375\n",
      "0.6 :  0.3734\n",
      "0.7 :  0.3883\n",
      "0.8 :  0.4\n",
      "0.9 :  0.4007\n",
      "0.95 :  0.3808\n",
      "0.99 :  0.2967\n",
      "=========================\n",
      "0.5492\n",
      "0.6 :  0.3813\n",
      "0.7 :  0.3965\n",
      "0.8 :  0.4085\n",
      "0.9 :  0.4094\n",
      "0.95 :  0.3889\n",
      "0.99 :  0.3027\n",
      "=========================\n",
      "0.5609\n",
      "0.6 :  0.389\n",
      "0.7 :  0.4044\n",
      "0.8 :  0.4167\n",
      "0.9 :  0.4179\n",
      "0.95 :  0.3971\n",
      "0.99 :  0.3085\n",
      "=========================\n",
      "0.5725\n",
      "0.6 :  0.3973\n",
      "0.7 :  0.4131\n",
      "0.8 :  0.4255\n",
      "0.9 :  0.427\n",
      "0.95 :  0.4057\n",
      "0.99 :  0.3151\n",
      "=========================\n",
      "0.5842\n",
      "0.6 :  0.4051\n",
      "0.7 :  0.4212\n",
      "0.8 :  0.4341\n",
      "0.9 :  0.4358\n",
      "0.95 :  0.414\n",
      "0.99 :  0.321\n",
      "=========================\n",
      "0.5959\n",
      "0.6 :  0.413\n",
      "0.7 :  0.4295\n",
      "0.8 :  0.4427\n",
      "0.9 :  0.4444\n",
      "0.95 :  0.4222\n",
      "0.99 :  0.3275\n",
      "=========================\n",
      "0.6076\n",
      "0.6 :  0.421\n",
      "0.7 :  0.4379\n",
      "0.8 :  0.4514\n",
      "0.9 :  0.4533\n",
      "0.95 :  0.4307\n",
      "0.99 :  0.3339\n",
      "=========================\n",
      "0.6193\n",
      "0.6 :  0.4294\n",
      "0.7 :  0.4466\n",
      "0.8 :  0.4604\n",
      "0.9 :  0.4625\n",
      "0.95 :  0.4394\n",
      "0.99 :  0.3409\n",
      "=========================\n",
      "0.631\n",
      "0.6 :  0.4374\n",
      "0.7 :  0.455\n",
      "0.8 :  0.4692\n",
      "0.9 :  0.4713\n",
      "0.95 :  0.4478\n",
      "0.99 :  0.3473\n",
      "=========================\n",
      "0.6426\n",
      "0.6 :  0.4455\n",
      "0.7 :  0.4635\n",
      "0.8 :  0.4779\n",
      "0.9 :  0.4802\n",
      "0.95 :  0.4562\n",
      "0.99 :  0.354\n",
      "=========================\n",
      "0.6543\n",
      "0.6 :  0.4537\n",
      "0.7 :  0.472\n",
      "0.8 :  0.4867\n",
      "0.9 :  0.4893\n",
      "0.95 :  0.465\n",
      "0.99 :  0.3607\n",
      "=========================\n",
      "0.666\n",
      "0.6 :  0.4618\n",
      "0.7 :  0.4806\n",
      "0.8 :  0.4956\n",
      "0.9 :  0.4982\n",
      "0.95 :  0.4735\n",
      "0.99 :  0.3673\n",
      "=========================\n",
      "0.6777\n",
      "0.6 :  0.4703\n",
      "0.7 :  0.4894\n",
      "0.8 :  0.5046\n",
      "0.9 :  0.5072\n",
      "0.95 :  0.4823\n",
      "0.99 :  0.3743\n",
      "=========================\n",
      "0.6894\n",
      "0.6 :  0.4779\n",
      "0.7 :  0.4975\n",
      "0.8 :  0.513\n",
      "0.9 :  0.516\n",
      "0.95 :  0.4905\n",
      "0.99 :  0.3805\n",
      "=========================\n",
      "0.7011\n",
      "0.6 :  0.4861\n",
      "0.7 :  0.506\n",
      "0.8 :  0.522\n",
      "0.9 :  0.525\n",
      "0.95 :  0.499\n",
      "0.99 :  0.3871\n",
      "=========================\n",
      "0.7128\n",
      "0.6 :  0.4942\n",
      "0.7 :  0.5146\n",
      "0.8 :  0.5309\n",
      "0.9 :  0.5341\n",
      "0.95 :  0.5079\n",
      "0.99 :  0.3938\n",
      "=========================\n",
      "0.7244\n",
      "0.6 :  0.5025\n",
      "0.7 :  0.5233\n",
      "0.8 :  0.5399\n",
      "0.9 :  0.5433\n",
      "0.95 :  0.5166\n",
      "0.99 :  0.4007\n",
      "=========================\n",
      "0.7361\n",
      "0.6 :  0.5106\n",
      "0.7 :  0.5319\n",
      "0.8 :  0.5487\n",
      "0.9 :  0.5523\n",
      "0.95 :  0.525\n",
      "0.99 :  0.407\n",
      "=========================\n",
      "0.7478\n",
      "0.6 :  0.5188\n",
      "0.7 :  0.5405\n",
      "0.8 :  0.5577\n",
      "0.9 :  0.5613\n",
      "0.95 :  0.5336\n",
      "0.99 :  0.4138\n",
      "=========================\n",
      "0.7595\n",
      "0.6 :  0.527\n",
      "0.7 :  0.549\n",
      "0.8 :  0.5665\n",
      "0.9 :  0.5702\n",
      "0.95 :  0.5423\n",
      "0.99 :  0.4206\n",
      "=========================\n",
      "0.7712\n",
      "0.6 :  0.535\n",
      "0.7 :  0.5574\n",
      "0.8 :  0.5752\n",
      "0.9 :  0.5791\n",
      "0.95 :  0.5508\n",
      "0.99 :  0.4273\n",
      "=========================\n",
      "0.7829\n",
      "0.6 :  0.5432\n",
      "0.7 :  0.5659\n",
      "0.8 :  0.584\n",
      "0.9 :  0.588\n",
      "0.95 :  0.5593\n",
      "0.99 :  0.434\n",
      "=========================\n",
      "0.7945\n",
      "0.6 :  0.5513\n",
      "0.7 :  0.5744\n",
      "0.8 :  0.5928\n",
      "0.9 :  0.5969\n",
      "0.95 :  0.5679\n",
      "0.99 :  0.4408\n",
      "=========================\n",
      "0.8062\n",
      "0.6 :  0.5597\n",
      "0.7 :  0.5832\n",
      "0.8 :  0.6019\n",
      "0.9 :  0.6061\n",
      "0.95 :  0.5768\n",
      "0.99 :  0.448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "0.8179\n",
      "0.6 :  0.5678\n",
      "0.7 :  0.5916\n",
      "0.8 :  0.6107\n",
      "0.9 :  0.6151\n",
      "0.95 :  0.5852\n",
      "0.99 :  0.4546\n",
      "=========================\n",
      "0.8296\n",
      "0.6 :  0.5762\n",
      "0.7 :  0.6004\n",
      "0.8 :  0.6198\n",
      "0.9 :  0.6244\n",
      "0.95 :  0.5941\n",
      "0.99 :  0.4618\n",
      "=========================\n",
      "0.8413\n",
      "0.6 :  0.5839\n",
      "0.7 :  0.6085\n",
      "0.8 :  0.6282\n",
      "0.9 :  0.6329\n",
      "0.95 :  0.6021\n",
      "0.99 :  0.4679\n",
      "=========================\n",
      "0.853\n",
      "0.6 :  0.592\n",
      "0.7 :  0.6168\n",
      "0.8 :  0.6369\n",
      "0.9 :  0.6416\n",
      "0.95 :  0.6105\n",
      "0.99 :  0.4743\n",
      "=========================\n",
      "0.8646\n",
      "0.6 :  0.6005\n",
      "0.7 :  0.6257\n",
      "0.8 :  0.646\n",
      "0.9 :  0.6508\n",
      "0.95 :  0.6193\n",
      "0.99 :  0.4813\n",
      "=========================\n",
      "0.8763\n",
      "0.6 :  0.6095\n",
      "0.7 :  0.635\n",
      "0.8 :  0.6555\n",
      "0.9 :  0.6604\n",
      "0.95 :  0.6286\n",
      "0.99 :  0.4894\n",
      "=========================\n",
      "0.888\n",
      "0.6 :  0.6177\n",
      "0.7 :  0.6435\n",
      "0.8 :  0.6644\n",
      "0.9 :  0.6694\n",
      "0.95 :  0.6372\n",
      "0.99 :  0.4963\n",
      "=========================\n",
      "0.8997\n",
      "0.6 :  0.6264\n",
      "0.7 :  0.6525\n",
      "0.8 :  0.6736\n",
      "0.9 :  0.6787\n",
      "0.95 :  0.6463\n",
      "0.99 :  0.5041\n",
      "=========================\n",
      "0.9114\n",
      "0.6 :  0.6349\n",
      "0.7 :  0.6613\n",
      "0.8 :  0.6827\n",
      "0.9 :  0.6878\n",
      "0.95 :  0.655\n",
      "0.99 :  0.5115\n",
      "=========================\n",
      "0.9231\n",
      "0.6 :  0.6435\n",
      "0.7 :  0.6702\n",
      "0.8 :  0.6918\n",
      "0.9 :  0.697\n",
      "0.95 :  0.6639\n",
      "0.99 :  0.519\n",
      "=========================\n",
      "0.9348\n",
      "0.6 :  0.6523\n",
      "0.7 :  0.6793\n",
      "0.8 :  0.7012\n",
      "0.9 :  0.7065\n",
      "0.95 :  0.673\n",
      "0.99 :  0.5268\n",
      "=========================\n",
      "0.9464\n",
      "0.6 :  0.6606\n",
      "0.7 :  0.6879\n",
      "0.8 :  0.7101\n",
      "0.9 :  0.7155\n",
      "0.95 :  0.6818\n",
      "0.99 :  0.534\n",
      "=========================\n",
      "0.9581\n",
      "0.6 :  0.6692\n",
      "0.7 :  0.6968\n",
      "0.8 :  0.7194\n",
      "0.9 :  0.7249\n",
      "0.95 :  0.6909\n",
      "0.99 :  0.5418\n",
      "=========================\n",
      "0.9698\n",
      "0.6 :  0.6776\n",
      "0.7 :  0.7056\n",
      "0.8 :  0.7285\n",
      "0.9 :  0.7341\n",
      "0.95 :  0.6999\n",
      "0.99 :  0.5496\n",
      "=========================\n",
      "0.9815\n",
      "0.6 :  0.6858\n",
      "0.7 :  0.7141\n",
      "0.8 :  0.7372\n",
      "0.9 :  0.743\n",
      "0.95 :  0.7085\n",
      "0.99 :  0.5568\n",
      "=========================\n",
      "0.9932\n",
      "0.6 :  0.6944\n",
      "0.7 :  0.7229\n",
      "0.8 :  0.7463\n",
      "0.9 :  0.7522\n",
      "0.95 :  0.7175\n",
      "0.99 :  0.5648\n",
      "(0.6993184761271871, 0.7280321605221918, 0.7515076927549689, 0.7574526690308752, 0.7227335123980844, 0.5693062668215182)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_nn_2(titles, input_vectors, occs, model):\n",
    "    nexample = len(titles)\n",
    "    accuracy1 = 0.\n",
    "    accuracy2 = 0.\n",
    "    accuracy3 = 0.\n",
    "    accuracy4 = 0.\n",
    "    accuracy5 = 0.\n",
    "    accuracy6 = 0.\n",
    "    prediction = None\n",
    "    for i in range(len(titles)):        \n",
    "        input_vector = input_vectors[i].reshape(1, -1)\n",
    "        prediction1, prediction2, prediction3, prediction4, prediction5, prediction6 = predict_nn_2(model, input_vector)\n",
    "        p1 = frozenset(prediction1)\n",
    "        p2 = frozenset(prediction2)\n",
    "        p3 = frozenset(prediction3)\n",
    "        p4 = frozenset(prediction4)\n",
    "        p5 = frozenset(prediction5)\n",
    "        p6 = frozenset(prediction6)\n",
    "        g = frozenset(occs[i])\n",
    "        accuracy1 += 1. / nexample * len(p1 & g) / len(p1 | g)\n",
    "        accuracy2 += 1. / nexample * len(p2 & g) / len(p2 | g)\n",
    "        accuracy3 += 1. / nexample * len(p3 & g) / len(p3 | g)\n",
    "        accuracy4 += 1. / nexample * len(p4 & g) / len(p4 | g)\n",
    "        accuracy5 += 1. / nexample * len(p5 & g) / len(p5 | g)\n",
    "        accuracy6 += 1. / nexample * len(p6 & g) / len(p6 | g)\n",
    "        if i % 1000 == 0:\n",
    "            print(\"=========================\")\n",
    "            print(round(i / nexample, 4))\n",
    "            print(\"0.6 : \", round(accuracy1, 4))\n",
    "            print(\"0.7 : \", round(accuracy2, 4))\n",
    "            print(\"0.8 : \", round(accuracy3, 4))\n",
    "            print(\"0.9 : \", round(accuracy4, 4))\n",
    "            print(\"0.95 : \", round(accuracy5, 4))\n",
    "            print(\"0.99 : \", round(accuracy6, 4))\n",
    "    return accuracy1, accuracy2, accuracy3, accuracy4, accuracy5, accuracy6\n",
    "\n",
    "# print(evaluate_nn_2(titles_train, summaries_train, occs_train, model))\n",
    "print(evaluate_nn_2(titles_train_test, data_test, occs_train_test, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_nn_3(model, input_vector, print_score = False):\n",
    "    \n",
    "    scores = model.predict(input_vector).reshape(100)\n",
    "    predictions = np.where(scores > 0.9)[0]\n",
    "    res = set(np.array(occupations)[predictions])\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253164 / 643108  -  39.365705293667624 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-038970ea3817>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" - \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-038970ea3817>\u001b[0m in \u001b[0;36mexport\u001b[0;34m(start)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitles_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0minput_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_nn_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0msol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'title'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtitles_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'prediction'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msol\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-80c73c0af18d>\u001b[0m in \u001b[0;36mpredict_nn_3\u001b[0;34m(model, input_vector, print_score)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_nn_3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_vector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moccupations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1397\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1398\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1399\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1400\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1401\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output, display\n",
    "import time\n",
    "\n",
    "sequences_res = tokenizer.texts_to_sequences(summaries_test)\n",
    "data_res = pad_sequences(sequences_res, maxlen=maxlen)\n",
    "\n",
    "def export(start=0):\n",
    "    with gzip.open('results.json.gz', 'wt') as output:\n",
    "        for i in range(start, len(titles_test)):    \n",
    "            input_vector = data_res[i].reshape(1, -1)\n",
    "            prediction = predict_nn_3(model, input_vector)\n",
    "            sol = list(prediction)            \n",
    "            output.write(json.dumps({'title':titles_test[i], 'prediction': sol}) + \"\\n\")\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            print(i,\"/\", len(titles_test), \" - \", i * 100 / len(titles_test), \"%\")\n",
    "\n",
    "export(253165)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***IMPORTANT*** Output format of requested file 'results.json.gz': each line must be a json string representing a dictionnary:\n",
    "> ```{ 'title': THE_ARTICLE_NAME, 'prediction': [THE_LIST_OF_OCCUPATIONS]}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example if testset_solutions is a dictionnary: article_name (key) -> prediction_list (value) use this function:\n",
    "def export(testset_solutions):\n",
    "    with gzip.open('results.json.gz', 'wt') as output:\n",
    "        for article in testset_solutions:\n",
    "            output.write(json.dumps({'title':article, 'prediction':testset_solutions[article]}) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
